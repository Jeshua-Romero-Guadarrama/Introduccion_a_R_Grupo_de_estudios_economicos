[["index.html", "R Grupo de estudios económicos Prefacio", " R Grupo de estudios económicos Jeshua Romero Guadarrama, Kevin Fernández, Apocryfo, Jenn, Daniel, Tifany Jiménez, Ernesto, Ezequiel, Rich Conejo, Angiebaram, Jesmarth, Adolfo Robles, Isaac Flores, Abdeel, Roberto Daniel 2021-08-08 Prefacio Publicado por Jeshua Romero Guadarrama en colaboración con JeshuaNomics: Git Hub Facebook Twitter Linkedin Vkontakte Tumblr YouTube Instagram Jeshua Romero Guadarrama es economista y actuario por la Universidad Nacional Autónoma de México, quien ha construido el presente proyecto en colaboración con JeshuaNomics, ubicado en la Ciudad de México, se puede contactar mediante el siguiente correo electrónico: jeshuanomics@gmail.com. Última actualización el domingo 08 del 08 de 2021 Los estudiantes con poca experiencia en el análisis avanzado de estadísticas a menudo tienen dificultades para entender los beneficios de desarrollar habilidades de programación al momento de aplicar diversos métodos descriptivos e inferenciales. Análisis estadístico con R para principiantes por Jeshua Romero Guadarrama (2021), ofrece una introducción interactiva a los aspectos esenciales de la programación por medio del lenguaje y software estadístico R, así como una guía para la aplicación de la teoría económica y econométrica en entornos específicos. En otras palabras, el objetivo es que los estudiantes se adentren al mundo de la economía aplicada mediante ejemplos empíricos presentados en la vida diaria y haciendo uso de las habilidades de programación recién adquiridas. Dicho objetivo se encuentra respaldado por ejercicios de programación interactivos y la incorporación de visualizaciones dinámicas de conceptos fundamentales mediante la flexibilidad de JavaScript, a través de la biblioteca D3.js. En los últimos años, el lenguaje de programación estadística R se ha convertido en una parte integral del plan de estudios de las clases de estadística que se imparten en las universidades. Regularmente una gran parte de los estudiantes no han estado expuestos a ningún lenguaje de programación antes y, por lo tanto, tienen dificultades para participar en el aprendizaje de R por sí mismos. Con poca experiencia en el análisis avanzado de estadísticas, es natural que los novicios tengan dificultades para comprender los beneficios de desarrollar habilidades en R para aprender y aplicar la estadística. Estos incluyen particularmente la capacidad de realizar, documentar y comunicar estudios empíricos y tener las facilidades para programar estudios de simulación, lo cual es útil para, por ejemplo, comprender y validar teoremas que generalmente no se asimilan o entienden fácilmente con el estudio de las fórmulas. Al ser un economistas aplicado y econometrista, me gustaría que mis colegas desarrollen capacidades de gran valor; en consecuencia, deseo compartir con las nuevas generaciones de economistas mis conocimientos. En lugar de confrontar a los estudiantes con ejercicios de codificación puros y literatura clásica complementaria, he pensado que sería mejor proporcionar material de aprendizaje interactivo que combine el código en R con el contenido del curso de texto Introducción a la Econometría de (stock2015?) que sirve de base para el presente material. El presente trabajo es un complemento empírico interactivo al estilo de un informe de investigación reproducible que permite a los estudiantes no solo aprender cómo los resultados de los estudios de casos se pueden replicar con R, sino que también fortalece su capacidad para utilizar las habilidades recién adquiridas en otras aplicaciones empíricas. Las convenciones usadas en el presente curso El texto en cursiva indica nuevos términos, nombres, botones y similares. El texto en negrita se usa generalmente en párrafos para referirse al código R. Esto incluye comandos, variables, funciones, tipos de datos, bases de datos y nombres de archivos. Texto de ancho constante sobre fondo gris indica un código R que usted puede escribir literalmente. Puede aparecer en párrafos para una mejor distinción entre declaraciones de código ejecutables y no ejecutables, pero se encontrará principalmente en forma de grandes bloques de código R. Estos bloques se denominan fragmentos de código. Reconocimiento A mi alma máter: Universidad Nacional Autónoma de México (Facultad de Economía y Facultad de Ciencias). Por brindarme valiosas oportunidades que coadyuvaron a mi formación. Esta obra está autorizado bajo la Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["contenido.html", "Contenido", " Contenido Introducción Sobre este curso Similitud con este curso Otro para principiantes Lo que puede omitir con seguridad Supuestos tontos Cómo está organizado este curso Parte I: Introducción al análisis estadístico con R Parte II: Descripción de datos Parte III: Sacar conclusiones a partir de los datos Parte IV: Trabajar con probabilidad Parte V: La parte de diez Apéndice A en línea: Más sobre probabilidad Apéndice B en línea: Estadísticas no paramétricas Apéndice C en línea: Diez temas que simplemente no encajan en ningún otro capítulo Iconos utilizados en este curso A dónde ir desde aquí "],["índice-de-contenido.html", "Índice de contenido", " Índice de contenido Parte I: Introducción al análisis estadístico con R Datos, estadísticas y decisiones Las nociones estadísticas (y relacionadas) que solo debe conocer Muestras y poblaciones Variables: dependientes e independientes Tipos de datos Un poco de probabilidad Estadística inferencial: probando hipótesis Hipótesis nulas y alternativas Dos tipos de error R: Qué hace y cómo lo hace Descargando R y RStudio Una sesión con R El directorio de trabajo Así que comencemos, ya Datos faltantes Funciones R Funciones definidas por el usuario comentarios R Estructuras Vectores Vectores numéricos Matrices Factores Listas Listas y estadísticas Marcos de datos Paquetes Más paquetes R Fórmulas Leyendo y escribiendo Hojas de cálculo Archivos CSV Archivos de texto Parte II: Descripción de datos Obtención de gráficos Encontrar patrones Graficar una distribución Salto de bares Rebanar el pastel La trama de dispersión De cajas y bigotes Gráficos básicos R Histogramas Añadiendo características gráficas Parcelas de barras Gráficos circulares Gráficos de puntos Parcelas de barras revisitadas Diagramas de dispersión Diagramas de caja Graduarse a ggplot2 Histogramas Parcelas de barras Gráficos de puntos Parcelas de barras revisitadas Diagramas de dispersión Diagramas de caja Terminando Encontrar su centro Medios: el atractivo de los promedios El promedio en R: mean() ¿Cuál es tu condición? Eliminar $-signos con with() Explorando los datos Valores atípicos: el defecto de los promedios Otros medios para un fin Medianas: atrapadas en el medio La mediana en R: median() Estadísticas à la Mode El modo en R Desviarse del promedio Medición de la variación Desviaciones cuadradas promedio: varianza y cómo calcularla Varianza de la muestra Varianza en R Regreso a las raíces: desviación estándar Desviación estándar de la población Desviación estándar de la muestra Desviación estándar en R Condiciones, condiciones, condiciones Cumplimiento de estándares y posiciones Atrapando algunas Z Características de las puntuaciones z Bonos versus Bambino Puntajes de exámenes Puntuaciones estándar en R ¿Cuál es tu posición? Clasificación en R Puntuaciones empatadas Nth más pequeño, Nth más grande Percentiles Rangos de porcentaje Resumiendo Resumiendo todo ¿Cuántos? Lo alto y lo bajo Viviendo en los momentos Un momento de enseñanza Volver a descriptivos Asimetría Curtosis Sintonización de la frecuencia Variables nominales: table() et al Variables numéricas: hist() Variables numéricas: stem() Resumiendo un marco de datos ¿Qué es normal? Golpear la curva Profundizando Parámetros de una distribución normal Trabajar con distribuciones normales Distribuciones en R Función de densidad normal Función de densidad acumulativa Cuantiles de distribuciones normales Muestreo aleatorio Un miembro distinguido de la familia Parte III: Sacar conclusiones a partir de los datos El juego de la confianza: estimación Comprensión de las distribuciones de muestreo Una idea EXTREMADAMENTE importante: el teorema del límite central (Aproximadamente) Simulando el teorema del límite central Predicciones del teorema del límite central Confianza: ¡tiene sus límites! Encontrar límites de confianza para una media Encajar en una t Prueba de hipótesis de una muestra Hipótesis, pruebas y errores Pruebas de hipótesis y distribuciones muestrales Coger algo de Z de nuevo Prueba Z en R t para uno t Prueba en R Trabajar con distribuciones t Visualización de distribuciones t Trazado de t en gráficos R base Trazando t en ggplot2 Una cosa más sobre ggplot2 Probando una varianza Pruebas en R Trabajar con distribuciones de chi-cuadrado Visualización de distribuciones de chi-cuadrado Trazado de chi-cuadrado en gráficos R base Trazar chi-cuadrado en ggplot2 Prueba de hipótesis de dos muestras Hipótesis construidas para dos Distribuciones de muestreo revisadas Aplicación del teorema del límite central Z una vez más Prueba Z para dos muestras en R t para dos Como guisantes en una vaina: variaciones iguales Prueba t en R Trabajando con dos vectores Trabajar con un marco de datos y una fórmula Visualizando los resultados Como p y q: varianzas desiguales Un conjunto emparejado: prueba de hipótesis para muestras emparejadas Prueba t de muestras pareadas en R Prueba de dos variaciones Prueba F en R F junto con t Trabajar con distribuciones F Visualización de distribuciones F Prueba de más de dos muestras Probando más de dos Un problema espinoso Una solución Relaciones significativas ANOVA en R Visualizando los resultados Después del ANOVA Contrastes en R Comparaciones no planificadas Otro tipo de hipótesis, otro tipo de prueba Trabajo con ANOVA de medidas repetidas ANOVA de medidas repetidas en R Visualizando los resultados Ponerse de moda Análisis de tendencias en R Pruebas más complicadas Rompiendo las combinaciones Interacciones El análisis ANOVA bidireccional en R Visualización de los resultados bidireccionales Dos tipos de variables. . . En seguida ANOVA mixto en R Visualización de los resultados de ANOVA mixtos Después del análisis Análisis multivariado de varianza MANOVA en R Visualización de los resultados de MANOVA Después del análisis Regresión: modelo lineal, múltiple y lineal general La trama de la dispersión Graficar líneas Regresión: ¡Qué línea! Uso de regresión para pronosticar Variación alrededor de la línea de regresión Prueba de hipótesis sobre regresión Regresión lineal en R Características del modelo lineal Haciendo predicciones Visualización del diagrama de dispersión y la línea de regresión Graficando los residuales Hacer malabares con muchas relaciones a la vez: regresión múltiple Regresión múltiple en R Haciendo predicciones Visualización del diagrama de dispersión 3D y el plano de regresión ANOVA: otra mirada Análisis de covarianza: el componente final del GLM Pero espera, hay más Correlación: el auge y la caída de las relaciones Parcelas de dispersión de nuevo Comprensión de la correlación Correlación y regresión Prueba de hipótesis sobre la correlación ¿Un coeficiente de correlación es mayor que cero? ¿Se diferencian dos coeficientes de correlación? Correlación en R Calcular un coeficiente de correlación Prueba de un coeficiente de correlación Prueba de la diferencia entre dos coeficientes de correlación Calcular una matriz de correlación Visualización de matrices de correlación Correlación múltiple Correlación múltiple en R Ajuste de R-cuadrado Correlación parcial Correlación parcial en R Correlación semiparcial Correlación semiparcial en R Regresión curvilínea: cuando las relaciones se complican ¿Qué es un logaritmo? ¿Qué es e? Regresión de potencia Regresión exponencial Regresión logarítmica Regresión polinomial: un poder superior ¿Qué modelo debería utilizar? Parte IV: Trabajar con probabilidad Introducción a la probabilidad ¿Qué es la probabilidad? Experimentos, ensayos, eventos y espacios de muestra Espacios muestrales y probabilidad Eventos compuestos Unión e intersección Intersección de nuevo La probabilidad condicional Trabajando con las probabilidades La base de la prueba de hipótesis Grandes espacios de muestra Permutaciones Combinaciones R Funciones para contar reglas Variables aleatorias: discretas y continuas Distribuciones de probabilidad y funciones de densidad La distribución binomial El binomio binomial y el binomio negativo en R Distribución binomial Distribución binomial negativa Prueba de hipótesis con la distribución binomial Más sobre pruebas de hipótesis: R versus tradición Introducción al modelado Modelado de una distribución Sumergirse en la distribución de Poisson Modelado con la distribución de Poisson Probando el ajuste del modelo Un comentario sobre chisq.test() Jugando a la pelota con un modelo Una discusión simulada Arriesgarse: el método Monte Carlo Cargando los dados Simulando el teorema del límite central Parte V: La parte de diez Diez consejos para emigrados de Excel Definir un vector en R es como nombrar un rango en Excel Operar en vectores es como operar en rangos con nombre A veces, las funciones estadísticas funcionan de la misma manera Y a veces no Contraste: Excel y R funcionan con diferentes formatos de datos Las funciones de distribución son (algo) similares Un marco de datos es (algo) como un rango con nombre de varias columnas La función sapply() es como arrastrar Usar edit() es (casi) como editar una hoja de cálculo Utilice el portapapeles para importar una tabla de Excel a R Diez valiosos recursos R en línea Sitios web para usuarios R R - blogueros Red de aplicaciones de Microsoft R Rápido - R RStudio Aprendizaje en línea Desbordamiento de pila Libros y documentación en línea R manuales Documentación R RDocumentación USTED PUEDE analizar El diario R "],["1-introducción-qué-es-r-y-para-qué-es-usado.html", "Capítulo 1 Introducción: ¿Qué es R y para qué es usado?", " Capítulo 1 Introducción: ¿Qué es R y para qué es usado? R es un lenguaje de programación y entorno computacional dedicado a la estadística. Decimos que es un lenguaje de programación porque nos permite dar instrucciones, usando código, a nuestros equipos de cómputo para que realicen tareas específicas (además de que es Turing Completo, pero profundizaremos en ello); para ello sólo necesitamos un intérprete para este código y es a esto a lo que llamamos un entorno computacional. Cuando instalamos R en nuestra computadora en realidad lo que estamos instalando es el entorno computacional, y para que podamos hacer algo en ese entorno necesitamos conocer la manera de escribir instrucciones que el software pueda interpretar y ejecutar. Eso es lo que aprenderemos a hacer en este curso. R es diferente a otros lenguajes de programación que por lo general están diseñados para realizar muchas tareas diferentes; esto es porque fue creado con el único propósito de hacer estadística. Esta característica es la razón de que R sea un lenguaje de programación peculiar, que puede resultar absurdo en algunos sentidos para personas con experiencia en otros lenguajes, pero también es la razón por la que R es una herramienta muy poderosa para el trabajo en estadística, puesto que funciona de la manera que una persona especializada en esta disciplina desearía que lo hiciera. Para entender mejor estas peculiaridades, nos conviene conocer un poco de los orígenes de este lenguaje de programación. "],["1-1-un-poco-de-historia.html", "1.1 Un poco de historia", " 1.1 Un poco de historia R tiene sus orígenes en S, un lenguaje de programación creado en los Laboratorios Bell de Estados Unidos. Sí, los mismos laboratorios que inventaron el transistor, el láser, el sistema operativo Unix y algunas otras cosas más. Dado que S y sus estándares son propiedad de los Laboratorios Bell, lo cual restringe su uso, Ross Ihaka y Robert Gentleman, de la Universidad de Auckland en Nueva Zelanda, decidieron crear una implementación abierta y gratuita de S. Este trabajo, que culminaría en la creación de R inició en 1992, teniendo una versión inicial del lenguaje en 1995 y en el 2000 una versión final estable. R hereda muchas características de S, por lo que puedes correr código de este lenguaje usando R sin mayor problema. Para lograr esto, en R frecuentemente existe más de una manera de realizar tareas comunes, una compatible con S y otra diseñada específicamente para R. Lo anterior tiene como resultado inconsistencias, sintaxis poco intuitiva y abundante frustración de cabeza para las personas que quieren aprender R. En el presente, el mantenimiento y desarrollo de R es realizado por el R Development Core Team, un equipo de especialistas en ciencias computacionales y estadística provenientes de diferentes instituciones y lugares alrededor del mundo. La versión de R mantenida por este equipo es conocida como base y como su nombre indica, es sobre aquella que se crean otras implementaciones de R así como los paquetes que expanden su funcionalidad. Para lograr que R sea usado sin restricciones es distribuido de manera gratuita, a través de la Licencia Pública General de GNU, por lo que es software libre y de código abierto. Si lo deseas, puedes examinar y estudiar el código que hace que R funcione o puedes crear versiones propias de R que se ajusten a tus necesidades particulares. Esta licencia también te permite usar R para los fines que desees, sin limitaciones, no importando si personales, académicos o comerciales. En la actualidad, el desarrollo de este lenguaje de programación se mantiene activa. La versión más reciente de R al momento de escribir este documento es la 3.4.2 Short Summer fue publicada en septiembre del 2017 y diariamente son publicados nuevos paquetes y sus respectivas actualizaciones. "],["1-2-quién-usa-r.html", "1.2 ¿Quién usa R?", " 1.2 ¿Quién usa R? R es un lenguaje relativamente joven pero que ha experimentado un crecimiento acelerado en su adopción durante los últimos 10 años. En septiembre de 2017, de acuerdo al TIOBE programming community index (2017), que es uno de los índices de más prestigio en el mundo en relación popularidad en el uso de lenguajes de programación, R era el lenguaje número 11 en popularidad, después de haber sido el lenguaje número 18 en el 2016. Esto es sobresaliente si consideramos que R es un lenguaje dedicado únicamente a la estadística, mientras que lenguajes como Python (número 5 en 2017) o Java (número 1) son lenguajes que pueden ser usados para todo tipo de tareas, desde crear sitios web hasta programar robots. La adopción de R se debe en gran medida a que permite responder preguntas mediante el uso de datos de forma efectiva, y como es un lenguaje abierto y gratuito, se facilita compartir código, crear herramientas para solucionar problemas comunes y que todo tipo de personas interesadas en análisis estadísticos puedan participar y contribuir al desarrollo y uso de R, no sólo aquellas que tengan acceso a licencias de software cerrado. Incluso compañías e instituciones que no tendrían ninguna dificultad para financiar el costo de licencias de software cerrado utilizan R. R, por citar un ejemplo, es usado por Facebook para analizar la manera en que sus usuarios interactúan con sus muros de publicaciones para así determinar qué contenido mostrarles. Esta es una tarea muy importante en Facebook, pues las interacciones de los usuarios con publicidad y contenido pagado son la principal fuente de ingreso de esta compañía. Además de que su división de recursos humanos emplea esta herramienta para estudiar las interacciones entre sus trabajadores. Google usa R para analizar la efectividad las campañas de publicidad implementadas en sus servicios, por ejemplo, los anuncios pagados que te aparecen cuando googleas algo. Nuevamente, esta es la principal fuente de ingresos de esta compañía. R También es usado para hacer predicciones económicas y otras actividades. Microsoft adquirió y ahora desarrolla una versión propia de R llamada OpenR, que ha hecho disponible para uso general del público. OpenR es empleada para realizar todo tipo de análisis estadísticos, por ejemplo, para empatar a jugadores en la plataforma de videojuegos XBOX Live (así que puedes culpar a R cuando te tocan partidas contra jugadores mucho más hábiles que tú). Otras compañías que usan R de modo cotidiano son American Express, IBM, Ford, Citibank, HP y Roche, entre muchas más (Bhalla, 2016; Level, 2017; Microsoft, 2014). Lo anterior ilustra algunas de las aplicaciones específicas de este lenguaje y de manera general podemos decir que R es usado para procesar, analizar, modelar y comunicar datos. Aunque R está diseñado para análisis estadístico, con el paso del tiempo los usuarios de este lenguaje han creado extensiones a R, llamadas paquetes, que han ampliado su funcionalidad. En la actualidad es posible realizar en R minería de textos, procesamiento de imagen, visualizaciones interactivas de datos y procesamiento de Big Data, entre muchas otras cosas. Así que, empecemos a usar R. Referencias Level (2017). How Big Companies Are Using R for Data Analysis. Recuperado en septiembre de 2017 de: http://www.northeastern.edu/levelblog/2017/05/31/big-companies-using-r-data-analysis/ Microsoft (2014). Companies using R in 2014. Recuperado en septiembre de 2017 de: http://blog.revolutionanalytics.com/2014/05/companies-using-r-in-2014.html Bhalla, D. (2016) Companies using R. Recuperado en septiembre de 2017 de: http://www.listendata.com/2016/12/companies-using-r.html R FAQ. Recuperado en Septiembre de 2017 de: https://cran.r-project.org/doc/FAQ/R-FAQ.html#What-is-R_003f TIOBE Index for September 2017. Recuperado en Septiembre de 2017 de: https://www.tiobe.com/tiobe-index/ Adesanya, T. (2017). A Gentler Introduction to Programming. Recuperado en Septiembre de 2017 de: https://medium.freecodecamp.org/a-gentler-introduction-to-programming-707453a79ee8 "],["2-instalación.html", "Capítulo 2 Instalación", " Capítulo 2 Instalación La manera de instalar R cambia dependiendo del sistema operativo utilices pero todas tienen en común el uso de CRAN. CRAN es el The Comprehensive R Archive Network, una red en la que se archivan todas las versiones de R base, así como todos los paquetes para R que han pasado por un proceso de revisión riguroso, realizado por el CRAN Team, que se encarga de asegurar su correcto funcionamiento. CRAN es una red porque existen copias de su contenido en diferentes servidores alrededor del mundo, los cuales se actualizan diariamente. De este modo, no importa de qué servidor de CRAN descargues R o algún paquete, lo que vas a obtener será la versión más reciente de ese recurso, que es igual a la disponible en todos los demás servidores. Como veremos más adelante, cuando descargamos un paquete de R, lo estamos haciendo desde CRAN, a menos que indiquemos otra cosa. El sitio oficial de CRAN, en el que encontrarás más información sobre este repositorio es el siguiente: https://cran.r-project.org/ "],["2-1-windows.html", "2.1 Windows", " 2.1 Windows Para instalar R en Windows, la forma más simple es descargar la versión más reciente de R base desde el siguiente enlace de CRAN: https://cran.r-project.org/bin/windows/base/ El archivo que necesitamos tiene la extensión .exe (por ejemplo R-3.5.1-win.exe). Una vez descargado, lo ejecutamos como cualquier instalable. Después de la instalación, estamos listos para usar R. "],["2-2-osx.html", "2.2 OSX", " 2.2 OSX Para instalar R en OSX, se sigue un procedimiento similar que en Windows. Necesitamos descargar los archivos binarios de R base desde CRAN y ejecutarlos. https://cran.r-project.org/bin/macosx/ Al concluir la instalación, podremos usar R, incluso llamándolo directamente desde la consola. "],["2-3-linux.html", "2.3 Linux", " 2.3 Linux En Linux, como suele ser el caso para casi todo, hay una manera fácil y una difícil de instalar R. La manera fácil depende de la presencia de R en los repositorios de la distribución de Linux que estés usando. Si R se encuentra en los repositorios de tu distribución, sólo es necesario usar el gestor de paquetes de tu preferencia para instalarlo, como cualquier otro software. Si R no se encuentra en los repositorios, debes agregar una entrada a tu lista de fuentes de software. Esta entrada depende de tu distribución. También tienes la opción de puedes compilar R directamente desde archivos fuente. Para todas las opciones anteriores, los detalles de instalación se se encuentran en el siguiente enlace: https://cran.r-project.org/bin/linux/ Si estás usando Linux no te debería ser difícil seguir las instrucciones presentadas. "],["2-4-rstudio-un-ide-para-r.html", "2.4 RStudio - un IDE para R", " 2.4 RStudio - un IDE para R Aunque podemos usar R directamente, es recomendable instalar y usar un entorno integrado de desarrollo (IDE, por sus siglas en inglés). Podemos utilizar R ejecutando nuestro código directamente desde documentos de texto plano, pero esta es una manera poco efectiva de trabajar, especialmente en proyectos complejos. Un IDE nos proporciona herramientas para escribir y revisar nuestro código, administrar los archivos que estamos usando, gestionar nuestro entorno de trabajo y algunas otras herramientas de productividad. Tareas que serían difíciles o tediosas de realizar de otro modo, son fáciles a través de un IDE. Hay varias opciones de IDE para R, y entre ellas mi preferido es RStudio. Este entorno, además de incorporar las funciones esenciales de una IDE, es desarrollado por un equipo que ha contribuido de manera significativa para lograr que R sea lenguaje de programación más accesible, con un énfasis en la colaboración y la reproducción de los análisis. Para instalar RStudio, es necesario con descargar y ejecutar alguno de los instaladores disponibles en su sitio oficial. Están disponibles versiones para Windows, OSX y Linux. https://www.rstudio.com/products/rstudio/download/ Si ya hemos instalado R en nuestro equipo, RStudio lo detectará automáticamente y podremos utilizarlo desde este entorno. Si no instalamos RStudio antes que R, no hay problema, cada vez que iniciamos este programa, verificará la instalación de R. "],["3-conceptos-básicos.html", "Capítulo 3 Conceptos básicos", " Capítulo 3 Conceptos básicos Para trabajar con R es necesario conocer un poco del vocabulario usado en en este lenguaje de programación. Los siguientes son conceptos básicos que usaremos a lo largo de todo el libro. "],["3-1-la-consola-de-r.html", "3.1 La consola de R", " 3.1 La consola de R Lo primero que nos encontramos al ejecutar R es una pantalla que nos muestra la versión de este lenguaje que estamos ejecutando y un prompt: &gt;_ Esta es la consola de R y corresponde al entorno computacional de este lenguaje. Es aquí donde nuestro código es interpretado. Podemos escribir código directamente en la consola y R nos dará el resultado de lo pidamos allí mismo. Esta es la razón por la que se dice que R permite el uso interactivo, pues no es necesario compilar nuestro código para ver sus resultados. Si estás usando RStudio, te encontrarás la consola de R en uno de los paneles de este programa. "],["3-2-ejecutar-llamar-correr-y-devolver.html", "3.2 Ejecutar, llamar, correr y devolver", " 3.2 Ejecutar, llamar, correr y devolver Cuando hablamos de ejecutar, llamar o correr nos referimos a pedir que R realice algo, en otras palabras, estamos dando una instrucción o una entrada. Cuando decimos que R nos devuelve algo, es que ha realizado algo que le hemos pedido, es decir, nos está dando una salida. Por ejemplo, si escribimos los siguiente en la consola lo siguiente y damos Enter, estamos pidiendo que se ejecute esta operación: &gt; 1 + 1 Y nos será devuelto su resultado: [1] 2 "],["3-3-objetos.html", "3.3 Objetos", " 3.3 Objetos En R, todo es un objeto. Todos los datos y estructuras de datos son objetos. Además, todos los objetos tienen un nombre para identificarlos. La explicación de esto es un tanto compleja y se sale del alcance de este libro. Se relaciona con el paradigma de programación orientada a objetos y ese es todo un tema en sí mismo. Lo importante es que recuerdes que al hablar de un objeto, estamos hablando de cualquier cosa que existe en R y que tiene un nombre. "],["3-4-constantes-y-variables.html", "3.4 Constantes y variables", " 3.4 Constantes y variables De manera análoga al uso de estos términos en lenguaje matemático, una constante es un objeto cuyo valor no podemos cambiar, en contraste, una variable es un objeto que puede cambiar de valor. Por ejemplo, en la siguiente expresión, \\(\\pi\\) y 2 son constantes, mientras que a y r son variables. \\(a = \\pi r ^ 2\\) Las constantes y variables en R tienen nombres que nos permiten hacer referencia a ellas en operaciones. Las constantes ya están establecidas por R, mientras que nosotros podemos crear variables, asignándoles valores a nombres. En R usamos &lt;- para hacer asignaciones. De este modo, podemos asignar el valor 3 a la variable radio radio &lt;- 3 Hablaremos sobre asignaciones más adelante, en el capítulo de operadores. Es recomendable que al crear una variable usemos nombres claros, no ambiguos y descriptivos. Esto previene confusión y hace que nuestro código sea más fácil de comprender por otras personas o por nosotros mismos en el futuro. Los nombres de las variables pueden incluir letras, números, puntos y guiones bajos. Deben empezar siempre con una letra o un punto y si empiezan con un punto, a este no le puede seguir un número. Finalmente, cuando te encuentres con un renglón de código que inicia con un gato (hashtag), esto representa un comentario, es código que no se ejecutará, sólo se mostrará. # Este es un comentario "],["3-5-funciones-introducción-básica.html", "3.5 Funciones (introducción básica)", " 3.5 Funciones (introducción básica) Una función es una serie de operaciones a la que les hemos asignados un nombre. Las funciones aceptan argumentos, es decir, especificaciones sobre cómo deben funcionar. Cuando llamamos una función, se realizan las operaciones que contiene, usando los argumentos que hemos establecido. En R reconocemos a una función usando la notación: nombre_de_la_función(). Por ejemplo: mean() quantile() summary() density() c() Al igual que con las variables, se recomienda que los nombres de las funciones sean claros, no ambiguos y descriptivos. Idealmente, el nombre de una función describe lo que hace. De hecho, es probable que adivines qué hacen casi todas funciones de la lista de arriba a partir de su nombre. Aunque estrictamente hablando una función es un objeto, para fines de explicación, en este libro nos referiremos a ambos como si fueran cosas diferentes. Las funciones son un tema que revisamos más adelante. Por el momento, recuerda que una función realiza operaciones y nos pide argumentos para poder llevarlas a cabo. "],["3-6-documentación.html", "3.6 Documentación", " 3.6 Documentación Las funciones de R base y aquellas que forman parte de paquete tienen un archivo de documentación. Este archivo describe qué hace la función, sus argumentos, detalles sobre las operaciones que realiza,los resultados que devuelve y ejemplos de uso. Para obtener la documentación de una función, escribimos el ? antes de su nombre y lo ejecutamos. También podemos usar la función help(), con el nombre de la función. Los dos procedimientos siguientes son equivalentes. ?mean() help(&quot;mean&quot;) Si usas RStudio, la documentación de la función se mostrará en uno de los paneles de este IDE. Si estas usando R directamente, se abrirá una ventana de tu navegador de Internet. También podemos obtener la documentación de un paquete, si damos el argumento package a la función help(), con el nombre de un paquete. Por ejemplo, la documentación del paquete stats, instalado por defecto en R base. help(package = &quot;stats&quot;) "],["3-7-directorio-de-trabajo.html", "3.7 Directorio de trabajo", " 3.7 Directorio de trabajo El directorio o carpeta de trabajo es el lugar en nuestra computadora en el que se encuentran los archivos con los que estamos trabajando en R. Este es el lugar donde R buscara archivos para importarlos y al que serán exportados, a menos que indiquemos otra cosa. Puedes encontrar cuál es tu directorio de trabajo con la función getwd(). Sólo tienes que escribir la función en la consola y ejecutarla. getwd() ## [1] &quot;D:/# Libros/# Libros-GitHub/R_grupo_de_estudios_economicos&quot; Se mostrará en la consola la ruta del directorio que está usando R. Puedes cambiar el directorio de trabajo usando la función setwd(), dando como argumento la ruta del directorio que quieres usar. setwd(&quot;C:\\otro_directorio&quot;) Por último, si deseas conocer el contenido de tu directorio de trabajo, puedes ejecutar. la función list.files(), sin argumentos, que devolverá una lista con el nombre de los archivos de tu directorio de trabajo. La función list.dirs(), también sin argumentos` te dará una lista de los directorios dentro del directorio de trabajo. # Ver archivos list.files() # Ver directorios list.dirs() 3.7.1 Sesión Los objetos y funciones de R son almacenados en la memoria RAM de nuestra computadora. Cuando ejecutamos R, ya sea directamente o a través de RStudio, estamos creando una instancia del entorno del entorno computacional de este lenguaje de programación. cada instancia es una sesión. Todos los objetos y funciones creadas en una sesión, permanecen sólo en ella, no son compartidos entre sesiones, sin embargo una sesión puede tener el mismo directorio de trabajo que otra sesión. Es posible tener más de una sesión de R activa en la misma computadora. Aunque ambas Cuando cerramos R, también cerramos nuestra sesión. Se nos preguntará si deseamos guardar el contenido de nuestra sesión para poder volver a ella después. Esto se guarda en un archivo con extensión **.Rdata* en tu directorio de trabajo. Para conocer los objetos y funciones que contiene nuestra sesión, usamos la función ls(), que nos devolverá una lista con los nombres de todo lo guardado en la sesión. ls() ## [1] &quot;radio&quot; De manera más precisa, nuestra sesión es un entorno de trabajo y los objetos pertenecen a un entorno específico. Los entornos son un concepto importante al hablar de lenguajes de programación, pero también son un tema que sale del alcance de este libro. Con que recuerdes que cada sesión de R tiene su propio entorno global, eso será suficiente. "],["3-8-paquetes.html", "3.8 Paquetes", " 3.8 Paquetes R puede ser expandido con paquetes. Cada paquete es una colección de funciones diseñadas para atender una tarea específica. Por ejemplo, hay paquetes para trabajo visualización geoespacial, análisis psicométricos, minería de datos, interacción con servicios de Internet y muchas otras cosas más. Estos paquetes se encuentran alojados en CRAN, así que pasan por un control riguroso antes de estar disponibles para su uso generalizado. Podemos instalar paquetes usando la función install.packages(), dando como argumento el nombre del paquete que deseamos instalar, entre comillas. Por ejemplo, para instalar el paquete readr, corremos lo siguiente. install.packages(&quot;readr&quot;) Hecho esto, aparecerán algunos mensajes en la consola mostrando el avance de la instalación Una vez concluida la instalación de un paquete, podrás usar sus funciones con la función library(). Sólo tienes que llamar esta función usando como argumento el nombre del paquete que quieres utilizar library(readr) Cuando haces esto, R importa las funciones contenidas en el paquete al entorno de trabajo actual. Es importante que tengas en mente que debes hacer una llamada a library() cada que inicies una sesión en R. Aunque hayas importado las funciones de un paquete con anterioridad, las sesiones de R se inician limpias, sólo con los objetos y funciones de base. Este comportamiento es para evitar problemas de compatibilidad y para propiciar buenas prácticas de colaboración. Si importamos paquetes automáticamente y usamos sus funciones sin indicar de donde provienen, al compartir nuestro código con otras personas, estas no tendrán la información completa para entender qué estamos haciendo. R, al pedirnos que cada sesión indiquemos qué estamos importando, nos obliga a ser explícito con todo lo que estamos haciendo. Es un poco latoso, pero te acostumbras a ello. En caso de escribir en install.packages() el nombre de un paquete no disponible en CRAN, se nos mostrará una advertencia y no se instalará nada. install.packages(&quot;un_paquete_falso&quot;) Los paquetes que hemos importado en nuestra sesión actual aparecen al llamar sessionInfo(). También podemos ver qué paquetes tenemos ya instalados ejecutando la función installed.packages() sin ningún argumento. Una instalación nueva de R tiene pocos paquetes instalados, pero esta lista puede crecer considerablemente con el tiempo. "],["3-9-scripts.html", "3.9 Scripts", " 3.9 Scripts Los scripts son documentos de texto con la extensión de archivo .R, por ejemplo mi_script.R. Estos archivos son iguales a cualquier documentos de texto, pero R los puede leer y ejecutar el código que contienen. Aunque R permite el uso interactivo, es recomendable que guardes tu código en un archivo .R, de esta manera puedes usarlo después y compartirlo con otras personas. En realidad, en proyectos complejos, es posible que sean necesarios múltiples scripts para distintos fines. Podemos abrir y ejecutar scripts en R usando la función source(), dándole como argumento la ruta del archivo .R en nuestra computadora, entre comillas. Por ejemplo. source(&quot;C:/Mis scripts/mi_script.R&quot;) Cuando usamos RStudio y abrimos un script con extensión .R, este programa nos abre un panel en el cual podemos ver su contenido. De este modo podemos ejecutar todo el código que contiene o sólo partes de él. "],["4-tipos-de-datos.html", "Capítulo 4 Tipos de datos", " Capítulo 4 Tipos de datos En R los datos pueden ser de diferentes tipos. Cada tipo tiene características particulares que lo distinguen de los demás. Entre otras cosas algunas operaciones sólo pueden realizarse con tipos de datos específicos En este capítulo revisaremos los tipos de datos más comunes en R y sus propiedades, así como la coerción entre tipos de dato. "],["4-1-datos-más-comunes.html", "4.1 Datos más comunes", " 4.1 Datos más comunes Los tipos de datos de uso más común en R son los siguientes. Tipo Ejemplo Nombre en inglés Entero 1 integer Numérico 1.3 numeric Cadena de texto uno character Factor uno factor Lógico TRUE logical Perdido NA NA Vacío NULL null Además de estos tipos, en R también contamos con datos complejos numéricos complejos (con una parte real y una imaginaria), raw (bytes), fechas y raster, entre otros. Estos tipos tiene aplicaciones muy específicas, por ejemplo, los datos de tipo fecha son ampliamente usados en economía, para análisis de series de tiempo. Revisemos las principales características de estos tipos de dato. "],["4-2-entero-y-numérico.html", "4.2 Entero y numérico", " 4.2 Entero y numérico Como su nombre lo indica, los datos enteros representan números enteros, sin una parte decimal o fraccionaria, que pueden ser usados en operaciones matemáticas. Por su parte, como su nombre lo indica, los datos numéricos representan números, la diferencia de estos con los datos enteros es que tiene una parte decimal o fraccionaria. Los datos numéricos también son llamados doble o float (flotantes). Este nombre se debe a que, en realidad, son números de doble precisión, pues tienen una parte entera y una fraccionaria decimal, y son llamados float debido a que se usa un punto flotante para su representación computacional. Para fines prácticos, estos términos son sinónimos. En este libro, siempre que hablemos de datos numéricos, nos referimos a este tipo. "],["4-3-cadena-de-texto.html", "4.3 Cadena de texto", " 4.3 Cadena de texto El tipo character representa texto y es fácil reconocerlo porque un dato siempre esta rodeado de comillas, simples o dobles. De manera convencional, nos referimos a este tipo de datos como cadenas de texto, es decir, secuencias de caracteres. Este es el tipo de datos más flexible de R, pues una cadena de texto puede contener letras, números, espacios, signos de puntuación y símbolos especiales. "],["4-4-factor.html", "4.4 Factor", " 4.4 Factor Un factor es un tipo de datos específico a R. Puede ser descrito como un dato numérico representado por una etiqueta. Supongamos que tenemos un conjunto de datos que representan el sexo de personas encuestadas por teléfono, pero estos se encuentran capturados con los números 1 y 2. El número 1 corresponde a femenino y el 2 a masculino. En R, podemos indicar que se nos muestre, en la consola y para otros análisis, los 1 como femenino y los 2 como masculino. Aunque para nuestra computadora, femenino tiene un valor de 1, pero a nosotros se nos muestra la palabra femenino. De esta manera reducimos el espacio de almacenamiento necesario para nuestros datos. Este comportamiento es similar a lo que ocurre con paquetes estadísticos comerciales como SPSS Statistics, en los que podemos asignar etiquetas a los datos, dependiendo de su valor. La diferencia se encuentra en que R trata a los factores de manera diferente a un dato numérico. Por último, cada una de las etiquetas o valores que puedes asumir un factor se conoce como nivel. En nuestro ejemplo con femenino y masculino, tendríamos dos niveles. "],["4-5-lógico.html", "4.5 Lógico", " 4.5 Lógico Los datos de tipo lógico sólo tienen dos valores posibles: verdadero (TRUE) y falso (FALSE). Representan si una condición o estado se cumple, es verdadero, o no, es falso. Este tipo de dato es, generalmente, el resultado de operaciones relacionales y lógicas, son esenciales para trabajar con álgebra Booleana, lo cual revisaremos en el (capítulo 5)(#-operadores). Como este tipo de dato sólo admite dos valores específicos, es el más restrictivo de R. "],["4-6-na-y-null.html", "4.6 NA y NULL", " 4.6 NA y NULL En R, usamos NA para representar datos perdidos, mientras que NULL representa la ausencia de datos. La diferencia entre las dos es que un dato NULL aparece sólo cuando R intenta recuperar un dato y no encuentra nada, mientras que NA es usado para representar de modo explícito datos perdidos, omitidos o que por alguna razón son faltantes. Por ejemplo, si tratamos de recuperar la edad de una persona encuestada que no existe, obtendríamos un NULL, pues no hay ningún dato que corresponda con ello. En cambio, si tratamos de recuperar su estado civil, y la persona encuestada no contestó esta pregunta, obtendríamos un NA. NA además puede aparecer como resultado de una operación realizada, pero no tuvo éxito en su ejecución. "],["4-7-coerción.html", "4.7 Coerción", " 4.7 Coerción En R, los datos pueden ser coercionados, es decir, forzados, para transformarlos de un tipo a otro. La coerción es muy importante. Cuando pedimos a R ejecutar una operación, intentará coercionar de manera implícita, sin avisarnos, los datos de su tipo original al tipo correcto que la permita realizar. Habrá ocasiones en las que R tenga éxito y la operación ocurra sin problemas, y otras en las que falle y obtengamos un error. Lo anterior ocurre porque no todos los tipos de datos pueden ser transformados a los demás, para ello se sigue una regla general. La coerción de tipos se realiza de los tipos de datos más restrictivos a los más flexibles. Las coerciones ocurren en el siguiente orden. lógico -&gt; entero -&gt; numérico -&gt; cadena de texto (logical -&gt; integer -&gt; numeric -&gt; character) Las coerciones no pueden ocurrir en orden inverso. Podemos coercionar un dato de tipo entero a uno numérico, pero uno de cadena de texto a numérico. Como los datos de tipo lógico sólo admiten dos valores (TRUE y FALSE), estos son los más restrictivos; mientras que los datos de cadena de texto, al admitir cualquier cantidad y combinación de caracteres, son los más flexibles. Los factores son un caso particular para la coerción. Dado que son valores numéricos con etiquetas, pueden ser coercionados a tipo numérico y cadena de texto; y los datos numéricos y cadena de texto pueden ser coercionados a factor. Sin embargo, al coercionar un factor tipo numérico, perdemos sus niveles. 4.7.1 Coerción explícita con la familia as() También podemos hacer coerciones explícitas usando la familia de funciones as(). Función Tipo al que hace coerción as.integer() Entero as.numeric() Numérico as.character() Cadena de texto as.factor() Factor as.logical() Lógico as.null() NULL Todas estas funciones aceptan como argumento datos o vectores (veremos qué es un vector en el capítulo 6). Cuando estas funciones tienen éxito en la coerción, nos devuelven datos del tipo pedido. Si fallan, obtenemos NA como resultado. Por ejemplo, intentemos convertir el número 5 a una cadena de texto. Para ello usamos la función as.character(). as.character(5) ## [1] &quot;5&quot; Esta es una coerción válida, así que tenemos éxito. Pero, si intentamos convertir la palabra cinco a un dato numérico, obtendremos una advertencia y NA. as.numeric(&quot;cinco&quot;) ## Warning: NAs introducidos por coerción ## [1] NA Comprobemos el comportamiento especial de los factores. Podemos coercionar al número 5 y la palabra cinco en un factor. as.factor(5) ## [1] 5 ## Levels: 5 as.factor(&quot;cinco&quot;) ## [1] cinco ## Levels: cinco Asignamos la palabra cinco como factor al objeto factor_cinco. factor_cinco &lt;- as.factor(&quot;cinco&quot;) #Resultado factor_cinco ## [1] cinco ## Levels: cinco Ahora podemos coercionar factor_cinco a cadena de texto y a numérico. # Cadena de texto as.character(factor_cinco) ## [1] &quot;cinco&quot; # Numérico as.numeric(factor_cinco) ## [1] 1 Si coercionamos un dato de tipo lógico a numérico, TRUE siempre devolverá 1 y FALSE dará como resultado 0. as.numeric(TRUE) ## [1] 1 as.numeric(FALSE) ## [1] 0 Por último, la función as.null() siempre devuelve NULL, sin importar el tipo de dato que demos como argumento. # Lógico as.null(FALSE) ## NULL # Numérico as.null(457) ## NULL # Cadena de texto as.null(&quot;palabra&quot;) ## NULL "],["4-8-verificar-el-tipo-de-un-dato.html", "4.8 Verificar el tipo de un dato", " 4.8 Verificar el tipo de un dato En ocasiones, tenemos datos pero no sabemos de simple vistazo de qué tipo son. Para esto casos, podemos usar la función class() para determinar el tipo de un dato. Esto es de utilidad para asegurarnos que las operaciones que deseamos realizar tendrán los datos apropiados para llevarse a cabo con éxito. class() recibe como argumento un dato o vector y devuelve el nombre del tipo al que pertenece, en inglés. Por ejemplo, verificamos el tipo de datos que son 3, 3 y TRUE. class(3) ## [1] &quot;numeric&quot; class(&quot;3&quot;) ## [1] &quot;character&quot; class(TRUE) ## [1] &quot;logical&quot; 4.8.1 Verificación con la familia de funciones is() También podemos verificar si un dato es de un tipo específico con la familia de funciones is(). Función Tipo que verifican is.integer() Entero is.numeric() Numérico is.character() Cadena de texto is.factor() Factor is.logical() Lógico is.na() NA is.null() NULL Estas funciones toman como argumento un dato, si este es del tipo que estamos verificando, nos devolverán TRUE y en caso contrario devolverán FALSE. Por ejemplo, verificamos que 5 sea numérico. is.numeric(5) ## [1] TRUE Obtenemos TRUE, pues es verdadero que este es un dato numérico. Verificamos que 5 sea de tipo cadena de texto. is.character(5) ## [1] FALSE El resultado es FALSE, por lo tanto este no es un dato de cadena de texto. Conociendo el tipo de datos con los que estamos trabajando, nos aseguramos de que obtendremos los resultados esperados para las operaciones que estemos realizando. "],["5-operadores.html", "Capítulo 5 Operadores", " Capítulo 5 Operadores Los operadores son los símbolos que le indican a R que debe realizar una tarea. Combinando datos y operadores es que logramos que R haga su trabajo. Existen operadores específicos para cada tipo de tarea. Los tipos de operadores principales son los siguientes: Aritméticos Relacionales Lógicos De asignación Familiarizarnos con los operadores nos permitirá manipular y transformar datos de distintos tipos. "],["5-1-operadores-aritméticos.html", "5.1 Operadores aritméticos", " 5.1 Operadores aritméticos Como su nombre lo indica, este tipo de operador es usado para operaciones aritméticas. En R tenemos los siguientes operadores aritméticos: Operador Operación Ejemplo Resultado + Suma 5 + 3 8 - Resta 5 - 3 2 * Multiplicación 5 * 3 18 / División 5 /3 1.666667 ^ Potencia 5 ^ 3 125 %% División entera 5 %% 3 2 Es posible realizar operaciones aritméticas con datos de tipo entero y numérico. Si escribes una operación aritmética en la consola de R y das Enter, esta se realiza y se devuelve su resultado. 15 * 3 ## [1] 45 Cuando intentas realizar una operación aritmética con otro tipo de dato, R primero intentará coercionar ese dato a uno numérico. Si la coerción tiene éxito se realizará la operación normalmente, si falla, el resultado será un error. Por ejemplo, 4 + \"tres\" devuelve: Error in 4 + \"tres\" : non-numeric argument for binary operator.\" 4 + &quot;tres&quot; ## Error in 4 + &quot;tres&quot;: argumento no-numérico para operador binario El mensaje non-numeric argument for binary operator aparece siempre que intentas realizar una operación aritmética con un argumento no numérico. Si te encuentras un un error que contiene este mensaje, es la primera pista para que identifiques donde ha ocurrido un problema. Cualquier operación aritmética que intentemos con un dato NA, devolverá NA como resultado. NA - 66 ## [1] NA 21 * NA ## [1] NA NA ^ 13 ## [1] NA 5.1.1 La división entera Entre los operadores aritméticos, el de división entera o módulo requiere una explicación adicional sobre su uso. La operación que realiza es una división de un número entre otro, pero en lugar de devolver el cociente, nos devuelve el residuo. Por ejemplo, si hacemos una división entera de 4 entre 2, el resultado será 0. Esta es una división exacta y no tiene residuo. 4 %% 2 ## [1] 0 En cambio, si hacemos una división entera de 5 entre 2, el resultado será 1, pues este es el residuo de la operación. 5 %% 2 ## [1] 1 "],["5-2-operadores-relacionales.html", "5.2 Operadores relacionales", " 5.2 Operadores relacionales Los operadores lógicos son usados para hacer comparaciones y siempre devuelven como resultado TRUE o FALSE (verdadero o falso, respectivamente). Operador Comparación Ejemplo Resultado &lt; Menor que 5 &lt; 3 FALSE &lt;= Menor o igual que 5 &lt;= 3 FALSE &gt; Mayor que 5 &gt; 3 TRUE &gt;= Mayor o igual que 5 &gt;= 3 TRUE == Exactamente igual que 5 == 3 FALSE != No es igual que 5 != 3 TRUE Es posible comparar cualquier tipo de dato sin que resulte en un error. Sin embargo, al usar los operadores &gt;, &gt;=, &lt; y &lt;= con cadenas de texto, estos tienen un comportamiento especial. Por ejemplo, \"casa\" &gt; \"barco\" nos devuelve TRUE. &quot;casa&quot; &gt; &quot;barco&quot; ## [1] TRUE Este resultado se debe a que se ha hecho una comparación por orden alfabético. En este caso, la palabra casa tendría una posición posterior a barco, pues empieza con c y esta letra tiene una posición posterior a la b en el alfabeto. Por lo tanto, es verdadero que sea mayor. Cuando intentamos comparar factores, siempre obtendremos como resultado NA y una advertencia acerca de que estos operadores no son significativos para datos de tipo factor. as.factor(&quot;casa&quot;) &gt; &quot;barco&quot; ## Warning in Ops.factor(as.factor(&quot;casa&quot;), &quot;barco&quot;): &#39;&gt;&#39; not meaningful for ## factors ## [1] NA "],["5-3-operadores-lógicos.html", "5.3 Operadores lógicos", " 5.3 Operadores lógicos Los operadores lógicos son usados para operaciones de álgebra Booleana, es decir, para describir relaciones lógicas, expresadas como verdadero (TRUE) o falso (FALSO). Operador Comparación Ejemplo Resultado x | y x Ó y es verdadero TRUE | FALSE TRUE x &amp; y x Y y son verdaderos TRUE &amp; FALSE FALSE !x x no es verdadero (negación) !TRUE FALSE isTRUE(x) x es verdadero (afirmación) isTRUE(TRUE) TRUE Los operadores | y &amp; siguen estas reglas: | devuelve TRUE si alguno de los datos es TRUE &amp; solo devuelve TRUE si ambos datos es TRUE | solo devuelve FALSE si ambos datos son FALSE &amp; devuelve FALSE si alguno de los datos es FALSE Estos operadores pueden ser usados con estos con datos de tipo numérico, lógico y complejo. Al igual que con los operadores relacionales, los operadores lógicos siempre devuelven TRUE o FALSE. Para realizar operaciones lógicas, todos los valores numéricos y complejos distintos a 0 son coercionados a TRUE, mientras que 0 siempre es coercionado a FALSE. Por ejemplo, 5 | 0 resulta en TRUE y 5 &amp; FALSE resulta en FALSE. Podemos comprobar lo anterior con la función isTRUE(). 5 | 0 ## [1] TRUE 5 &amp; 0 ## [1] FALSE isTRUE(0) ## [1] FALSE isTRUE(5) ## [1] FALSE Estos operadores se pueden combinar para expresar relaciones complejas. Por ejemplo, la negación FALSE Y FALSE dará como resultado TRUE. !(FALSE | FALSE) ## [1] TRUE También podemos combinar operadores lógicos y relacionales, dado que esto últimos dan como resultado TRUE y FALSE. ## [1] TRUE "],["5-4-operadores-de-asignación.html", "5.4 Operadores de asignación", " 5.4 Operadores de asignación Este es probablemente el operador más importante de todos, pues nos permite asignar datos a variables. Operador Operación &lt;- Asigna un valor a una variable = Asigna un valor a una variable Aunque podemos usar el signo igual para una asignación, a lo largo de este libro utilizaremos &lt;-, por ser característico de R y fácil de reconocer visualmente. Después de realizar la operación de asignación, podemos usar el nombre de la variable para realizar operaciones con ella, como si fuera del tipo de datos que le hemos asignado. Si asignamos un valor a una variable a la que ya habíamos asignado datos, nuestra variable conserva el valor más reciente. Además, esta operación nos permite guardar el resultado de operaciones, de modo que los podemos recuperar sin necesidad de realizar las operaciones otra vez. Basta con llamar el nombre de la variable en la consola En este ejemplo, asignamos valores a las variables estatura y peso. estatura &lt;- 1.73 peso &lt;- 83 Llamamos a sus valores asignados estatura ## [1] 1.73 peso ## [1] 83 Usamos los valores asignados para realizar operaciones. peso / estatura ^ 2 ## [1] 27.7323 Cambiamos el valor de una variable a uno nuevo y realizamos operaciones peso &lt;- 76 peso ## [1] 76 peso / estatura ^ 2 ## [1] 25.39343 estatura &lt;- 1.56 peso &lt;- 48 peso / estatura ^ 2 ## [1] 19.72387 Asignamos el resultado de una operación a una variable nueva. bmi &lt;- peso / estatura ^ 2 bmi ## [1] 19.72387 Como podrás ver, es posible asignar a una variable valores de otra variable o el resultado de operaciones con otras variables. velocidad_inicial &lt;- 110 velocidad_final &lt;- 185 tiempo_inicial &lt;- 0 tiempo_final &lt;- 15 variacion_velocidad &lt;- velocidad_final - velocidad_inicial variacion_tiempo &lt;- tiempo_final - tiempo_inicial variacion_velocidad / variacion_tiempo ## [1] 5 "],["5-5-orden-de-operaciones.html", "5.5 Orden de operaciones", " 5.5 Orden de operaciones En R, al igual que en matemáticas, las operaciones tienen un orden de evaluación definido. Cuanto tenemos varias operaciones ocurriendo al mismo tiempo, en realidad, algunas de ellas son realizadas antes que otras y el resultado de ellas dependerá de este orden. El orden de operaciones incluye a las aritméticas, relacionales, lógicas y de asignación. En la tabla siguiente se presenta el orden en que ocurren las operaciones que hemos revisado en este capítulo. Orden Operadores 1 ^ 2 * / 3 + - 4 &lt; &gt; &lt;= &gt;= == != 5 ! 6 &amp; 7 | 8 &lt;- Si deseamos que una operación ocurra antes que otra, rompiendo este orden de evaluación, usamos paréntesis. Podemos tener paréntesis anidados. "],["6-estructuras-de-datos.html", "Capítulo 6 Estructuras de datos", " Capítulo 6 Estructuras de datos Las estructuras de datos son objetos que contienen datos. Cuando trabajamos con R, lo que estamos haciendo es manipular estas estructuras. Las estructuras tienen diferentes características. Entre ellas, las que distinguen a una estructura de otra son su número de dimensiones y si son homogeneas o hereterogeneas. La siguiente tabla muestra las principales estructuras de control que te encontrarás en R. Dimensiones Homogéneas Heterogéneas 1 Vector Lista 2 Matriz Data frame n Array Adaptado de Wickham (2016). Veamos las características de cada una de ellas. "],["6-1-vectores.html", "6.1 Vectores", " 6.1 Vectores Un vector es la estructura de datos más sencilla en R. Un vector es una colección de uno o más datos del mismo tipo. Todos los vectores tienen tres propiedades: Tipo. Un vector tiene el mismo tipo que los datos que contiene. Si tenemos un vector que contiene datos de tipo numérico, el vector será también de tipo numérico. Los vectores son atómicos, pues sólo pueden contener datos de un sólo tipo, no es posible mezclar datos de tipos diferentes dentro de ellos. Largo. Es el número de elementos que contiene un vector. El largo es la única dimensión que tiene esta estructura de datos. Atributos. Los vectores pueden tener metadatos de muchos tipos, los cuales describen características de los datos que contienen. Todos ellos son incluidos en esta propiedad. En este libro no se usarán vectores con metadatos, por ser una propiedad con usos van más allá del alcance de este libro. Cuando una estructura únicamente puede contener datos de un sólo tipo, como es el caso de los vectores, decimos que es homogénea, pero no implica que necesariamente sea atómica. Regresaremos sobre esto al hablar de matrices y arrays. Como los vectores son la estructura de datos más sencilla de R, datos simples como el número 3, son en realidad vectores. En este caso, un vector de tipo numérico y largo igual a 1. 3 ## [1] 3 Verificamos que el 3 es un vector con la función is.vector(). is.vector(3) ## [1] TRUE Y usamos la función length() para conocer su largo. length(3) ## [1] 1 Lo mismo ocurre con los demás tipos de datos, por ejemplo, con cadenas de texto y datos lógicos. is.vector(&quot;tres&quot;) ## [1] TRUE is.vector(TRUE) ## [1] TRUE 6.1.1 Creación de vectores Creamos vectores usando la función c() (combinar). Llamamos esta función y le damos como argumento los elementos que deseamos combinar en un vector, separados por comas. # Vector numérico c(1, 2, 3, 5, 8, 13) ## [1] 1 2 3 5 8 13 # Vector de cadena de texto c(&quot;arbol&quot;, &quot;casa&quot;, &quot;persona&quot;) ## [1] &quot;arbol&quot; &quot;casa&quot; &quot;persona&quot; # Vector lógico c(TRUE, TRUE, FALSE, FALSE, TRUE) ## [1] TRUE TRUE FALSE FALSE TRUE Si deseamos agregar un elemento a un vector ya existente, podemos hacerlo combinando nuestro vector original con los elementos nuevos y asignando el resultado a nuestro vector original. mi_vector &lt;- c(TRUE, FALSE, TRUE) mi_vector &lt;- c(mi_vector, FALSE) mi_vector ## [1] TRUE FALSE TRUE FALSE Naturalmente, podemos crear vectores que son combinación de vectores. mi_vector_1 &lt;- c(1, 3, 5) mi_vector_2 &lt;- c(2, 4, 6) mi_vector_3 &lt;- c(mi_vector_1, mi_vector_2) mi_vector_3 ## [1] 1 3 5 2 4 6 Si intentamos combinar datos de diferentes tipos en un mismo vector, R realizará coerción automáticamente. El vector resultante será del tipo más flexible entre los datos que contenga, siguiendo las reglas de coerción. Creamos un vector numérico. mi_vector &lt;- c(1, 2, 3) class(mi_vector) ## [1] &quot;numeric&quot; Si intentamos agregar un dato de tipo cadena de texto, nuestro vector ahora será de tipo cadena de texto. mi_vector_nuevo &lt;- c(mi_vector, &quot;a&quot;) class(mi_vector_nuevo) ## [1] &quot;character&quot; Como las cadenas de texto son el tipo de dato más flexible, siempre que creamos un vector que incluye un dato de este tipo, el resultado será un vector de texto. mi_vector_mezcla &lt;- c(FALSE, 2, &quot;tercero&quot;, 4.00) class(mi_vector_mezcla) ## [1] &quot;character&quot; Podemos crear vectores de secuencias numéricas usando :. De un lado de los dos puntos escribimos el número de inicio de la secuencia y del otro el final. Por ejemplo, creamos una secuencia del 1 al 10. 1:10 ## [1] 1 2 3 4 5 6 7 8 9 10 También podemos crear una secuencia del 10 al 1. 10:1 ## [1] 10 9 8 7 6 5 4 3 2 1 Las secuencias creadas con : son consecutivas con incrementos o decrementos de 1. Estas secuencias pueden empezar con cualquier número, incluso si este es negativo o tiene cifras decimales # Número negativo -43:-30 ## [1] -43 -42 -41 -40 -39 -38 -37 -36 -35 -34 -33 -32 -31 -30 # Número con cifras decimales 67.23:75 ## [1] 67.23 68.23 69.23 70.23 71.23 72.23 73.23 74.23 Si nuestro número de inicio tiene cifras decimales, estas serán respetadas al hacer los incrementos o decrementos de uno en uno. En contraste, si es nuestro número de final el que tiene cifras decimales, este será redondeado. # Se conservan los decimales del inicio -2.48:2 ## [1] -2.48 -1.48 -0.48 0.52 1.52 56.007:50 ## [1] 56.007 55.007 54.007 53.007 52.007 51.007 50.007 # Se redondean los decimales del final 166:170.05 ## [1] 166 167 168 169 170 968:960.928 ## [1] 968 967 966 965 964 963 962 961 6.1.2 Vectorización de operaciones Existen algunas operaciones al aplicarlas a un vector, se aplican a cada uno de sus elementos. A este proceso le llamamos vectorización. Las operaciones aritméticas y relacionales pueden vectorizarse. Si las aplicamos a un vector, la operación se realizará para cada uno de los elementos que contiene. Por ejemplo, creamos un vector numérico. mi_vector &lt;- c(2, 3, 6, 7, 8, 10, 11) Si aplicamos operaciones aritméticas, obtenemos un vector con un resultado por cada elemento. # Operaciones aritméticas mi_vector + 2 ## [1] 4 5 8 9 10 12 13 mi_vector * 2 ## [1] 4 6 12 14 16 20 22 mi_vector %% 2 ## [1] 0 1 0 1 0 0 1 Al aplicar operaciones relacionales, obtenemos un vector de TRUEy FALSE, uno para cada elemento comparado. mi_vector &gt; 7 ## [1] FALSE FALSE FALSE FALSE TRUE TRUE TRUE mi_vector &lt; 7 ## [1] TRUE TRUE TRUE FALSE FALSE FALSE FALSE mi_vector == 7 ## [1] FALSE FALSE FALSE TRUE FALSE FALSE FALSE Esta manera de aplicar una operación es muy eficiente. Comparada con otros procedimientos, requiere de menos tiempo de cómputo, lo cual a veces es considerable, en particular cuando trabajamos con un número grande de datos. Aunque el nombre de este proceso es vectorización, también funciona, en ciertas circunstancias, para otras estructuras de datos. "],["6-2-matrices-y-arrays.html", "6.2 Matrices y arrays", " 6.2 Matrices y arrays Las matrices y arrays pueden ser descritas como vectores multidimensionales. Al igual que un vector, únicamente pueden contener datos de un sólo tipo, pero además de largo, tienen más dimensiones. En un sentido estricto, las matrices son una caso especial de un array, que se distingue por tener específicamente dos dimensiones, un largo\" y un alto. Las matrices son, por lo tanto, una estructura con forma rectangular, con renglones y columnas. Como las matrices son usadas de manera regular en matemáticas y estadística, es una estructura de datos de uso común en R común y en la que nos enfocaremos en este libro. Los arrays, por su parte, pueden tener un número arbitrario de dimensiones. Pueden ser cubos, hipercubos y otras formas. Su uso no es muy común en R, aunque a veces es deseable contar con objetos n-dimensionales para manipular datos. Como los arrays tienen la restricción de que todos sus datos deben ser del mismo tipo, no importando en cuántas dimensiones se encuentren, esto limita sus usos prácticos. En general, es preferible usar listas en lugar de arrays, una estructura de datos que además tienen ciertas ventajas que veremos más adelante. 6.2.1 Creación de matrices Creamos matrices en R con la función matrix(). La función matrix() acepta dos argumentos, nrow y ncol. Con ellos especificamos el número de renglones y columnas que tendrá nuestra matriz. # Un vector numérico del uno al doce 1:12 ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 # matrix() sin especificar renglones ni columnas matrix(1:12) ## [,1] ## [1,] 1 ## [2,] 2 ## [3,] 3 ## [4,] 4 ## [5,] 5 ## [6,] 6 ## [7,] 7 ## [8,] 8 ## [9,] 9 ## [10,] 10 ## [11,] 11 ## [12,] 12 # Tres renglones y cuatro columnas matrix(1:12, nrow = 3, ncol = 4) ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 # Cuatro columnas y tres columnas matrix(1:12, nrow = 4, ncol = 3) ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12 # Dos renglones y seis columnas matrix(1:12, nrow = 4, ncol = 3) ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12 Los datos que intentemos agrupar en una matriz serán acomodados en orden, de arriba a abajo, y de izquierda a derecha, hasta formar un rectángulo. Si multiplicamos el número de renglones por el número de columnas, obtendremos el número de celdas de la matriz. En los ejemplo anteriores, el número de celdas es igual al número de elementos que queremos acomodar, así que la operación ocurre sin problemas. Cuando intentamos acomodar un número diferente de elementos y celdas, ocurren dos cosas diferentes. Si el número de elementos es mayor al número de celdas, se acomodarán todos los datos que sean posibles y los demás se omitirán. matrix(1:12, nrow = 3, ncol = 3) ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 Si, por el contrario, el número de celdas es mayor que el número de elementos, estos se reciclaran. En cuanto los elementos sean insuficientes para acomodarse en las celdas, R nos devolverá una advertencia y se empezaran a usar los elementos a partir del primero de ellos matrix(1:12, nrow = 5, ncol = 4) ## Warning in matrix(1:12, nrow = 5, ncol = 4): la longitud de los datos [12] no es ## un submúltiplo o múltiplo del número de filas [5] en la matriz ## [,1] [,2] [,3] [,4] ## [1,] 1 6 11 4 ## [2,] 2 7 12 5 ## [3,] 3 8 1 6 ## [4,] 4 9 2 7 ## [5,] 5 10 3 8 Otro procedimiento para crear matrices es la unión vectores con las siguientes funciones: cbind() para unir vectores, usando cada uno como una columna. rbind() para unir vectores, usando cada uno como un renglón. De este modo podemos crear cuatro vectores y unirlos para formar una matriz. Cada vector será un renglón en esta matriz. Creamos cuatro vectores, cada uno de largo igual a cuatro. vector_1 &lt;- 1:4 vector_2 &lt;- 5:8 vector_3 &lt;- 9:12 vector_4 &lt;- 13:16 Usamos rbind() para crear un matriz, en la que cada vector será un renglón. matriz &lt;- rbind(vector_1, vector_2, vector_3, vector_4) # Resultado matriz ## [,1] [,2] [,3] [,4] ## vector_1 1 2 3 4 ## vector_2 5 6 7 8 ## vector_3 9 10 11 12 ## vector_4 13 14 15 16 Si utilizamos cbind(), entonces cada vector será una columna. matriz &lt;- cbind(vector_1, vector_2, vector_3, vector_4) # Resultado matriz ## vector_1 vector_2 vector_3 vector_4 ## [1,] 1 5 9 13 ## [2,] 2 6 10 14 ## [3,] 3 7 11 15 ## [4,] 4 8 12 16 Al igual que con matrix(), los elementos de los vectores son reciclados para formar una estructura rectangular y se nos muestra un mensaje de advertencia. # Elementos de largo diferente vector_1 &lt;- 1:2 vector_2 &lt;- 1:3 vector_3 &lt;- 1:5 matriz &lt;- cbind(vector_1, vector_2, vector_3) ## Warning in cbind(vector_1, vector_2, vector_3): number of rows of result is not ## a multiple of vector length (arg 1) # Resultado matriz ## vector_1 vector_2 vector_3 ## [1,] 1 1 1 ## [2,] 2 2 2 ## [3,] 1 3 3 ## [4,] 2 1 4 ## [5,] 1 2 5 Finalmente, las matrices pueden contener NAs. Creamos dos vectores con un NA en ellos. vector_1 &lt;- c(NA, 1, 2) vector_2 &lt;- c(3, 4, NA) Creamos una matriz con rbind(). matriz &lt;- rbind(vector_1, vector_2) # Resultados matriz ## [,1] [,2] [,3] ## vector_1 NA 1 2 ## vector_2 3 4 NA Como NA representa datos perdidos, puede estar presente en compañía de todo tipo de de datos. 6.2.2 Propiedades de las matrices No obstante que las matrices y arrays son estructuras que sólo pueden contener un tipo de datos, no son atómicas. Su clase es igual a matriz (matrix) o array según corresponda. Verificamos esto usando la función class(). mi_matriz &lt;- matrix(1:10) class(mi_matriz) ## [1] &quot;matrix&quot; &quot;array&quot; Las matrices y arrays pueden tener más de una dimensión. Obtenemos el número de dimensiones de una matriz o array con la función dim(). Esta función nos devolverá varios números, cada uno de ellos indica la cantidad de elementos que tiene una dimensión. mi_matriz &lt;- matrix(1:12, nrow = 4, ncol = 3) dim(mi_matriz) ## [1] 4 3 Cabe señalar que si usamos dim() con un vector, obtenemos NULL. Esto ocurre con todos los objetos unidimensionales mi_vector &lt;- 1:12 dim(mi_vector) ## NULL Finalmente, las operaciones aritméticas también son vectorizadas al aplicarlas a una matriz. La operación es aplicada a cada uno de los elementos de la matriz. Creamos una matriz. mi_matriz &lt;- matrix(1:9, nrow = 3, ncol = 3) # Resultado mi_matriz ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 Intentemos sumar, multiplicar y elevar a la tercera potencia. # Suma mi_matriz + 1 ## [,1] [,2] [,3] ## [1,] 2 5 8 ## [2,] 3 6 9 ## [3,] 4 7 10 # Multiplicación mi_matriz * 2 ## [,1] [,2] [,3] ## [1,] 2 8 14 ## [2,] 4 10 16 ## [3,] 6 12 18 # Potenciación mi_matriz ^ 3 ## [,1] [,2] [,3] ## [1,] 1 64 343 ## [2,] 8 125 512 ## [3,] 27 216 729 Si intentamos vectorizar una operación utilizando una matriz con NAs, esta se aplicará para los elementos válidos, devolviendo NA cuando corresponda. Creamos una matriz con NAs. vector_1 &lt;- c(NA, 2, 3) vector_2 &lt;- c(4, 5, NA) matriz &lt;- rbind(vector_1, vector_2) # Resultado matriz ## [,1] [,2] [,3] ## vector_1 NA 2 3 ## vector_2 4 5 NA Intentamos dividir sus elementos entre dos. matriz / 2 ## [,1] [,2] [,3] ## vector_1 NA 1.0 1.5 ## vector_2 2 2.5 NA Finalmente, podemos usar la función t() para transponer una matriz, es decir, rotarla 90°. Creamos una matriz con tres renglones y dos columnas. matriz &lt;- matrix(1:6, nrow = 3) # Resultado matriz ## [,1] [,2] ## [1,] 1 4 ## [2,] 2 5 ## [3,] 3 6 Usamos t() para transponer. matriz_t &lt;- t(matriz) # Resultado matriz_t ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 Obtenemos una matriz con dos renglones y dos columnas. "],["6-3-data-frames.html", "6.3 Data frames", " 6.3 Data frames Los data frames son estructuras de datos de dos dimensiones (rectangulares) que pueden contener datos de diferentes tipos, por lo tanto, son heterogéneas. Esta estructura de datos es la más usada para realizar análisis de datos y seguro te resultará familiar si has trabajado con otros paquetes estadísticos. Podemos entender a los data frames como una versión más flexible de una matriz. Mientras que en una matriz todas las celdas deben contener datos del mismo tipo, los renglones de un data frame admiten datos de distintos tipos, pero sus columnas conservan la restricción de contener datos de un sólo tipo. En términos generales, los renglones en un data frame representan casos, individuos u observaciones, mientras que las columnas representan atributos, rasgos o variables. Por ejemplo, así lucen los primeros cinco renglones del objeto iris, el famoso conjunto de datos Iris de Ronald Fisher, que está incluido en todas las instalaciones de R. ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa Los primeros cinco renglones corresponden a cinco casos, en este caso flores. Las columnas son variables con los rasgos de cada flor: largo y ancho de sépalo, largo y ancho de pétalo, y especie. Para crear un data frame usamos la función data.frame(). Esta función nos pedirá un número de vectores igual al número de columnas que deseemos. Todos los vectores que proporcionemos deben tener el mismo largo. Esto es muy importante: Un data frame está compuesto por vectores. Más adelante se hará evidente porque esta característica de un data frame es sumamente importante y también, cómo podemos sacarle provecho. Además, podemos asignar un nombre a cada vector, que se convertirá en el nombre de la columna. Como todos los nombres, es recomendable que este sea claro, no ambiguo y descriptivo. mi_df &lt;- data.frame( &quot;entero&quot; = 1:4, &quot;factor&quot; = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;), &quot;numero&quot; = c(1.2, 3.4, 4.5, 5.6), &quot;cadena&quot; = as.character(c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;)) ) mi_df ## entero factor numero cadena ## 1 1 a 1.2 a ## 2 2 b 3.4 b ## 3 3 c 4.5 c ## 4 4 d 5.6 d # Podemos usar dim() en un data frame dim(mi_df) ## [1] 4 4 # El largo de un data frame es igual a su número de columnas length(mi_df) ## [1] 4 # names() nos permite ver los nombres de las columnas names(mi_df) ## [1] &quot;entero&quot; &quot;factor&quot; &quot;numero&quot; &quot;cadena&quot; # La clase de un data frame es data.frame class(data.frame) ## [1] &quot;function&quot; Si los vectores que usamos para construir el data frame no son del mismo largo, los datos no se reciclaran. Se nos devolverá un error. data.frame( &quot;entero&quot; = 1:3, &quot;factor&quot; = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;), &quot;numero&quot; = c(1.2, 3.4, 4.5, 5.6), &quot;cadena&quot; = as.character(c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;)) ) ## Error in data.frame(entero = 1:3, factor = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;), numero = c(1.2, : arguments imply differing number of rows: 3, 4 También podemos coercionar esta matriz a un data frame. Creamos una matriz. matriz &lt;- matrix(1:12, ncol = 4) Usamos as.data.frame() para coercionar una matriz a un data frame. df &lt;- as.data.frame(matriz) Verificamos el resultado class(df) ## [1] &quot;data.frame&quot; # Resultado df ## V1 V2 V3 V4 ## 1 1 4 7 10 ## 2 2 5 8 11 ## 3 3 6 9 12 6.3.1 Propiedades de un data frame Al igual que con una matriz, si aplicamos una operación aritmética a un data frame, esta se vectorizará. Los resultados que obtendremos dependerán del tipo de datos de cada columna. R nos devolverá todas las advertencias que ocurran como resultado de las operaciones realizadas, por ejemplo, aquellas que hayan requerido una coerción. mi_df &lt;- data.frame( &quot;entero&quot; = 1:4, &quot;factor&quot; = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;), &quot;numero&quot; = c(1.2, 3.4, 4.5, 5.6), &quot;cadena&quot; = as.character(c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;)) ) # mi_df * 2 "],["6-4-listas.html", "6.4 Listas", " 6.4 Listas Las listas, al igual que los vectores, son estructuras de datos unidimensionales, sólo tienen largo, pero a diferencia de los vectores cada uno de sus elementos puede ser de diferente tipo o incluso de diferente clase, por lo que son estructuras heterogéneas. Podemos tener listas que contengan datos atómicos, vectores, matrices, arrays, data frames u otras listas. Esta última característica es la razón por la que una lista puede ser considerada un vector recursivo, pues es un objeto que puede contener objetos de su misma clase. Para crear una lista usamos la función list(), que nos pedirá los elementos que deseamos incluir en nuestra lista. Para esta estructura, no importan las dimensiones o largo de los elementos que queramos incluir en ella. Al igual que con un data frame, tenemos la opción de poner nombre a cada elemento de una lista. Por último, no es posible vectorizar operaciones aritméticas usando una lista, se nos devuelve un error como resultado. mi_vector &lt;- 1:10 mi_matriz &lt;- matrix(1:4, nrow = 2) mi_df &lt;- data.frame(&quot;num&quot; = 1:3, &quot;let&quot; = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)) mi_lista &lt;- list(&quot;un_vector&quot; = mi_vector, &quot;una_matriz&quot; = mi_matriz, &quot;un_df&quot; = mi_df) mi_lista ## $un_vector ## [1] 1 2 3 4 5 6 7 8 9 10 ## ## $una_matriz ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 ## ## $un_df ## num let ## 1 1 a ## 2 2 b ## 3 3 c Creamos una lista que contiene otras listas. lista_recursiva &lt;- list(&quot;lista1&quot; = mi_lista, &quot;lista2&quot; = mi_lista) # Resultado lista_recursiva ## $lista1 ## $lista1$un_vector ## [1] 1 2 3 4 5 6 7 8 9 10 ## ## $lista1$una_matriz ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 ## ## $lista1$un_df ## num let ## 1 1 a ## 2 2 b ## 3 3 c ## ## ## $lista2 ## $lista2$un_vector ## [1] 1 2 3 4 5 6 7 8 9 10 ## ## $lista2$una_matriz ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 ## ## $lista2$un_df ## num let ## 1 1 a ## 2 2 b ## 3 3 c 6.4.1 Propiedades de una lista Una lista es unidimensional, sólo tiene largo. El largo de una lista es igual al número de elementos que contiene, sin importar de qué tipo o clase sean. Usamos la lista recursiva que creamos en la sección anterior para ilustrar esto. length(lista_recursiva) ## [1] 2 Dado que una lista siempre tiene una sola dimensión, la función dim() nos devuelve NULL. dim(lista_recursiva) ## NULL Las listas tienen clase list, sin importar qué elementos contienen. class(lista_recursiva) ## [1] &quot;list&quot; Finalmente, no es posible vectorizar operaciones aritméticas usando listas. Al intentarlo nos es devuelto un error. mi_lista / 2 ## Error in mi_lista/2: argumento no-numérico para operador binario Si deseamos aplicar una función a cada elemento de una lista, usamos lapply(), como veremos en el capítulo 10. "],["6-5-coerción-1.html", "6.5 Coerción", " 6.5 Coerción Al igual que con los datos, cuando intentamos hacer operaciones con una estructura de datos, R intenta coercionarla al tipo apropiado para poder llevarlas a cabo con éxito. También podemos usar alguna de las funciones de la familia as() coercionar de un tipo de estructura de datos. A continuación se presentan las más comunes. Función Coerciona a Coerciona exitosamente a as.vector() Vector Matrices as.matrix() Matrices Vectores, Data frames as.data.frame() Data frame Vectores, Matrices as.list() Lista Vectores, Matrices, Data frames Como podrás ver, las estructuras de datos más sencillas, (unidimensionales, homogéneas) pueden ser coercionadas a otras más complejas (multidimensionales, heterogéneas), pero la operación inversa casi nunca es posible. Veamos algunos ejemplos. Creamos un vector, una matriz, un data frame y una lista. mi_vector &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) mi_matriz &lt;- matrix(1:4, nrow = 2) mi_df &lt;- data.frame(&quot;a&quot; = 1:2, &quot;b&quot; = c(&quot;a&quot;, &quot;b&quot;)) mi_lista &lt;- list(&quot;a&quot; = mi_vector, &quot;b&quot; = mi_matriz, &quot;c&quot; = mi_df) Intentemos coercionar a vector con as.vector(). as.vector(mi_matriz) ## [1] 1 2 3 4 as.vector(mi_df) ## a b ## 1 1 a ## 2 2 b as.vector(mi_lista) ## $a ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; ## ## $b ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 ## ## $c ## a b ## 1 1 a ## 2 2 b La coerción que intentamos sólo tuvo éxito para una matriz. Para data frame y lista, nos devolvió los mismos objetos. Nota que as.vector() no devolvió un error o una advertencia a pesar de que no tuvo éxito al coercionar, en este caso un data frame o una lista. Esto es importante, pues no puedes confiar que as.vector() tuvo éxito porque corrió sin errores, es necesaria una verificación adicional. Como R intenta coercionar automáticamente, esto puede producir resultados inesperados si no tenemos cuidado. Intentemos coercionar a matriz con as.matrix(). as.matrix(mi_vector) ## [,1] ## [1,] &quot;a&quot; ## [2,] &quot;b&quot; ## [3,] &quot;c&quot; as.matrix(mi_df) ## a b ## [1,] &quot;1&quot; &quot;a&quot; ## [2,] &quot;2&quot; &quot;b&quot; as.matrix(mi_lista) ## [,1] ## a character,3 ## b integer,4 ## c data.frame,2 El vector fue coercionado a una matriz con una sola columna. Por su parte, al correr la función con un data frame, coercionamos también todos los datos que contiene, siguiendo las reglas de coerción de tipos de dato que vimos en el capítulo 4. Al coercionar una lista a una matriz, efectivamente obtenemos un objeto de este tipo, sin embargo perdemos toda la información que contiene, por lo tanto, no podemos considerar que esta es una coerción exitosa. Del mismo modo que con as.vector(), no nos es mostrado ningún error ni advertencia. Intentemos coercionar a matriz con as.data.frame(). as.data.frame(mi_vector) ## mi_vector ## 1 a ## 2 b ## 3 c as.data.frame(mi_matriz) ## V1 V2 ## 1 1 3 ## 2 2 4 as.data.frame(mi_lista) ## Error in (function (..., row.names = NULL, check.rows = FALSE, check.names = TRUE, : arguments imply differing number of rows: 3, 2 Tuvimos éxito al coercionar vectores y matrices. El vector, al igual que cuando fue coercionado a matriz, devolvió como resultado un objeto con una sola columna, mientras que la matriz conservó sus renglones y columnas. En este caso, al intentar la coerción de lista a data frame, obtenemos un error. Esta es la única situación en la que esto ocurre utilizando las funciones revisadas en esta sección. Por último, intentemos coercionar a matriz con as.list(). as.list(mi_vector) ## [[1]] ## [1] &quot;a&quot; ## ## [[2]] ## [1] &quot;b&quot; ## ## [[3]] ## [1] &quot;c&quot; as.list(mi_matriz) ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 2 ## ## [[3]] ## [1] 3 ## ## [[4]] ## [1] 4 as.list(mi_df) ## $a ## [1] 1 2 ## ## $b ## [1] &quot;a&quot; &quot;b&quot; Dado que las listas son el tipo de objeto más flexible de todos, hemos tenido éxito en todos los casos con nuestra coerción. Nota que para los vectores y matrices, cada uno de los elementos es transformado en un elemento dentro de la lista resultante. Si tuviéramos una matriz con cuarenta y ocho celdas, obtendríamos una lista con ese mismo número de elementos. En cambio, para un data frame, el resultado es una lista, en la que cada elemento contiene los datos de una columna del data frame original. Un data frame con diez columnas resultará en una lista de diez elementos. Conocer cómo ocurre la coerción de estructuras de datos te ayudará a entender mejor algunos resultados devueltos por funciones de R, además de que te facilitará la manipulación y procesamiento de datos. "],["7-subconjuntos.html", "Capítulo 7 Subconjuntos", " Capítulo 7 Subconjuntos En R, podemos obtener subconjuntos de nuestras estructuras de datos. Es decir, podemos extraer partes de una estructura de datos (nuestro conjunto). Hacemos esto para seleccionar datos que tienen características específicas, por ejemplo, todos los valores mayores a cierto número o aquellos que coinciden exactamente con un valor de nuestro interés. Para realizar esta operación haremos uso de índices, operadores lógicos y álgebra Booleana. Algunos procedimientos para obtener subconjuntos pueden usarse con cualquier estructura de datos, mientras que otras sólo funcionan con algunas de ellas. En este capítulo revisaremos cómo extraer subconjuntos de vectores, matrices, data frames y listas, usando índices, nombres y condicionales. "],["7-1-índices.html", "7.1 Índices", " 7.1 Índices Usar índices para obtener subconjuntos es el procedimiento más universal en R, pues funciona para todas las estructuras de datos. Un índice en R representa una posición. Cuando usamos índices le pedimos a R que extraiga de una estructura los datos que se encuentran en una o varias posiciones específicas dentro de ella. A diferencia de la mayoría de los lenguajes de programación, los índices en R empiezan en 1, no en 0. El índice del primer elemento de una estructura de datos siempre es 1, el segundo 2, y así sucesivamente. Un aspecto muy importante de este procedimiento es que, para data frames y listas, cuando extraemos un subconjunto de un objeto usando corchetes, obtenemos como resultado un objeto de la misma clase que el objeto original. Si extraemos un subconjunto de un data frame, obtenemos un vector; y si extraemos de una lista, obtenemos una lista. El uso de índices tiene además otras características particulares para las distintas estructuras de datos, así que veremos este procedimiento para cada una de ellas. 7.1.1 Vectores Empecemos creando un vector que contiene los nombres de distintos niveles educativos. nivel &lt;- c(&quot;Preescolar&quot;, &quot;Primaria&quot;, &quot;Secundaria&quot;, &quot;Educación Media Superior&quot;, &quot;Educación Superior&quot;) nivel ## [1] &quot;Preescolar&quot; &quot;Primaria&quot; ## [3] &quot;Secundaria&quot; &quot;Educación Media Superior&quot; ## [5] &quot;Educación Superior&quot; Este es un vector de largo igual a cinco. length(nivel) ## [1] 5 ¿Cómo obtendríamos el tercer elemento de este vector usando índices? ¿O del primer al cuarto elemento? ¿O el segundo y quinto elemento? Para obtener subconjuntos con índices escribimos corchetes [] después del nombre de un objeto. Dentro de los corchetes escribimos el o los números que corresponden a la posición que nos interesa extraer del objeto. Por ejemplo: objeto[3] lista[4:5] dataframe[c(2, 7), ] Entonces, para extraer el tercer elemento de nuestro vector nivel hacemos lo siguiente. nivel[3] ## [1] &quot;Secundaria&quot; Para extraer del primer al cuarto elemento de un vector, usamos un vector con una secuencia numérica del 1 al 4 creada con :. nivel[1:4] ## [1] &quot;Preescolar&quot; &quot;Primaria&quot; ## [3] &quot;Secundaria&quot; &quot;Educación Media Superior&quot; Sin embargo, si intentamos extraer el segundo y quinto elemento del vector nivel corriendo lo siguiente, obtendremos un error. nivel[2, 5] ## Error in nivel[2, 5]: número incorreto de dimensiones ¿Porqué no ha funcionado lo anterior? El mensaje de error nos da una pista muy importante. Al usar una coma dentro de los corchetes estamos dando la instrucción a R de buscar los índices solicitados en más de una dimensión. El número antes de la coma será buscado en la primera dimensión del objeto, y el segundo número, en su segunda dimensión. Entonces, al llamar nivel[2, 5], lo que estamos pidiendo es que R extraiga el elemento que se encuentra en la posición 2 de la primera dimensión del vector, y el elemento en la posición 5 de su segunda dimensión. Como los vectores son unidimensionales, es imposible cumplir esta instrucción y se produce un error. Recuerda que en R, un número sencillo es también un vector, por lo tanto, cuando escribimos vector[3], en realidad estamos dando como índice un vector que contiene al número 3. Por lo tanto, si deseamos extraer elementos en posiciones no consecutivas, debemos usar vectores generados con c(). De este modo damos un vector, de más de un número de largo al corchete, pero todos se encuentran en una misma dimensión. Aplicando lo anterior, si escribimos dentro de los corchetes c(2, 5), entonces tendremos éxito al extraer el segundo y quinto elemento de nivel. nivel[c(2, 5)] ## [1] &quot;Primaria&quot; &quot;Educación Superior&quot; Para estructuras de dos dimensiones, como son matrices y data frames, el primer vector de un índice, antes de una coma, es la posición en los renglones y el segundo es la posición las columnas. Obtener subconjuntos por renglones y columnas es un tipo de operación muy común al trabajar con data frames y matrices. 7.1.2 Data frames Creamos un data frame llamado mi_df. mi_df &lt;- data.frame(&quot;nombre&quot; = c(&quot;Armando&quot;, &quot;Elsa&quot;, &quot;Ignacio&quot;, &quot;Olga&quot;), &quot;edad&quot; = c(20, 24, 22, 30), &quot;sexo&quot; = c(&quot;H&quot;, &quot;M&quot;, &quot;M&quot;, &quot;H&quot;), &quot;grupo&quot; = c(0, 1, 1, 0)) # Resultado mi_df ## nombre edad sexo grupo ## 1 Armando 20 H 0 ## 2 Elsa 24 M 1 ## 3 Ignacio 22 M 1 ## 4 Olga 30 H 0 Usamos dim() para confirmar que nuestro objeto tiene dos dimensiones: tres renglones y tres columnas. dim(mi_df) ## [1] 4 4 Si usamos un índice con un sólo número, extraemos una columna completa, con todos sus renglones. mi_df[1] ## nombre ## 1 Armando ## 2 Elsa ## 3 Ignacio ## 4 Olga Si usamos un vector, sin comas, obtenemos varias columnas. mi_df[c(1, 3)] ## nombre sexo ## 1 Armando H ## 2 Elsa M ## 3 Ignacio M ## 4 Olga H Al usar comas, el vector antes de la coma nos devolverá un renglón completo. mi_df[3,] ## nombre edad sexo grupo ## 3 Ignacio 22 M 1 Nota que si dejamos vació el espacio después de la coma, se nos devuelven todas las columnas del data frame. Si el espacio que dejamos vacío es el que se encuentra después de la coma, obtenemos columnas. Esto es equivalente a usar un solo vector dentro de los corchetes. mi_df[ ,1] ## [1] &quot;Armando&quot; &quot;Elsa&quot; &quot;Ignacio&quot; &quot;Olga&quot; Al combinar índices para renglones y columnas, obtenemos los datos que se encuentran en una posición específica. Por ejemplo, el dato en el tercer renglón y la tercer columna. mi_df[3, 3] ## [1] &quot;M&quot; El segundo y tercer dato de la tercera columna. mi_df[2:3, 3] ## [1] &quot;M&quot; &quot;M&quot; El cuarto renglón de la tercera y cuarta columna. mi_df[4, 3:4] ## sexo grupo ## 4 H 0 También podemos usar vectores de más de un número. Por ejemplo, los datos en en el primer y segundo renglón, y en la segunda y cuarta columna. mi_df[1:2, c(2, 4)] ## edad grupo ## 1 20 0 ## 2 24 1 Por último, en todos los casos anteriores, hemos obtenido como resultado un data frame. sub_df &lt;- mi_df[1:2, c(2, 4)] class(sub_df) ## [1] &quot;data.frame&quot; Si damos un índice inválido para las columnas, es decir, un número de columna que no exista, se nos devuelve un renglón. Intentemos obtener la séptima columna de mi_df. mi_df[7] ## Error in `[.data.frame`(mi_df, 7): undefined columns selected Sin embargo, para los renglones simplemente se nos devuelve NA. mi_df[7, ] ## nombre edad sexo grupo ## NA &lt;NA&gt; NA &lt;NA&gt; NA 7.1.3 Matrices El procedimiento anterior para data frames funciona de la misma manera para las matrices, con una excepción. Si usamos como índice un sólo número, entonces obtendremos el valor que se encuentre en esa posición, contando celdas de arriba a abajo y de izquierda a derecha. Creamos una matriz con 4 renglones y dos columnas. mi_matriz &lt;- matrix(1:8, nrow = 4) # Resultado mi_matriz ## [,1] [,2] ## [1,] 1 5 ## [2,] 2 6 ## [3,] 3 7 ## [4,] 4 8 Si damos como índice el número 8, R no intentará devolvernos la octava columna de la matriz, sino la octava celda. mi_matriz[8] ## [1] 8 Fuera de este caso, los índices de renglones y columna tienen el mismo comportamiento que en un data frame. # Tercer renglón mi_matriz[3, ] ## [1] 3 7 # Segunda columna mi_matriz[ ,2] ## [1] 5 6 7 8 # Tercer renglón y segunda columna mi_matriz[3, 2] ## [1] 7 Nota que en este caso obtenemos vectores al extraer un subconjunto. 7.1.4 Arrays Para objetos de tres o más dimensiones se siguen las mismas reglas que con las matrices, aunque ya no es tan fácil hablar de renglones y columnas. Creamos un array de cuatro dimensiones. mi_array &lt;- array(data = 1:16, dim = c(2, 2, 2, 2)) Veamos nuestro resultado y comprobamos con dim() su número de dimensiones. mi_array ## , , 1, 1 ## ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 ## ## , , 2, 1 ## ## [,1] [,2] ## [1,] 5 7 ## [2,] 6 8 ## ## , , 1, 2 ## ## [,1] [,2] ## [1,] 9 11 ## [2,] 10 12 ## ## , , 2, 2 ## ## [,1] [,2] ## [1,] 13 15 ## [2,] 14 16 # Comprobamos el número de dimensiones de nuestro objeto dim(mi_array) ## [1] 2 2 2 2 Intentemos extraer varios subconjuntos, sólo para ilustrar cómo funcionan los índices con arrays. mi_array[1, , , ] ## , , 1 ## ## [,1] [,2] ## [1,] 1 5 ## [2,] 3 7 ## ## , , 2 ## ## [,1] [,2] ## [1,] 9 13 ## [2,] 11 15 mi_array[1, 2, , ] ## [,1] [,2] ## [1,] 3 11 ## [2,] 7 15 mi_array[1, 2, 1, ] ## [1] 3 11 mi_array[1, 2, 1, 2] ## [1] 11 Nota que como resultados obtenemos matrices, a menos que hagamos una extraigamos el contenido de una sola celda. "],["7-2-nombres.html", "7.2 Nombres", " 7.2 Nombres Un segundo método para extraer subconjuntos es usar los nombres de los elementos en una estructura de datos. Este forma de obtener subconjuntos es usada principalmente con data frames y listas. De manera similar a los índices, usamos corchetes cuadrados [] después del nombre de un objeto, pero en lugar de escribir un número, escribimos el nombre del elemento que deseamos extraer como una cadena de texto, es decir, entre comillas. 7.2.1 Data frames Los elementos de un data frame son sus columnas y cada una de ellas tiene un nombre, lo que estamos pidiendo a R con este método es que nos devuelva los elementos cuyo nombre coincida con el que hemos proporcionado Para mostrar el uso de este método, utilizaremos el mismo data frame que en la sección anterior. Si escribimos entre corchetes nombre, obtendremos toda la columna nombre. mi_df[&quot;nombre&quot;] ## nombre ## 1 Armando ## 2 Elsa ## 3 Ignacio ## 4 Olga Al escribir grupo, nos es devuelta toda la columna con ese nombre. mi_df[&quot;grupo&quot;] ## grupo ## 1 0 ## 2 1 ## 3 1 ## 4 0 De igual manera que con los índices, al escribir una coma dentro de los corchetes, estamos pidiendo con ello extraer elementos en más de una dimensión. Lo que está antes escrito antes de la coma corresponde a renglones, y lo que está después, a columnas. Si ejecutamos lo siguiente, obtendremos NA en lugar de obtener las columnas edad y sexo. mi_df[&quot;edad&quot;, &quot;sexo&quot;] ## [1] NA Lo anterior ocurre porque R intenta encontrar un renglón llamado edad y una columna llamada sexo, al no encontrarlas, nos devuelve NA. Recuerda que aunque no es lo más común, los renglones de un data frame pueden tener nombres. Al igual que con los índices, si damos el nombre de un renglón que existe, obtenemos NA. Es sólo al solicitar un nombre de columna no válido que se nos devuelve un error. Pedimos un nombre de renglón inexistente y obtenemos NA. mi_df[&quot;localidad&quot;, ] ## nombre edad sexo grupo ## NA &lt;NA&gt; NA &lt;NA&gt; NA Pero si pedimos un nombre inválido de columna, nos es devuelto un error. mi_df[, &quot;localidad&quot;] ## Error in `[.data.frame`(mi_df, , &quot;localidad&quot;): undefined columns selected Para extraer más de una columna, escribimos un vector de texto entre los corchetes. Por ejemplo mi_df[c(&quot;edad&quot;, &quot;sexo&quot;)] ## edad sexo ## 1 20 H ## 2 24 M ## 3 22 M ## 4 30 H Además, las columnas son devueltas en el orden que las pedimos, lo cual es conveniente cuando estamos procesando y recodificando datos. mi_df[c(&quot;sexo&quot;, &quot;edad&quot;)] ## sexo edad ## 1 H 20 ## 2 M 24 ## 3 M 22 ## 4 H 30 7.2.2 Listas Para una lista, el procedimiento es prácticamente el mismo que para un data frame, pero en lugar de obtener columnas, obtenemos los elementos contenidos en ella. La primera diferencia con los data frame es que, dado que las listas son unidimensionales, si usamos una coma dentro de los corchetes, nos será devuelto un error. Creamos una lista llamada mi_lista. mi_lista &lt;- list(&quot;uno&quot; = 1, &quot;dos&quot; = &quot;2&quot;, &quot;tres&quot; = as.factor(3), &quot;cuatro&quot; = matrix(1:4, nrow = 2)) Intentamos obtener un subconjunto con una coma. mi_lista[&quot;uno&quot;, &quot;dos&quot;] ## Error in mi_lista[&quot;uno&quot;, &quot;dos&quot;]: número incorreto de dimensiones Si pedimos un nombre que no existe en la lista, se nos devuelve NULL en lugar de un error. mi_lista[&quot;cinco&quot;] ## $&lt;NA&gt; ## NULL Para todo lo demás, los nombres tienen el mismo comportamiento que para los data frames. Extremos un elemento de la lista. mi_lista[&quot;dos&quot;] ## $dos ## [1] &quot;2&quot; Extraemos más de un elemento de la lista. mi_lista[c(&quot;cuatro&quot;, &quot;tres&quot;)] ## $cuatro ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 ## ## $tres ## [1] 3 ## Levels: 3 "],["7-3-subconjuntos-por-índice-y-nombre.html", "7.3 Subconjuntos por índice y nombre", " 7.3 Subconjuntos por índice y nombre Al extraer subconjuntos podemos combinar índices con nombres dentro del mismo corchete para objetos multidimensionales, por ejemplo, usando un índice antes de la coma y un nombre después de ella. Esto nos da una gran flexibilidad para hacer subconjuntos con data frames y matrices. En particular, es útil al definir funciones y al trabajar con conjuntos de datos de los tenemos información incompleta. Por ejemplo, extraemos el tercer y cuarto renglón de la columna nombre en nuestro data frame mi_df. mi_df[3:4, &quot;nombre&quot;] ## [1] &quot;Ignacio&quot; &quot;Olga&quot; También podemos usar vectores dentro de los corchetes. Extraemos los renglones con los nombres 48 y 100 de las primera y cuarta columna de iris. iris[c(&quot;48&quot;, &quot;100&quot;), c(1, 4)] ## Sepal.Length Petal.Width ## 48 4.6 0.2 ## 100 5.7 1.3 "],["7-4-el-signo-de-dolar-y-los-corchetes-dobles.html", "7.4 El signo de dolar $ y los corchetes dobles [[]]", " 7.4 El signo de dolar $ y los corchetes dobles [[]] Otra manera en la que podemos extraer subconjuntos usando nombres, es con el signo de dólar $. Para usar este método, escribir el signo $ después del nombre de un objeto de la siguiente forma: objeto$nombre. Este método permite extraer un sólo elemento a la vez, funciona para data frames y listas, y para el caso de los data frame, el elemento extraído siempre será una columna. Por ejemplo, extraemos la columna nombre del data frame mi_df. mi_df$nombre ## [1] &quot;Armando&quot; &quot;Elsa&quot; &quot;Ignacio&quot; &quot;Olga&quot; También podemos escribir el nombre del elemento que deseamos entre comillas, esto es útil si el nombre tiene espacios. mi_df$&quot;nombre&quot; ## [1] &quot;Armando&quot; &quot;Elsa&quot; &quot;Ignacio&quot; &quot;Olga&quot; Si intentamos dar más de un nombre después del signo $, nos es devuelto un error. mi_df$c(&quot;nombre&quot;, &quot;edad&quot;) El resultado de las operaciones anteriores no es un data frame, sino un vector. class(mi_df$nombre) ## [1] &quot;character&quot; Esta es una característica distintiva de este método, al usar el signo $ para extraer un elemento de un data frame o una lista, obtenemos un objeto de la clase que ese elemento era originalmente. Recuerda que un data frame está formado por vectores. Como vimos en el capítulo 6, una manera de generar data frames es combinar vectores. Estos vectores, aunque estén contenidos dentro de un data frame, conservan todas las características de un vector y es posible extraerlos como tales. Cuando usamos el signo $, le pedimos a R que extraiga de un objeto un subconjunto con sus propiedades originales. Por esta razón, para los data frame, siempre son devueltos vectores, mientras que para las listas lo que obtenemos depende del tipo de objeto contenido en ellas. Por ejemplo, si extraemos el elemento uno de mi_lista usando corchetes, obtenemos una lista. class(mi_lista[&quot;uno&quot;]) ## [1] &quot;list&quot; Pero si usamos el signo $, el resultado es un vector numérico, pues esta es su clase original. class(mi_lista$uno) ## [1] &quot;numeric&quot; Si intentamos extraer el elemento cuatro, obtendremos una matriz. class(mi_lista$&quot;cuatro&quot;) ## [1] &quot;matrix&quot; &quot;array&quot; De manera similar, podemos extraer elementos de un objeto, con su clase original, usando índices y corchetes dobles [[]]. La ventaja de usar corchetes dobles es no sólo podemos usar índices, sino que los podemos combinar con nombres, lo cual nos da acceso a una mayor flexibilidad para extraer subconjuntos y permite usarlos en estructuras de datos con elementos sin nombre. Por ejemplo, para extraer la columna edad de mi_df con corchetes dobles, podemos usar su índice, 2, o su nombre. # Usando un índice mi_df[[2]] ## [1] 20 24 22 30 # Usando un nombre mi_df[[&quot;edad&quot;]] ## [1] 20 24 22 30 A diferencia de los corchetes sencillos, no podemos extraer más de una columna de un data frame usando corchetes dobles y un vector. Si escribimos un vector numérico dentro de corchetes dobles, será interpretado como si cada número estuviera separado por una coma, indicando las dimensiones de las cuales se extraerán datos. Por ejemplo, intentamos extraer las columnas uno y tres de mi_df. mi_df[[c(1, 3)]] ## [1] &quot;Ignacio&quot; El resultado que obtenemos no es el esperado. Lo que ocurre es que cuando escribimos lo siguiente. mi_df[[c(1, 3)]] R lo interpreta como: mi_df[[1, 3]] Es decir, R extraerá el dato en el renglón uno y la columna tres, en lugar de las columnas uno y tres. mi_df[[1, 3]] ## [1] &quot;H&quot; Por lo tanto si escribimos lo siguiente: mi_df[[1:3]] R lo interpretará como buscar el dato uno en la primera dimensión, el dato dos, en la segunda dimensión, y el dato tres en la tercera dimensión. Como un data frame solo tiene dos dimensiones, se nos devolverá un error mi_df[[1:3]] ## Error in .subset2(x, i, exact = exact): falló indexación recursiva en nivel 2 Lo mismo ocurre con vectores que contienen nombres de columnas. mi_df[[c(&quot;nombre&quot;, &quot;edad&quot;)]] ## Error in .subset2(x, i, exact = exact): subíndice fuera de los límites Como las listas son unidimensionales, sólo podemos extraer un elemento a la vez usando corchetes dobles [[]] mi_lista[[1]] ## [1] 1 Si damos más de un índice o nombre, siempre obtendremos un error. # Más de un índice mi_lista[[1:2]] ## Error in mi_lista[[1:2]]: subíndice fuera de los límites # Más de un nombre mi_lista[[&quot;uno&quot;, &quot;dos&quot;]] ## Error in mi_lista[[&quot;uno&quot;, &quot;dos&quot;]]: número incorrecto de subíndices 7.4.1 Los data frames y listas son como cajas de manzanas Para comprender mejor el comportamiento comportamiento del signo $ y los corchetes dobles, imagina que los data frames y listas son cajas que contienen manzanas. Los data frame contienen las manzanas en bolsas, y estas bolsas serían vectores, en los cuales cada manzana sería un elemento. Por su parte, las listas pueden tener manzanas en diferentes presentaciones: bolsas, manzanas sueltas o incluso otras cajas de manzana. Cuando usamos corchetes, estamos sacando manzanas de una caja, usando una caja más pequeña, otro data frame o lista. Así, con un data frame, obtenemos otra caja que contiene bolsas de manzanas, y con una lista obtenemos otra caja con manzanas en las presentaciones que se encuentren. En contraste, al usar el signo $ o corchetes dobles, estamos sacando bolsas de manzana directamente en un data frame, y en las listas estamos sacando las manzanas en su presentación original, sin que haya de por medio otra caja en ninguno de estos casos. Dado lo anterior, podemos extraer subconjuntos de subconjuntos combinando diferentes tipos de corchetes. # Subconjunto de un subconjunto: Data frame. mi_df[[2]][3] ## [1] 22 # Subconjunto de un subcojunto: Lista. mi_lista[[&quot;cuatro&quot;]][2] ## [1] 2 No te preocupes mucho si lo anterior te parece confuso, lo es. No profundizaremos sobre este tema específico de los subconjuntos en este libro, pero ten en cuenta que si te encuentras con código como el del ejemplo anterior, lo que está ocurriendo es la extracción de subconjuntos de subconjuntos. "],["7-5-condicionales.html", "7.5 Condicionales", " 7.5 Condicionales Las condicionales nos permiten obtener subconjuntos que para los que una o más condiciones son verdaderas (TRUE). Para este procedimiento usamos operadores lógicos y condicionales, como revisamos en el capítulo 5 y lo podemos aplicar a data frames. Con este procedimiento, podemos extraer todos los datos de una encuesta que corresponden a mujeres, o a personas que viven en una entidad específica, o que tienen un ingreso superior a la media, o cualquier otra condición que se verificable con álgebra Booleana. Realizamos la extracción de subconjuntos mediante operaciones relacionales y lógicas dentro de corchetes. Esta operación tiene la siguiente estructura. objeto[condicion, columnas_devueltas] En donde: objeto: es un data frame. condición: un subconjunto de objeto, que devuelva un columna como vector, al que se le aplica una o más operaciones lógicas o relacionales. columnas_devueltas: los índices o nombres de las columnas que deseamos sean devueltas como resultado. Dentro del corchete escribimos, antes de una coma, el código para obtener un subconjunto que extraiga una columna, del data frame al que queremos extraer un subconjunto usando un subconjunto. Este primer subconjunto debe ser extraído con alguno de los dos procedimientos que da como resultado un vector, ya sea el signo $ o corchetes dobles [[]]. Necesitamos extraer un vector, porque aplicaremos a todos los elementos de esa columna una operación relacional, usando vectorización, como lo vimos en el capítulo 6. Todos los elementos para los que el resultado de esta operación sea TRUE, formarán parte de nuestro subconjunto usando condicionales. Como cada elemento de una columna de un data frame está ubicado en su propio renglón, podemos decir que nos serán devueltos los renglones para los que la condición sea verdadera. Si dejamos el espacio para columnas_devueltas vacío, nuestro resultado será un data frame con las mismas columnas que el data frame original. Si damos un índice o un nombre, entonces obtendremos un data frame sólo con las columnas solicitadas. De este modo, podemos extraer renglones y columnas específicas. Veamos esto en práctica, extrayendo subconjuntos de los datos iris. 7.5.1 Usando condicionales Intentaremos extraer todos los datos en iris en los que el largo del sépalo, columna Petal.Width, sea mayor que 7.5. Primero, obtenemos el subconjunto de esta columna de iris. Usaremos el signo $ iris$Sepal.Length ## [1] 5.1 4.9 4.7 4.6 5.0 5.4 4.6 5.0 4.4 4.9 5.4 4.8 4.8 4.3 5.8 5.7 5.4 5.1 ## [19] 5.7 5.1 5.4 5.1 4.6 5.1 4.8 5.0 5.0 5.2 5.2 4.7 4.8 5.4 5.2 5.5 4.9 5.0 ## [37] 5.5 4.9 4.4 5.1 5.0 4.5 4.4 5.0 5.1 4.8 5.1 4.6 5.3 5.0 7.0 6.4 6.9 5.5 ## [55] 6.5 5.7 6.3 4.9 6.6 5.2 5.0 5.9 6.0 6.1 5.6 6.7 5.6 5.8 6.2 5.6 5.9 6.1 ## [73] 6.3 6.1 6.4 6.6 6.8 6.7 6.0 5.7 5.5 5.5 5.8 6.0 5.4 6.0 6.7 6.3 5.6 5.5 ## [91] 5.5 6.1 5.8 5.0 5.6 5.7 5.7 6.2 5.1 5.7 6.3 5.8 7.1 6.3 6.5 7.6 4.9 7.3 ## [109] 6.7 7.2 6.5 6.4 6.8 5.7 5.8 6.4 6.5 7.7 7.7 6.0 6.9 5.6 7.7 6.3 6.7 7.2 ## [127] 6.2 6.1 6.4 7.2 7.4 7.9 6.4 6.3 6.1 7.7 6.3 6.4 6.0 6.9 6.7 6.9 5.8 6.8 ## [145] 6.7 6.7 6.3 6.5 6.2 5.9 Si aplicamos la operación relacional &gt; 7 a este subconjunto, obtenemos un vector, con TRUE o FALSE para cada elemento de iris$Petal.Width. iris$Sepal.Length &gt; 7.5 ## [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [97] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE ## [109] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE ## [121] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [133] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [145] FALSE FALSE FALSE FALSE FALSE FALSE Escribimos esta operación dentro de los corchetes, antes de una coma. No escribimos nada después de la coma, para obtener un subconjunto con todas las columnas de iris. iris[iris$Sepal.Length &gt; 7.5, ] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 106 7.6 3.0 6.6 2.1 virginica ## 118 7.7 3.8 6.7 2.2 virginica ## 119 7.7 2.6 6.9 2.3 virginica ## 123 7.7 2.8 6.7 2.0 virginica ## 132 7.9 3.8 6.4 2.0 virginica ## 136 7.7 3.0 6.1 2.3 virginica Podemos pedir que se nos devuelvan sólo los datos de la columna Species, escribiendo el índice o nombre de esta columna después de la coma. # Usando índice iris[iris$Sepal.Length &gt; 7.5, 5] ## [1] virginica virginica virginica virginica virginica virginica ## Levels: setosa versicolor virginica #USando nombres iris[iris$Sepal.Length &gt; 7.5, &quot;Species&quot;] ## [1] virginica virginica virginica virginica virginica virginica ## Levels: setosa versicolor virginica Nota que si pedimos una sola columna en nuestros resultado, el resultado será un vector en lugar de un data frame. class(iris[iris$Sepal.Length &gt; 7.5, &quot;Species&quot;]) ## [1] &quot;factor&quot; Podemos realizar más de una operación relacional antes de la coma, usando operadores lógicos. Por ejemplo, extraemos todos los datos para los que el ancho del pétalo (Sepal.Width) sea menor que 3 y la especie (Species) sea setosa. iris[iris$Sepal.Width &lt; 3 &amp; iris$Species == &quot;setosa&quot;, ] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 9 4.4 2.9 1.4 0.2 setosa ## 42 4.5 2.3 1.3 0.3 setosa Recuerda que puedes usar el operados ! para negaciones. De este modo puedes extraer todos los datos que no son de la especie virginica o su largo sea menor a 4.7. iris[!(iris$Petal.Length &lt; 4.7 | iris$Species == &quot;virginica&quot;), ] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 51 7.0 3.2 4.7 1.4 versicolor ## 53 6.9 3.1 4.9 1.5 versicolor ## 57 6.3 3.3 4.7 1.6 versicolor ## 64 6.1 2.9 4.7 1.4 versicolor ## 71 5.9 3.2 4.8 1.8 versicolor ## 73 6.3 2.5 4.9 1.5 versicolor ## 74 6.1 2.8 4.7 1.2 versicolor ## 77 6.8 2.8 4.8 1.4 versicolor ## 78 6.7 3.0 5.0 1.7 versicolor ## 84 6.0 2.7 5.1 1.6 versicolor ## 87 6.7 3.1 4.7 1.5 versicolor En realidad podemos usar un vector lógico para extraer subconjuntos, sin necesidad de realizar una operación relacional. Por ejemplo, para obtener los dato en el primer y cuarto renglón de mi_df. mi_df[c(TRUE, FALSE, FALSE, TRUE), ] ## nombre edad sexo grupo ## 1 Armando 20 H 0 ## 4 Olga 30 H 0 Si damos un vector lógico de largo menor que el número de renglones en un data frame, el vector es reciclado. Al utilizar vector c(FALSE, TRUE), nos serán devueltos el segundo y cuarto renglón de mi_df. mi_df[c(FALSE, TRUE), ] ## nombre edad sexo grupo ## 2 Elsa 24 M 1 ## 4 Olga 30 H 0 Si proporcionamos una condición que no se cumple en ningún caso, es decir, devuelve un vector que consiste sólo de FALSE, el subconjunto que obtenemos es una data frame sin renglones. iris[iris$Species == &quot;oceanica&quot;, ] ## [1] Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;0 rows&gt; (or 0-length row.names) Finalmente, si no escribimos una coma dentro del corchete después de la condicional, obtendremos un data frame sin columnas, que para fines prácticos es un objeto sin utilidad. iris[iris$Petal.Width &gt;= 5] ## data frame with 0 columns and 150 rows 7.5.2 La función subset() Una alternativa para usar condicionales, sin necesidad de corchetes, es la función subset(). Esta función tiene los siguientes argumentos: x: Un objeto, generalmente un data frame. subset: Una condición, expresada como operaciones relacionales o condicionales, que se aplicarán a una columna de x. select: Un vector con los nombres de las columnas a conservar en el resultado. Si no asignamos un valor a este argumento, se nos devuelven todas las columnas de x. Puedes leer la documentación completa llamando ?subset. Como podrás ver, subset() necesita como argumentos los mismos elementos que usamos para extraer subconjuntos con corchetes. La principal diferencia se encuentra en el argumento subset. Al usar corchetes, necesitamos aplicar una operación relacional a un vector, extraído de nuestro objeto original, por ejemplo iris[iris$Species == \"setosa\", ]. Con subset(), basta proporcionar el nombre del elemento al que aplicaremos las operaciones relacionales y condicionales. Para este caso, escribimos Species == \"setosa\". Nota que el nombre de la columna está escrito sin comillas. Por ejemplo, para extraer de iris todos los datos en los que el largo del sépalo es mayor que 7.5, damos a iris como argumento x y Sepal.Lenght como argumento subset. Dejamos sin definir el argumento select, para obtener todas las columnas. subset(x = iris, subset = Sepal.Length &gt; 7.5) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 106 7.6 3.0 6.6 2.1 virginica ## 118 7.7 3.8 6.7 2.2 virginica ## 119 7.7 2.6 6.9 2.3 virginica ## 123 7.7 2.8 6.7 2.0 virginica ## 132 7.9 3.8 6.4 2.0 virginica ## 136 7.7 3.0 6.1 2.3 virginica Obtenemos el mismo resultado que al llamar iris[iris.Sepal.Length &gt; 7.5, ]. Damos además c(\"Sepal.Length\", \"Species\") como argumento a select y con ello sólo nos serán devueltas las columnas con estos nombres. subset(x = iris, subset = Sepal.Length &gt; 7.5, select = c(&quot;Sepal.Length&quot;, &quot;Species&quot;)) ## Sepal.Length Species ## 106 7.6 virginica ## 118 7.7 virginica ## 119 7.7 virginica ## 123 7.7 virginica ## 132 7.9 virginica ## 136 7.7 virginica Desde luego, esto es equivalente a llamar iris[iris.Sepal.Length &gt; 7.5, c(\"Sepal.Length\", \"Species\")]. También podemos usar operaciones lógicas para el argumento subset. Por ejemplo, los datos para los que el largo del sépalo sea mayor que 7.5 y el ancho del pétalo sea igual a 3. subset(x = iris, subset = Sepal.Length &gt; 7.5 &amp; Sepal.Width == 3) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 106 7.6 3 6.6 2.1 virginica ## 136 7.7 3 6.1 2.3 virginica Si damos como argumento a subset una comparación con un nombre de columna no válido, obtenemos un error. subset(x = iris, subset = Sepal.Weight &gt; 7.5 ) ## Error in eval(e, x, parent.frame()): objeto &#39;Sepal.Weight&#39; no encontrado Al igual que si usamos corchetes, si pedimos una condición que para ningún caso es verdadera, nuestro resultado es un data frame sin renglones. subset(x = iris, subset = Sepal.Length &gt; 9 ) ## [1] Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;0 rows&gt; (or 0-length row.names) Si damos como argumento select el nombre de una columna inválida, también se nos devuelve un error. subset(x = iris, subset = Sepal.Length &gt; 7, select = &quot;Sepal.Weight&quot;) ## Error in `[.data.frame`(x, r, vars, drop = drop): undefined columns selected Finalmente, también podemos dar un vector lógico como argumento subset para obtener subconjuntos. Por ejemplo, el segundo y cuarto renglón de mi_df. subset(x = mi_df, subset = c(TRUE, FALSE, FALSE, TRUE)) ## nombre edad sexo grupo ## 1 Armando 20 H 0 ## 4 Olga 30 H 0 La función subset() casi siempre resulta en código más breve y es más fácil de interpretar por el usuario que los subconjuntos condicionales con corchetes. Sin embargo, la mayor parte de las veces, usar uno u otro procedimiento depende de tu preferencia personal y de las necesidades de los proyectos en los que colaboras. "],["8-funciones.html", "Capítulo 8 Funciones", " Capítulo 8 Funciones La instalación base de R tiene suficientes funciones para que realicemos todas las tareas básicas de análisis de datos, desde importar información hasta crear documentos para comunicarla (¡este libro ha sido creado con R!). Sin embargo, es común que necesitemos realizar tareas para las que no existe una función específica o que para encontrar solución necesitemos combinar o utilizar funciones en sucesión, lo cual puede complicar nuestro código. Ilustremos lo anterior con un ejemplo. "],["8-1-por-qué-necesitamos-crear-nuestrar-propias-funciones.html", "8.1 ¿Por qué necesitamos crear nuestrar propias funciones?", " 8.1 ¿Por qué necesitamos crear nuestrar propias funciones? Supongamos que tenemos un jefe que nos ha pedido crear un histograma con datos de edad que hemos recogido en una encuesta. Esto es sencillo de resolver pues contamos con la función hist() que hace exactamente esto. Sólo tenemos que dar un vector numérico como argumento para generar una gráfica (veremos esto con más detalle en el capítulo 12). Primero, generaremos datos aleatorios sacados de una distribución normal con la función rnorm(). Esta función tiene los siguientes argumentos: n: Cantidad de números a generar. mean: Media de la distribución de la que sacaremos nuestros números. sd: Desviación estándar de la distribución de la que sacaremos nuestros números. Además, llamaremos set.seed() para que estos resultados sean replicables. Cada que llamamos rnorm() se generan número aleatorios diferentes, pero si antes llamamos a set.seed(), con un número específico como argumento obtendremos los mismos resultados. Obtendremos 1500 números con media 15 y desviación estándar .75. set.seed(173) edades &lt;- rnorm(n = 1500, mean = 15, sd = .75) Veamos los primero diez números de nuestro objeto. edades[1:10] ## [1] 15.79043 14.68603 16.29119 14.66079 15.25658 14.62890 14.87498 16.35364 ## [9] 16.04607 16.35803 Ahora, sólo tenemos que ejecutar hist() con el argumento x igual a nuestro vector y obtendremos un histograma. # Histograma hist(x = edades) Estupendo. Hemos logrado nuestro objetivo. Nuestro jefe está satisfecho, pero le gustaría que en el histograma se muestre la media y desviación estándar de los datos, que tenga un título descriptivo y que los ejes estén etiquetados en español, además de que las barras sean de color dorado. Suena complicado, pero podemos calcular la media de los datos usando la función mean(), la desviación estándar con sd() y podemos agregar los resultados de este cálculo al histograma usando la función abline(). Para agregar título, etiquetas en español y colores al histograma sólo basta agregar los argumentos apropiados a la función hist(). No te preocupes mucho por los detalles de todo esto, lo veremos más adelante. Calculamos media y desviación estándar de nuestros datos. media &lt;- mean(edades) desv_est &lt;- sd(edades) Agregamos líneas con abline(), para la media de rojo y desviación estándar con azul. También ajustamos los argumentos de hist(). hist(edades, main = &quot;Edades&quot;, xlab = &quot;Datos&quot;, ylab = &quot;Frecuencia&quot;, col = &quot;gold&quot;) abline(v = media, col = &quot;red&quot;) abline(v = media + (desv_est * c(1, -1)), col = &quot;blue&quot;) Con esto nuestro jefe ahora sí ha quedado complacido. Tanto, que nos pide que hagamos un histograma igual para todas las variables numéricas de esa encuesta. Que son cincuenta en total. Para cumplir con esta tarea podríamos usar el código que ya hemos escrito. Simplemente lo copiamos y pegamos cincuenta veces, cambiando los valores para cada una de variables que nos han pedido. Pero hacer las cosas de este modo propicia errores y es difícil de corregir y actualizar. Para empezar, si copias el código anterior cincuenta veces, tendrás un script con más de 400 líneas. Si en algún momento te equivocas porque escribiste Enceusta en lugar de Encuesta, incluso con las herramientas de búsqueda de RStudio, encontrar donde está el error será una tarea larga y tediosa. Y si tu jefe en esta ejemplo quiere que agregues, quites o modifiques tu histograma, tendrás que hacer el cambio cincuenta veces, una para cada copia del código. De nuevo, con esto se incrementa el riesgo de que ocurran errores. Es en situaciones como esta en las que se hace evidente la necesidad de crear nuestras propias funciones, capaces de realizar una tarea específica a nuestros problemas, y que pueda usarse de manera repetida. Así reducimos errores, facilitamos hacer correcciones o cambios y nos hacemos la vida más fácil, a nosotros y a quienes usen nuestro código después. "],["8-2-funciones-definidas-por-el-usuario.html", "8.2 Funciones definidas por el usuario", " 8.2 Funciones definidas por el usuario Una función tiene un nombre, argumentos y un cuerpo. Las funciones definidas por el usuario son creadas usando la siguiente estructura. nombre &lt;- function(argumentos) { operaciones } Cuando asignamos una función a un nombre decimos que hemos definido una función. El nombre que asignamos a una función nos permite ejecutarla y hacer referencias a ella. Podemos asignar la misma función a diferentes nombres o cambiar una función a la que ya le hemos asignado un nombre. Es recomendable elegir nombres claros, no ambiguos y descriptivos. Una vez que la función tiene nombre, podemos llamarla usando su nombre, al igual que con las funciones por defecto de R. Los argumentos son las variables que necesita la función para realizar sus operaciones. Aparecen entre paréntesis, separados por comas. Los valores son asignados al nombre del argumento por el usuario cada vez que ejecuta una función. Esto permite que usemos nuestras funciones en distintas situaciones con diferentes datos y especificaciones. Los argumentos pueden ser datos, estructuras de datos, conexiones a archivos u otras funciones y todos deben tener nombres diferentes. El cuerpo de la función contiene, entre llaves, todas las operaciones que se ejecutarán cuando la función sea llamada. El cuerpo de una función puede ser tan sencillo o complejo como nosotros deseemos, incluso podemos definir funciones dentro de una función (y definir funciones dentro de una función dentro de otra función, aunque esto se vuelve confuso rápidamente). Si el código del cuerpo de la función tiene errores, sus operaciones no se realizarán y nos será devuelto un mensaje de error al ejecutarla. R no avisa si nuestra función va a funcionar o no hasta que intentamos correrla. Una ventaja de usar RStudio es que nos indica errores de sintaxis en nuestro código, lo cual puede prevenir algunos errores. Sin embargo, hay alguno que no detecta, como realizar operaciones o coerciones imposibles. Para ver esto en acción, crearemos una función sencilla para obtener el área de un cuadrilátero. "],["8-3-nuestra-primera-función.html", "8.3 Nuestra primera función", " 8.3 Nuestra primera función Partimos del algoritmo para calcular el área de un cuadrilátero: lado x lado. Podemos convertir esto a operaciones de R y asignarlas a una función llamada area_cuad de la siguiente manera: area_cuad &lt;- function(lado1, lado2) { lado1 * lado2 } Las partes de nuestra función son: Nombre: area_cuad. Argumentos: lado1, lado2. Estos son los datos que necesita la función para calcular el área, representan el largo de los lados de un cuadrilátero. Cuerpo: La operación lado1 * lado2, escrita de manera que R pueda interpretarla. Ejecutaremos nuestra función para comprobar que funciona. Nota que lo único que hacemos cada que la llamamos es cambiar la medida de los lados del cuadrilátero para el que calcularemos un área, en lugar de escribir la operación lado1 * lado2 en cada ocasión. area_cuad(lado1 = 4, lado2 = 6) ## [1] 24 area_cuad(lado1 = 36, lado2 = 36) ## [1] 1296 En cada llamada a nuestra función estamos asignando valores distintos a los argumentos usando el signo de igual. Si no asignamos valores a un argumento, se nos mostrará un error area_cuad(lado1 = 14) ## Error in area_cuad(lado1 = 14): el argumento &quot;lado2&quot; está ausente, sin valor por omisión En R, podemos especificar los argumentos por posición. El orden de los argumentos se determina cuando creamos una función. En este caso, nosotros determinamos que el primer argumento que recibe area_cuad es lado1 y el segundo es lado2. Así, podemos escribir lo siguiente y obtener el resultado esperado. area_cuad(128, 64) ## [1] 8192 Esto es equivalente a escribir lado1 = 128, lado2 = 64 como argumentos. Podemos crear ahora una función ligeramente más compleja para calcular el volumen de un prisma rectangular Siguiendo la misma lógica de transformar un algoritmo a código de R, podemos crear una función con el algoritmo: arista x arista x arista. Definimos la función area_prisma(). area_prisma &lt;- function(arista1, arista2, arista3) { arista1 * arista2 * arista3 } Probemos nuestra función. area_prisma(arista1 = 3, arista2 = 6, arista3 = 9) ## [1] 162 También podríamos escribir esta función aprovechando nuestra función area_cuad. area_prisma &lt;- function(arista1, arista2, arista3) { area_cuad(arista1, arista2) * arista3 } # Probemos la función area_prisma(3, 6, 9) ## [1] 162 Con esto estamos listos para definir una función para crear histogramas con las características que nos pidió nuestro jefe hipotético. "],["8-4-definiendo-la-función-crear-histograma.html", "8.4 Definiendo la función crear_histograma()", " 8.4 Definiendo la función crear_histograma() Definiremos una función con el nombre crear_histograma() para generar un gráfico con las especificaciones que se nos han pedido. Partimos de una función sin argumentos y el cuerpo vacío. crear_histograma &lt;- function() { } Para que esta función realice lo que deseamos necesitamos: Los datos que serán graficados. El nombre de la variable graficada Por lo tanto, nuestros argumentos serán: datos nombre crear_histograma &lt;- function(datos, nombre) { } Ya sabemos las operaciones realizaremos, sólo tenemos que incluirlas al cuerpo de nuestro función. Reemplazaremos las variables que hacen referencia a un objeto en particular por el nombre de nuestros argumentos. De esta manera será generalizable a otros casos. En este ejemplo, cambiamos la referencia a la variable edades por referencias al argumento datos y la referencia a Edades, que usaremos como título del histograma, por una referencia al argumento nombre. crear_histograma &lt;- function(datos, nombre) { media &lt;- mean(datos) desv_est &lt;- sd(datos) hist(datos, main = nombre, xlab = &quot;Datos&quot;, ylab = &quot;Frecuencia&quot;, col = &quot;gold&quot;) abline(v = media, col = &quot;red&quot;) abline(v = media + (desv_est * c(1, -1)), col = &quot;blue&quot;) } Probemos nuestra función usando datos distintos, generados de manera similar a las edades, con la función rnorm(). Generaremos datos de ingreso, con una media igual a 15000 y una desviación estándar de 4500. ingreso &lt;- rnorm(1500, mean = 15000, sd = 4500) # Resultado ingreso[1:10] ## [1] 14365.18 16621.70 13712.35 21796.08 14226.73 13830.29 22187.37 17879.22 ## [9] 11040.41 17923.13 Corremos nuestra función. crear_histograma(ingreso, &quot;Ingreso&quot;) Luce bien. Probemos ahora con datos sobre el peso de las personas. siguiendo el mismo procedimiento. peso &lt;- rnorm(75, mean = 60, sd = 15) crear_histograma(peso, &quot;Peso&quot;) Las funciones definidas por el usuario pueden devolvernos errores. Por ejemplo, si introducimos datos que no son apropiados para las operaciones a realizar, nuestra función no se ejecutará correctamente. crear_histograma(&quot;Cuatro&quot;, ingreso) ## Warning in mean.default(datos): argument is not numeric or logical: returning NA ## Warning in var(if (is.vector(x) || is.factor(x)) x else as.double(x), na.rm = ## na.rm): NAs introducidos por coerción ## Error in hist.default(datos, main = nombre, xlab = &quot;Datos&quot;, ylab = &quot;Frecuencia&quot;, : &#39;x&#39; must be numeric Por esta razón es importante crear documentación para las funciones que hayas creado. Puede ser tan sencilla como una explicación de qué hace la función y qué tipo de datos necesita para realizar sus operaciones. La primera persona beneficiada por esto eres tu, pues tu yo de un mes en el futuro puede haber olvidado por completo la lógica de una función específica, así que la documentación es una manera de recordar tu trabajo. Una manera simple de documentar tus funciones es con comentarios. # crear_histograma # Devuelve un histograma con lineas indicando la media y desviación estándar de un vector de datos numérico # Argumentos: # - datos: Un vector numérico. # - nombre: Una cadena de texto. 8.4.1 Ejecutando Ahora, podremos cumplir con la solicitud de nuestro jefe ficticio usando cincuenta llamadas a una función en lugar de correr más de cuatrocientas líneas de código y que hemos reducido la probabilidad de cometer errores. Además, si a nuestro jefe se le ocurren nuevas características para los histogramas, basta con cambiar el cuerpo de nuestra función una vez y esto se verá reflejado en nuestro cincuenta casos al correr de nuevo el código. Por ejemplo, supongamos que nuestro jefe también quiere que el histograma muestre la mediana de nuestros datos y que las barras sean de color naranja. Basta con hacer un par de cambios. crear_histograma &lt;- function(datos, nombre) { media &lt;- mean(datos) desv_est &lt;- sd(datos) mediana &lt;- median(datos) hist(datos, main = nombre, xlab = &quot;Datos&quot;, ylab = &quot;Frecuencia&quot;, col = &quot;orange&quot;) abline(v = media, col = &quot;red&quot;) abline(v = media + (desv_est * c(1, -1)), col = &quot;blue&quot;) abline(v = mediana, col = &quot;green&quot;) } # Resultado crear_histograma(peso, &quot;Peso con mediana&quot;) Quizás estés pensando que escribir una función cincuenta veces de todos modos es demasiada repetición y aún se presta a cometer errores. Lo cual es cierto, pero podemos hacer más breve nuestro código y menos susceptible a equivocaciones con la familia de funciones apply, que revisaremos en el capítulo 10. "],["9-estructuras-de-control.html", "Capítulo 9 Estructuras de control", " Capítulo 9 Estructuras de control Como su nombre lo indica, las estructuras de control nos permiten controlar la manera en que se ejecuta nuestro código. Las estructuras de control establecen condicionales en nuestros código. Por ejemplo, qué condiciones deben cumplirse para realizar una operación o qué debe ocurrir para ejecutar una función. Esto es de gran utilidad para determinar la lógica y el orden en que ocurren las operaciones, en especial al definir funciones. Las estructuras de control más usadas en R son las siguientes. Estructura de control Descripción if, else Si, de otro modo for Para cada uno en while Mientras break Interrupción next Siguiente "],["9-1-if-else.html", "9.1 if, else", " 9.1 if, else if (si) es usado cuando deseamos que una operación se ejecute únicamente cuando una condición se cumple. else (de otro modo) es usado para indicarle a R qué hacer en caso de la condición de un if no se cumpla. Un if es la manera de decirle a R: SI esta condición es cierta, ENTONCES haz estas operaciones. El modelo para un if es: if(Condición) { operaciones_si_la_condición_es_TRUE } Si la condición se cumple, es decir, es verdadera (TRUE), entonces se realizan las operaciones. En caso contrario, no ocurre nada y el código con las operaciones no es ejecutado. Por ejemplo, le pedimos a R que nos muestre el texto Verdadero si la condición se cumple. # Se cumple la condición y se muestra &quot;verdadero&quot; if(4 &gt; 3) { &quot;Verdadero&quot; } ## [1] &quot;Verdadero&quot; # No se cumple la condición y no pasa nada if(4 &gt; 5) { &quot;Verdadero&quot; } else complementa un if, pues indica qué ocurrirá cuando la condición no se cumple, es falsa (FALSE), en lugar de no hacer nada. Un if con else es la manera de decirle a R: SI esta condición es es cierta, ENTONCES haz estas operaciones, DE OTRO MODO haz estas otras operaciones. El modelo para un if con un else es: if(condición) { operaciones_si_la_condición_es_TRUE } else { operaciones_si_la_condición_es_FALSE } Usando los ejemplos anteriores, podemos mostrar Falso si no se cumple la condición, en lugar de que no ocurra nada. # Se cumple la condición y se muestra &quot;Verdadero&quot; if(4 &gt; 3) { &quot;Verdadero&quot; } else { &quot;Falso&quot; } ## [1] &quot;Verdadero&quot; # No se cumple la condición y se muestra &quot;Falso&quot; if(4 &gt; 5) { &quot;Verdadero&quot; } else { &quot;Falso&quot; } ## [1] &quot;Falso&quot; 9.1.1 Usando if y else Para ilustrar el uso de if else definiremos una función que calcule el promedio de calificaciones de un estudiante y, dependiendo de la calificación calculada, nos devuelva un mensaje específico. Empezamos definiendo una función para calcular promedio. En realidad, sólo es la aplicación de la función mean() ya existente en R base, pero la ampliaremos después. promedio &lt;- function(calificaciones) { mean(calificaciones) } promedio(c(6, 7, 8, 9, 8)) ## [1] 7.6 promedio(c(5, 8, 5, 6, 5)) ## [1] 5.8 Ahora deseamos que esta función nos muestre si un estudiante ha aprobado o no. Si asumimos que un estudiante necesita obtener 6 o más de promedio para aprobar, podemos decir que: SI el promedio de un estudiante es igual o mayor a 6, ENTONCES mostrar Aprobado, DE OTRO MODO, mostrar Reprobado. Aplicamos esta lógica con un if, else en la función promedio(). promedio &lt;- function(calificaciones) { media &lt;- mean(calificaciones) if(media &gt;= 6) { print(&quot;Aprobado&quot;) } else { print(&quot;Reprobado&quot;) } } Probemos nuestra función promedio(c(6, 7, 8, 9, 8)) ## [1] &quot;Aprobado&quot; promedio(c(5, 8, 5, 6, 5)) ## [1] &quot;Reprobado&quot; Está funcionando, aunque los resultados podrían tener una mejor presentación. Usaremos la función paste0() para pegar el promedio de calificaciones, como texto, con el resultado de Aprobado o Reprobado. Esta función acepta como argumentos cadenas de texto y las pega (concatena) entre sí, devolviendo como resultado una nueva cadena. Primero concatenaremos la palabra Calificación: a la media obtenida con la función promedio() y después el resultado de esta operación con la palabra aprobado o reprobado, según corresponda. promedio &lt;- function(calificaciones) { media &lt;- mean(calificaciones) texto &lt;- paste0(&quot;Calificación: &quot;, media, &quot;, &quot;) if(media &gt;= 6) { print(paste0(texto, &quot;aprobado&quot;)) } else { print(paste0(texto, &quot;reprobado&quot;)) } } Pongamos a prueba nuestra función. promedio(c(6, 7, 8, 9, 8)) ## [1] &quot;Calificación: 7.6, aprobado&quot; promedio(c(5, 8, 5, 6, 5)) ## [1] &quot;Calificación: 5.8, reprobado&quot; Por supuesto, como lo vimos en el capítulo sobre funciones, podemos hacer aún más compleja a promedio(), pero esto es suficiente para conocer mejor las aplicaciones de if else. 9.1.2 ifelse La función ifelse( ) nos permite vectorizar if, else. En lugar de escribir una línea de código para cada comparación, podemos usar una sola llamada a esta función, que se aplicará a todos los elementos de un vector. Si intentamos usar if else con un vector, se nos mostrará una advertencia. if(1:10 &lt; 3) { &quot;Verdadero&quot; } ## Warning in if (1:10 &lt; 3) {: la condición tiene longitud &gt; 1 y sólo el primer ## elemento será usado ## [1] &quot;Verdadero&quot; Este mensaje nos dice que sólo se usará el primer elemento del vector para evaluar su la condición es verdadera y lo demás será ignorado. En cambio, con ifelse se nos devolverá un valor para cada elemento de un vector en el que la condición sea TRUE, además nos devolverá otro valor para los elementos en que la condición sea FALSE. Esta función tiene la siguiente forma. ifelse(vector, valor_si_TRUE, valor_si_FALSE) Si intentamos el ejemplo anterior con ifelse(), se nos devolverá un resultado para cada elemento del vector, no sólo del primero de ellos. ## [1] &quot;Verdadero&quot; &quot;Verdadero&quot; &quot;Falso&quot; &quot;Falso&quot; &quot;Falso&quot; &quot;Falso&quot; ## [7] &quot;Falso&quot; &quot;Falso&quot; &quot;Falso&quot; &quot;Falso&quot; De este modo podemos usar ifelse() para saber si los números en un vector son pares o nones. num &lt;- 1:8 ifelse(num %% 2 == 0, &quot;Par&quot;, &quot;Non&quot;) ## [1] &quot;Non&quot; &quot;Par&quot; &quot;Non&quot; &quot;Par&quot; &quot;Non&quot; &quot;Par&quot; &quot;Non&quot; &quot;Par&quot; También tenemos la opción de crear condiciones más complejas usando operadores Booleanos. Por ejemplo, pedimos sólo los números que son exactamente divisibles entre 2 y 3. num &lt;- 1:20 ifelse(num %% 2 == 0 &amp; num %% 3, &quot;Divisible&quot;, &quot;No divisible&quot;) ## [1] &quot;No divisible&quot; &quot;Divisible&quot; &quot;No divisible&quot; &quot;Divisible&quot; &quot;No divisible&quot; ## [6] &quot;No divisible&quot; &quot;No divisible&quot; &quot;Divisible&quot; &quot;No divisible&quot; &quot;Divisible&quot; ## [11] &quot;No divisible&quot; &quot;No divisible&quot; &quot;No divisible&quot; &quot;Divisible&quot; &quot;No divisible&quot; ## [16] &quot;Divisible&quot; &quot;No divisible&quot; &quot;No divisible&quot; &quot;No divisible&quot; &quot;Divisible&quot; Desde luego, esto es particularmente útil para recodificar datos. num &lt;- c(0, 1, 0, 0, 0, 1, 1) num &lt;- ifelse(num == 0, &quot;Hombre&quot;, &quot;Mujer&quot;) num ## [1] &quot;Hombre&quot; &quot;Mujer&quot; &quot;Hombre&quot; &quot;Hombre&quot; &quot;Hombre&quot; &quot;Mujer&quot; &quot;Mujer&quot; "],["9-2-for.html", "9.2 for", " 9.2 for La estructura for nos permite ejecutar un bucle (loop), realizando una operación para cada elemento de un conjunto de datos. Su estructura es la siguiente: for(elemento in objeto) { operacion_con_elemento } Con lo anterior le decimos a R: PARA cada elemento EN un objeto, haz la siguiente operación. Al escribir un bucle for la parte que corresponde al elemento la podemos llamar como nosotros deseemos, pero la parte que corresponde al objeto debe ser el nombre de un objeto existente. Los dos bucles siguientes son equivalentes, sólo cambia el nombre que le hemos puesto al elemento. objeto &lt;- 1:10 for(elemento in objeto) { operacion_con_elemento } for(i in objeto) { operacion_con_elemento } Tradicionalmente se usa la letra i para denotar al elemento, pero nosotros usaremos nombres más descriptivos en este capítulo. 9.2.1 Usando for Vamos a obtener el cuadrado de cada uno de los elementos en un vector numérico del 1 al 6, que representa las caras de un dado. dado &lt;- 1:6 for(cara in dado) { dado ^ 2 } Notarás que al ejecutar el código anterior parece que no ha ocurrido nada. En realidad, sí se han realizado las operaciones, pero R no ha devuelto sus resultados. Las operaciones en un for se realizan pero sus resultados nunca son devueltos automáticamente, es necesario pedirlos de manera explícita. A diferencia de otros lenguajes de programación en los que pedimos los resultados de un bucle con return(), en R este procedimiento sólo funciona con funciones. Una solución para mostrar los resultados de un bucle for es usar la función print(). for(cara in dado) { print(cara ^ 2) } ## [1] 1 ## [1] 4 ## [1] 9 ## [1] 16 ## [1] 25 ## [1] 36 Comprobamos que la operación ha sido realizada a cada elemento de nuestro objeto. Sin embargo, usar print() sólo mostrará los resultados de las operaciones en la consola, no los asignará a un objeto. Si deseamos asignar los resultados de un bucle for a un objeto, usamos índices. Aprovechamos que el primer elemento en un bucle siempre es identificado con el número 1 y que continuará realizando operaciones hasta llegar al total de elementos que hemos especificado. for(numero in 1:10) { print(numero) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 ## [1] 8 ## [1] 9 ## [1] 10 En nuestro ejemplo, pasamos por los valores de dado, cara por cara. La primera cara será igual a 1, la segunda a 2, y así sucesivamente hasta el 6. Podemos usar estos valores para asignar cada valor resultante de nuestras operaciones a una posición específica en un vector, incluso si este está vacío. Creamos un vector vacío, asignándole como NULL como valor. mi_vector &lt;- NULL Ejecutamos nuestro bucle. for(cara in dado) { mi_vector[cara] &lt;- cara ^ 2 } Aunque no fueron mostrados en la consola, los resultados han sido asignados al objeto mi_vector. mi_vector ## [1] 1 4 9 16 25 36 9.2.2 for y vectorización Notarás que el resultado que obtuvimos usando for es el mismo que si vectorizamos la operación. dado ^ 2 ## [1] 1 4 9 16 25 36 Dado que en R contamos con vectorización de operaciones, que podemos usar las funciones de la familia apply (discutido en siguiente capítulo) en objetos diferentes a vectores y que la manera de recuperar los resultados de un for es un tanto laboriosa, este tipo de bucle no es muy popular en R. En R generalmente hay opciones mejores, en cuanto a simplicidad y velocidad de cómputo, que un bucle for. Sin embargo, es conveniente que conozcas esta estructura de control, pues hay ocasiones en la que es la mejor herramienta para algunos problemas específicos. "],["9-3-while.html", "9.3 while", " 9.3 while Este es un tipo de bucle que ocurre mientras una condición es verdadera (TRUE). La operación se realiza hasta que se se llega a cumplir un criterio previamente establecido. El modelo de while es: while(condicion) { operaciones } Con esto le decimos a R: MIENTRAS esta condición sea VERDADERA, haz estas operaciones. La condición generalmente es expresada como el resultado de una o varias operaciones de comparación, pero también puede ser el resultado de una función. 9.3.1 Usando while Probemos sumar +1 a un valor, mientras que este sea menor que 5. Al igual que con for, necesitamos la función print() para mostrar los resultados en la consola. umbral &lt;- 5 valor &lt;- 0 while(valor &lt; umbral) { print(&quot;Todavía no.&quot;) valor &lt;- valor + 1 } ## [1] &quot;Todavía no.&quot; ## [1] &quot;Todavía no.&quot; ## [1] &quot;Todavía no.&quot; ## [1] &quot;Todavía no.&quot; ## [1] &quot;Todavía no.&quot; ¡Ten cuidado con crear bucles infinitos! Si ejecutas un while con una condición que nunca será FALSE, este nunca se detendrá. Si corres lo siguiente, presiona la tecla ESC para detener la ejecución, de otro modo, correrá por siempre y puede llegar a congelar tu equipo. while(1 &lt; 2) { print(&quot;Presiona ESC para detener&quot;) } El siguiente es un error común. Estamos sumando 1 a i con cada iteración del bucle, pero como no estamos asignando este nuevo valor a i, su valor se mantiene igual, entonces la condición nunca se cumplirá y el bucle será infinito. De nuevo, si corres lo siguiente, presiona la tecla ESC para detener la ejecución. i &lt;- 0 while(i &lt; 10) { i + 1 } Un uso común de while es que realice operaciones que queremos detener cuando se cumple una condición, pero desconocemos cuándo ocurrirá esto. Supongamos que, por alguna razón queremos sumar calificaciones, del 1 al 10 al azar, hasta llegar a un número que mayor o igual a 50. Además nos interesa saber cuántas calificaciones sumaron y cuál fue el resultado al momento de cumplir la condición. Para obtener números al azar del 1 al 10, usamos la función sample(). Esta función va a tomar una muestra al azar de tamaño igual a 1 (argumento size) de un vector del 1 al 10 (argumento x) cada vez que se ejecute. Por lo tanto, cada vez que corras el ejemplo siguiente obtendrás un resultado distinto, pero siempre llegarás a un valor mayor a 50. Creamos dos objetos, conteo y valor. Les asignamos el valor 0. conteo &lt;- 0 valor &lt;- 0 Nuestro while hará dos cosas. Primero, tomará un número al azar del 1 al 10, y lo sumará a valor. Segundo, le sumará 1 a conteo cada que esto ocurra, de esta manera sabremos cuántas iteraciones ocurrieron para llegar a un valor que no sea menor a 50. while(valor &lt; 50) { valor &lt;- valor + sample(x = 1:10, size = 1) conteo &lt;- conteo + 1 } Aunque no son mostrados en la consola los resultados son asignados a los objetos valory conteo valor ## [1] 57 conteo ## [1] 9 Por último, si intentamos ejecutar un while para el que la condición nunca es igual a TRUE, este no realizará ninguna operación. conteo &lt;- 0 while(&quot;dado&quot; == &quot;ficha&quot;) { conteo &lt;- conteo + 1 } conteo ## [1] 0 "],["9-4-break-y-next.html", "9.4 break y next", " 9.4 break y next break y next son palabras reservadas en R, no podemos asignarles nuevos valores y realizan una operación específica cuando aparecen en nuestro código. break nos permite interrumpir un bucle, mientras que next nos deja avanzar a la siguiente iteración del bucle, saltándose la actual. Ambas funcionan para for y while. 9.4.1 Usando break Para interrumpir un bucle con break, necesitamos que se cumpla una condición. Cuando esto ocurre, el bucle se detiene, aunque existan elementos a los cuales aún podría aplicarse. Interrumpimos un for cuando i es igual a 3, aunque aún queden 7 elementos en el objeto. for(i in 1:10) { if(i == 3) { break } print(i) } ## [1] 1 ## [1] 2 Interrumpimos un while antes de se cumpla la condición de que numero sea mayor a 5, en cuanto este tiene el valor de 15. numero &lt;- 20 while(numero &gt; 5) { if(numero == 15) { break } numero &lt;- numero - 1 } numero ## [1] 15 Como habrás notado, la aplicación de break es muy similar a while, realizar una operación hasta que se cumple una condición, y ambos pueden usarse en conjunto. 9.4.2 Usando next Por su parte, usamos next para saltarnos una iteración en un bucle. Cuando la condición se cumple, esa iteración es omitida. for(i in 1:4) { if(i == 3) { next } print(i) } ## [1] 1 ## [1] 2 ## [1] 4 Estas dos estructuras de control nos dan un control fino sobre nuestro código. aunque los dos ejemplos de arriba son con for, también funcionan con while y repeat. En realidad, break es indispensable para repeat. "],["9-5-repeat.html", "9.5 repeat", " 9.5 repeat Este es un bucle que se llevará a cabo el número de veces que especifiquemos, usando un break para detenerse. repeat asegura que las operaciones que contiene sean iteradas al menos en una ocasión. La estructura de repeat es el siguiente: repeat { operaciones un_break_para_detener } Si no incluimos un break, el bucle se repetirá indefinidamente y sólo lo podremos detener pulsando la tecla ESC, así que hay que tener cuidado al usar esta estructura de control. Por ejemplo, el siguiente repeat sumará +1 a valor hasta que este sea igual a cinco, entonces se detendrá. valor &lt;- 0 mi_vector &lt;- NULL repeat{ valor &lt;- valor + 1 if(valor == 5) { break } } # Resultado valor ## [1] 5 Este tipo de bucle es quizás el menos utilizado de todos, pues en R existen alternativas para obtener los mismos resultados de manera más sencilla y sin el riesgo de crear un bucle infinito. Sin embargo, puede ser la mejor alternativa para problemas específicos. "],["10-la-familia-apply.html", "Capítulo 10 La familia apply", " Capítulo 10 La familia apply La familia de funciones apply es usada para aplicar una función a cada elemento de una estructura de datos. En particular, es usada para aplicar funciones en matrices, data frames, arrays y listas. Con esta familia de funciones podemos automatizar tareas complejas usando poca líneas de código y es una de las características distintivas de R como lenguaje de programación. La familia de funciones apply es una expresión de los rasgos del paradigma funcional de programación presentes en R. Sobre esto no profundizaremos demasiado, pero se refiere saber que en R las funciones son ciudadanos de primera, con la misma importancia que los objetos, y por lo tanto, operamos en ellas. La familia de funciones apply no sólo recibe datos como argumentos, también recibe funciones. 10.0.1 Un recordatorio sobre vectorización Para entender más fácilmente el uso de la familia 0, recordemos la vectorización de operaciones. Hay operaciones que, si las aplicamos a un vector, son aplicadas a todos sus elementos. mi_vector &lt;- 1:10 mi_vector ## [1] 1 2 3 4 5 6 7 8 9 10 mi_vector ^ 2 ## [1] 1 4 9 16 25 36 49 64 81 100 Lo anterior es, generalmente, preferible a escribir una operación para cada elemento o a usar un bucle for, como se describió en el capítulo sobre estructuras de control. Como todo lo que ocurre en R es una función, podemos decir que al vectorizar estamos aplicando una función a cada elemento de un vector. La familia de funciones apply nos permite implementar esto en estructuras de datos distintas a los vectores. 10.0.2 Las funciones de la familia apply La familia apply esta formada por las siguientes funciones: apply() eapply() lapply() mapply() rapply() sapply() tapply() vapply() Es una familia numerosa y esta variedad de funciones se debe a que varias de ellas tienen aplicaciones sumamente específicas. Todas las funciones de esta familia tienen una característica en común: reciben como argumentos a un objeto y al menos una función. Hasta ahora, todas las funciones que hemos usado han recibido como argumentos estructuras de datos, sean vectores, data frames o de otro tipo. Las funciones de la familia apply tienen la particularidad que pueden recibir a otra función como un argumento. Lo anterior puede sonar confuso, pero es más bien intuitivo al verlo implementado. Nosotros trabajaremos con las funciones más generales y de uso común de esta familia: apply() lapply() Estas dos funciones nos permitirán solucionar casi todos los problemas a los que nos encontremos. Además, conociendo su uso, las demás funciones de la familia apply serán relativamente fáciles de entender. "],["10-1-apply.html", "10.1 apply", " 10.1 apply apply aplica una función a todos los elementos de una matriz. La estructura de esta función es la siguiente. apply(X, MARGIN, FUN) apply tiene tres argumentos: X: Una matriz o un objeto que pueda coercionarse a una matriz, generalmente, un data frame. MARGIN: La dimensión (margen) que agrupará los elementos de la matriz X, para aplicarles una función. Son identificadas con números, 1 son renglones y 2 son columnas. FUN: La función que aplicaremos a la matriz X en su dimensión MARGIN. 10.1.1 ¿Qué es X X es una matriz o cualquier otro objeto que sea posible coercionar a una matriz. Esto es, principalmente, vectores y data frames. Recuerda que puedes coercionar objetos a matriz usando as.matrix() y puedes comprobar si un objeto es de esta clase con is.matrix(). # Creamos un data frame mi_df &lt;- data.frame(v1 = 1:3, v2 = 4:6) mi_df ## v1 v2 ## 1 1 4 ## 2 2 5 ## 3 3 6 # Coerción a matriz mi_matriz &lt;- as.matrix(mi_df) # Verificamos que sea matriz is.matrix(mi_matriz) ## [1] TRUE # Resultado mi_matriz ## v1 v2 ## [1,] 1 4 ## [2,] 2 5 ## [3,] 3 6 Aunque también podemos coercionar listas y arrays a matrices, los resultados que obtenemos no siempre son apropiados para apply(), por lo que no es recomendable usar estos objetos como argumentos. 10.1.2 ¿Qué es MARGIN? Recuerda que las matrices y los data frames están formadas por vectores y que estas estructuras tienen dos dimensiones, ordenadas en renglones y columnas. Esto lo vimos en en Matrices y arrays y Data frames. Para MARGIN: 1 es renglones. 2 es columnas. Por ejemplo, podemos usar apply() para obtener la sumatoria de los elementos de una matriz, por renglón. Creamos una matriz de cuatro renglones. matriz &lt;- matrix(1:14, nrow = 4) ## Warning in matrix(1:14, nrow = 4): la longitud de los datos [14] no es un ## submúltiplo o múltiplo del número de filas [4] en la matriz Aplicamos apply(), dando la función sum() el argumento FUN, nota que sólo necesitamos el nombre de la función, sin paréntesis. Por último, damos el argumento MARGIN = 1, para aplicar la función por renglón. apply(X = matriz, MARGIN = 1, FUN = sum) ## [1] 28 32 22 26 Esto es equivalente a hacer lo siguiente. sum(matriz[1, ]) ## [1] 28 sum(matriz[2, ]) ## [1] 32 sum(matriz[3, ]) ## [1] 22 sum(matriz[4, ]) ## [1] 26 Y naturalmente, es equivalente a hacer lo siguiente. Estamos aplicando una función a cada elemento de nuestra matriz. Los elementos son los renglones. Cada renglón es un vector. Cada vector es usado como argumento de la función. Si cambiamos el argumento MARGIN de MARGIN = 1 a MARGIN = 2, entonces la función se aplicará por columna. apply(X = matriz, MARGIN = 2, FUN = sum) ## [1] 10 26 42 30 En este caso, la función sum() ha sido aplicado a cada elementos de nuestra matriz, los elementos son las columnas, y cada columna es un vector. 10.1.3 ¿Qué es FUN? FUN es un argumento que nos pide el nombre de una función que se se aplicarla a todos los elementos de nuestra matriz. El ejemplo de la sección anterior aplicamos las funciones mean() y sum() usando sus nombres, sin paréntesis, esto es, sin especificar argumentos. Podemos dar como argumento cualquier nombre de función, siempre y cuando ésta acepte vectores como argumentos. Probemos cambiando el argumento FUN. Usaremos la función mean() para obtener la media de cada renglón y de cada columna. Aplicado a los renglones. apply(matriz, 1, mean) ## [1] 7.0 8.0 5.5 6.5 Aplicado a las columnas apply(matriz, 2, mean) ## [1] 2.5 6.5 10.5 7.5 Las siguientes llamadas a sd(), max() y quantile() se ejecutan sin necesidad de especificar argumentos. # Desviación estándar apply(matriz, 1, FUN = sd) ## [1] 5.163978 5.163978 4.434712 4.434712 # Máximo apply(matriz, 1, FUN = max) ## [1] 13 14 11 12 # Cuantiles apply(matriz, 1, FUN = quantile) ## [,1] [,2] [,3] [,4] ## 0% 1 2 1.0 2.0 ## 25% 4 5 2.5 3.5 ## 50% 7 8 5.0 6.0 ## 75% 10 11 8.0 9.0 ## 100% 13 14 11.0 12.0 10.1.4 ¿Cómo sabe FUN cuáles son sus argumentos? Recuerda que podemos llamar una función y proporcionar sus argumentos en orden, tal como fueron establecidos en su definición. Por lo tanto, el primer argumento que espera la función, será la X del apply(). Para ilustrar esto, usaremos la función quantile(). Llama ?quantile en la consola para ver su documentación. ?quantile quantile() espera siempre un argumento x, que debe ser un vector numérico, además tener varios argumentos adicionales. probs es un vector numérico con las probabilidades de las que queremos extraer cuantiles. na.rm, si le asignamos TRUE quitará de x los NA y NaN antes de realizar operaciones. names, si le asignamos TRUE, hará que el objeto resultado de la función tenga nombres. type espera un valor entre 1 y 9, para determinar el algoritmo usado para el cálculo de los cuantiles. En orden, el primer argumento es x, el segundo probs, y así sucesivamente. Cuando usamos quantile() en un apply(), el argumento x de la función será cada elemento de nuestra matriz. Es decir, los vectores como renglones o columnas de los que está constituida la matriz. Esto funcionará siempre y cuando los argumentos sean apropiados para la función. Si proporcionamos un argumento inválido, la función no se ejecutará y apply fallará. Por ejemplo, intentamos obtener cuantiles de las columnas de una matriz, en la que una de ellas es de tipo carácter. Creamos una matriz. matriz2 &lt;- matrix(c(1:2, &quot;a&quot;, &quot;b&quot;), nrow = 2) # Resultado Aplicamos la función y obtenemos un error. apply(matriz2, 2, quantile) ## Error in (1 - h) * qs[i]: argumento no-numérico para operador binario Por lo tanto, apply sólo puede ser usado con funciones que esperan vectores como argumentos. 10.1.5 ¿Qué pasa si deseamos utilizar los demás argumentos de una función con apply? En los casos en los que una función tiene recibe más de un argumento, asignamos los valores de estos del nombre de la función, separados por comas, usando sus propios nombres (a este procedimiento es al que se refiere el argumento ... descrito en la documentación de apply). Supongamos que deseamos encontrar los cuantiles de un vector, correspondientes a las probabilidades .33 y .66. Esto es definido con el argumento probs de esta función. Para ello, usamos quantile() y después de haber escrito el nombre de la función, escribimos el nombre del argumento probs y los valores que deseamos para este. apply(X = matriz, MARGIN = 2, FUN = quantile, probs = c(.33, .66)) ## [,1] [,2] [,3] [,4] ## 33% 1.99 5.99 9.99 1.99 ## 66% 2.98 6.98 10.98 12.78 Como podrás ver, hemos obtenido los resultados esperados. Si además deseamos que el resultado aparezca sin nombres, entonces definimos el valor del argumento names de la misma manera. apply(matriz, 2, quantile, probs = c(.33, .66), names = FALSE) ## [,1] [,2] [,3] [,4] ## [1,] 1.99 5.99 9.99 1.99 ## [2,] 2.98 6.98 10.98 12.78 De este modo es posible aplicar funciones complejas que aceptan múltiples argumentos, con la ventaja que usamos pocas líneas de código. 10.1.6 ¿Qué tipo de resultados devuelve apply? En los ejemplos anteriores, el resultado de apply() en algunas ocasiones fue un vector y en otros fue una matriz. Si aplicamos mean(), obtenemos como resultado un vector. mat_media &lt;- apply(matriz, 1, mean) class(mat_media) ## [1] &quot;numeric&quot; Pero si aplicamos quantile(), obtenemos una matriz. mat_cuant &lt;- apply(matriz, 1, quantile) class(mat_cuant) ## [1] &quot;matrix&quot; &quot;array&quot; Este comportamiento se debe a que apply() nos devolverá objetos del mismo tipo que la función aplicada devuelve. Dependiendo de la función, será el tipo de objeto que obtengamos. Sin embargo, este comportamiento puede causarte algunos problemas. En primer lugar, anterior te obliga a conocer de antemano el tipo del resultado que obtendrás, lo cual no siempre es fácil de determinar, en particular si las funciones que estás utilizando son poco comunes o tienen comportamientos poco convencionales. Cuando estás trabajando en proyectos en los que el resultado de una operación será usado en operaciones posteriores, corres el riesgo de que en alguna parte del proceso, un apply() te devuelva un resultado que te impida continuar adelante. Con algo de práctica es más o menos sencillo identificar problemas posibles con los resultados de apply(), pero es algo que debes tener en cuenta, pues puede explicar por qué tu código no funciona como esperabas. En este sentido, lapply() tiene la ventaja de que siempre devuelve una lista. "],["10-2-lapply.html", "10.2 lapply", " 10.2 lapply lapply() es un caso especial de apply(), diseñado para aplicar funciones a todos los elementos de una lista. La l de su nombre se refiere, precisamente, a lista. lapply() intentará coercionar a una lista el objeto que demos como argumento y después aplicará una función a todos sus elementos. lapply siempre nos devolverá una lista como resultado. A diferencia de apply, sabemos que siempre obtendremos un objeto de tipo lista después de aplicar una función, sin importar cuál función sea. Dado que en R todas las estructuras de datos pueden coercionarse a una lista, lapply() puede usarse en un número más amplio de casos que apply(), además de que esto nos permite utilizar funciones que aceptan argumentos distintos a vectores. La estructura de esta función es: lapply(X, FUN) En donde: X es una lista o un objeto coercionable a una lista. FUN es la función a aplicar. Estos argumentos son idéntico a los de apply(), pero a diferencia aquí no especificamos MARGIN, pues las listas son estructuras con una unidimensionales, que sólo tienen largo. 10.2.1 Usando lapply() Probemos lapply() aplicando una función a un data frame. Usaremos el conjunto de datos trees, incluido por defecto en R base. trees contiene datos sobre el grueso, alto y volumen de distinto árboles de cerezo negro. Cada una de estas variables está almacenada en una columna del data frame. Veamos los primeros cinco renglones de trees. trees[1:5, ] ## Girth Height Volume ## 1 8.3 70 10.3 ## 2 8.6 65 10.3 ## 3 8.8 63 10.2 ## 4 10.5 72 16.4 ## 5 10.7 81 18.8 Aplicamos la función mean(), usando su nombre. lapply(X = trees, FUN = mean) ## $Girth ## [1] 13.24839 ## ## $Height ## [1] 76 ## ## $Volume ## [1] 30.17097 Dado que un data frame está formado por columnas y cada columna es un vector atómico, cuando usamos lapply() , la función es aplicada a cada columna. lapply(), a diferencia de apply() no puede aplicarse a renglones. En este ejemplo, obtuvimos la media de grueso (Girth), alto (Height) y volumen (Volume), como una lista. Verificamos que la clase de nuestro resultado es una lista con class(). arboles &lt;- lapply(X = trees, FUN = mean) class(arboles) ## [1] &quot;list&quot; Esto es muy conveniente, pues la recomendación para almacenar datos en un data frame es que cada columna represente una variable y cada renglón un caso (por ejemplo, el enfoque tidy de Wickham (2014)). Por lo tanto, con lapply() podemos manipular y transformar datos, por variable. Al igual que con apply(), podemos definir argumentos adicionales a las funciones que usemos, usando sus nombres, después del nombre de la función. lapply(X = trees, FUN = quantile, probs = .8) ## $Girth ## 80% ## 16.3 ## ## $Height ## 80% ## 81 ## ## $Volume ## 80% ## 42.6 Si usamos lapply con una matriz, la función se aplicará a cada celda de la matriz, no a cada columna. Creamos una matriz. matriz &lt;- matrix(1:9, ncol = 3) # Resultado matriz ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 Llamamos a lapply(). lapply(matriz, quantile, probs = .8) ## [[1]] ## 80% ## 1 ## ## [[2]] ## 80% ## 2 ## ## [[3]] ## 80% ## 3 ## ## [[4]] ## 80% ## 4 ## ## [[5]] ## 80% ## 5 ## ## [[6]] ## 80% ## 6 ## ## [[7]] ## 80% ## 7 ## ## [[8]] ## 80% ## 8 ## ## [[9]] ## 80% ## 9 Para usar una matriz con lapply() y que la función se aplique a cada columna, primero la coercionamos a un data frame con la función as.data.frame() lapply(as.data.frame(matriz), quantile, probs = .8) ## $V1 ## 80% ## 2.6 ## ## $V2 ## 80% ## 5.6 ## ## $V3 ## 80% ## 8.6 Si deseamos aplicar una función a los renglones de una matriz, una manera de lograr es transponer la matriz con t() y después coercionar a un data frame. matriz_t &lt;- t(matriz) lapply(as.data.frame(matriz_t), quantile, probs = .8) ## $V1 ## 80% ## 5.8 ## ## $V2 ## 80% ## 6.8 ## ## $V3 ## 80% ## 7.8 Con vectores como argumento, lapply() aplicará la función a cada elementos del vector, de manera similar a una vectorización de operaciones. Por ejemplo, usamos lapply() para obtener la raíz cuadrada de un vector numérico del 1 al 4, con la función sqrt(). mi_vector &lt;- 1:4 lapply(mi_vector, sqrt) ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 1.414214 ## ## [[3]] ## [1] 1.732051 ## ## [[4]] ## [1] 2 10.2.2 Usando lapply() en lugar de un bucle for En muchos casos es posible reemplazar un bucle for() por un lapply(). De hecho, lapply() está haciendo lo mismo que un for(), está iterando una operación en todos los elementos de una estructura de datos. Por lo tanto, el siguiente código con un for() mi_vector &lt;- 6:12 resultado &lt;- NULL posicion &lt;- 1 for(numero in mi_vector) { resultado[posicion] &lt;- sqrt(numero) posicion &lt;- posicion + 1 } resultado ## [1] 2.449490 2.645751 2.828427 3.000000 3.162278 3.316625 3.464102  nos dará los mismos resultados que el siguiente código con lapply(). resultado &lt;- NULL resultado &lt;- lapply(mi_vector, sqrt) resultado ## [[1]] ## [1] 2.44949 ## ## [[2]] ## [1] 2.645751 ## ## [[3]] ## [1] 2.828427 ## ## [[4]] ## [1] 3 ## ## [[5]] ## [1] 3.162278 ## ## [[6]] ## [1] 3.316625 ## ## [[7]] ## [1] 3.464102 El código con lapply() es mucho más breve y más sencillo de entender, al menos para otros usuarios de R. El inconveniente es que obtenemos una lista como resultado en lugar de un vector, pero eso es fácil de resolver usando la función as.numeric() para hacer coerción a tipo numérico. as.numeric(resultado) ## [1] 2.449490 2.645751 2.828427 3.000000 3.162278 3.316625 3.464102 El siguiente código es la manera en la que usamos for() si deseamos aplicar una función a todas sus columnas, tiene algunas partes que no hemos discutido, pero es sólo para ilustrar la diferencia simplemente usar trees_max &lt;- lapply(trees, max). trees_max &lt;- NULL i &lt;- 1 columnas &lt;- ncol(trees) for(i in 1:columnas) { trees_max[i] &lt;- max(trees[, i]) i &lt;- i +1 } trees_max ## [1] 20.6 87.0 77.0 10.2.3 Usando lapply con listas Hasta hora hemos hablado de usar lapply() con objetos que pueden coercionarse a una lista, pero ¿qué pasa si usamos esta función con una lista que contiene a otros objetos? Pues la función se aplicará a cada uno de ellos. Por lo tanto, así podemos utilizar funciones que acepten todo tipo de objetos como argumento. Incluso podemos aplicar funciones a listas recursivas, es decir, listas de listas. Por ejemplo, obtendremos el coeficiente de correlación de cuatro data frames contenidos en una sola lista. Esto no es posible con apply(), porque sólo podemos usar funciones que aceptan vectores como argumentos, pero con lapply() no es ningún problema. Empezaremos creando una lista de data frames. Para esto, usaremos las función rnorm(), que genera números al azar y set.seed(), para que obtengas los mismos resultados aquí mostrados. rnorm() creara n números al azar (pseudoaleatorios, en realidad), sacados de una distribución normal con media 0 y desviación estándar 1. set.seed() es una función que fija los resultados de una generación de valores al azar. Cada que ejecutas rnorm() obtienes resultados diferentes, pero si das un número como argumento seed a set.seed(), siempre obtendrás los mismos números. # Fijamos seed set.seed(seed = 2018) # Creamos una lista con tres data frames dentro tablas &lt;- list( df1 = data.frame(a = rnorm(n = 5), b = rnorm(n = 5), c = rnorm(n = 5)), df2 = data.frame(d = rnorm(n = 5), e = rnorm(n = 5), f = rnorm(n = 5)), df3 = data.frame(g = rnorm(n = 5), h = rnorm(n = 5), i = rnorm(n = 5)) ) # Resultado tablas ## $df1 ## a b c ## 1 -0.42298398 -0.2647112 -0.6430347 ## 2 -1.54987816 2.0994707 -1.0300287 ## 3 -0.06442932 0.8633512 0.7124813 ## 4 0.27088135 -0.6105871 -0.4457721 ## 5 1.73528367 0.6370556 0.2489796 ## ## $df2 ## d e f ## 1 -1.0741940 1.2638637 -0.2401222 ## 2 -1.8272617 0.2501979 -1.0586618 ## 3 0.0154919 0.2581954 0.4194091 ## 4 -1.6843613 1.7855342 -0.2709566 ## 5 0.2044675 -1.2197058 -0.6318248 ## ## $df3 ## g h i ## 1 -0.2284119 -0.4897908 -0.3594423 ## 2 1.1786797 1.4105216 -1.2995363 ## 3 -0.2662727 -1.0752636 -0.8698701 ## 4 0.5281408 0.2923947 1.0543623 ## 5 -1.7686592 -0.2066645 -0.1486396 Para obtener el coeficiente de correlación usaremos la función cor(). Esta función acepta como argumento una data frame o una matriz. Con este objeto, calculará el coeficiente de correlación R de Pearson existente entre cada una de sus columnas. Como resultado obtendremos una matriz de correlación. Por ejemplo, este es el resultado de aplicar cor() a iris. cor(iris[1:4]) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Sepal.Length 1.0000000 -0.1175698 0.8717538 0.8179411 ## Sepal.Width -0.1175698 1.0000000 -0.4284401 -0.3661259 ## Petal.Length 0.8717538 -0.4284401 1.0000000 0.9628654 ## Petal.Width 0.8179411 -0.3661259 0.9628654 1.0000000 Con lapply aplicaremos cor() a cada uno de los data frames contenidos en nuestra lista. El resultado será una lista de matrices de correlaciones. Esto lo logramos con una línea de código. lapply(X = tablas, FUN = cor) ## $df1 ## a b c ## a 1.0000000 -0.4427336 0.6355358 ## b -0.4427336 1.0000000 -0.1057007 ## c 0.6355358 -0.1057007 1.0000000 ## ## $df2 ## d e f ## d 1.0000000 -0.6960942 0.4709283 ## e -0.6960942 1.0000000 0.2624429 ## f 0.4709283 0.2624429 1.0000000 ## ## $df3 ## g h i ## g 1.0000000 0.6228793 -0.1472657 ## h 0.6228793 1.0000000 -0.1211321 ## i -0.1472657 -0.1211321 1.0000000 De esta manera puedes manipular información de múltiples data frames, matrices o listas con muy pocas líneas de código y, en muchos casos, más rápidamente que con las alternativas existentes. Finalmente, si asignamos los resultados de las última operación a un objeto, podemos usarlos y manipularlos de la misma manera que cualquier otra lista. correlaciones &lt;- lapply(tablas, cor) # Extraemos el primer elemento de la lista correlaciones[[1]] ## a b c ## a 1.0000000 -0.4427336 0.6355358 ## b -0.4427336 1.0000000 -0.1057007 ## c 0.6355358 -0.1057007 1.0000000 "],["11-importar-y-exportar-datos.html", "Capítulo 11 Importar y exportar datos", " Capítulo 11 Importar y exportar datos Hasta ahora, hemos trabajado con datos ya existentes en R base o que hemos generado nosotros mismos, sin embargo, lo usual es que usemos datos almacenados en archivos fuera de R. R puede importar datos de una amplia variedad de tipos de archivo con las funciones en base además de que esta capacidad es ampliada con el uso de paquetes específicos. Cuando importamos un archivo, estamos guardando su contenido en nuestra sesión como un objeto. Dependiendo del procedimiento que usemos será el tipo de objeto creado. De manera análoga, podemos exportar nuestros objetos de R a archivos en nuestra computadora. "],["11-1-descargando-datos.html", "11.1 Descargando datos", " 11.1 Descargando datos Antes de empezar a importar datos, vale la pena señalar que podemos descargar archivos de internet usando R con la función download.file(). De esta manera tendremos acceso a una vasta diversidad de fuentes de datos. Entre otras, podrás descargar los archivos La función download.file() nos pide como argumento url, la dirección de internet del archivo que queremos descargar y destfile el nombre que tendrá el archivo en nuestra computadora. Ambos argumentos como cadenas de texto, es decir, entre comillas. Por ejemplo, para descargar una copia del set iris disponible en el UCI Machine Learning Repository usamos la siguiente dirección como argumento url: https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data Y asignamos iris.data al argumento dest. download.file( url = &quot;https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data&quot;, destfile = &quot;iris.data&quot; ) El resultado es un archivo llamado iris.data en nuestro directorio de trabajo. Este método funciona con prácticamente todo tipo de archivos, aunque en algunos casos será necesario agregar el argumento method = \"wb\", por asegurar que el archivo obtenido funcione correctamente. "],["11-2-tablas-datos-rectangulares.html", "11.2 Tablas (datos rectangulares)", " 11.2 Tablas (datos rectangulares) Como vimos en el capítulo 7, las estructura rectangular, en renglones y columnas, es común y conveniente para el análisis de datos. Nos referiremos a esta forma de organizar datos como tabla. R cuenta con la función genérica read.table(), que puede leer cualquier tipo de archivo que contenga una tabla. La condición para que R interprete un archivo como una tabla es que tenga renglones y en cada renglón, los datos estén separados por comas, o algún otro carácter, indicando columnas. Es decir, algo que luzca de la siguiente manera. 1, 20, 8, 5 1, 31, 6, 5 2, 18, 9, 5 2, 25, 10, 5 Por supuesto, en lugar de comas podemos tener puntos y coma, dos puntos, tabuladores o cualquier otro signo de puntuación como separador de columnas. La función read.table() acepta un número considerable de argumentos. Los más importantes son los siguientes. file: La ruta del archivo que importaremos, como cadena de texto. Si el archivo se encuentra en nuestro directorio de trabajo, es suficiente dar el nombre del archivo, sin la ruta completa. header: Si nuestro archivo tiene encabezados, para ser interpretados como nombres de columna, definimos este argumento como TRUE. sep: El carácter que es usado como separador de columnas. Por defecto es ; col.names: Un vector opcional, de tipo carácter, con los nombres de las columnas en la tabla. stringsAsFactors: Esta función convierte automáticamente los datos de texto a factores. Si este no es el comportamiento que deseamos, definimos este argumento como FALSE. Puedes consultar todos los argumentos de esta función ejecutando ?read.table en la consola. Es importante señalar que el objeto obtenido al usar esta función es siempre un data frame. Probemos con un archivo con extensión .data, descargado desde el repositorio de Github de este libro. download.file( url = &quot;https://raw.githubusercontent.com/jboscomendoza/r-principiantes-bookdown/master/datos/breast-cancer-wis.data&quot;, dest = &quot;breast-cancer-wis.data&quot; ) Estos datos pertenecen a una base de diagnósticos de cáncer mamario de la Universidad de Wisconsin, usado para probar métodos de aprendizaje automático. Puedes encontrar la información completa sobre este conjunto de datos en el siguiente enlace: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29 Nos damos cuenta de que hemos tenido éxito en la descarga si aparece un mensaje en la consola de R indicando los resultados de nuestra operación. Usamos sin especificar ningún otro argumento. bcancer &lt;- read.table(file = &quot;datos/breast-cancer-wis.data&quot;) Veamos los primeros renglones de nuestros datos usando la función head() head(bcancer) ## V1 ## 1 1000025,5,1,1,1,2,1,3,1,1,2 ## 2 1002945,5,4,4,5,7,10,3,2,1,2 ## 3 1015425,3,1,1,1,2,2,3,1,1,2 ## 4 1016277,6,8,8,1,3,4,3,7,1,2 ## 5 1017023,4,1,1,3,2,1,3,1,1,2 ## 6 1017122,8,10,10,8,7,10,9,7,1,4 Nuestros datos no lucen particularmente bien. Necesitamos ajustar algunos parámetros al importarlos. No hay datos de encabezado, por lo que header será igual a FALSE y el separador de columnas es una coma, así que el valor de sep será , No conocemos cuál es el nombre de las columnas, así que por el momento no proporcionaremos uno. bcancer &lt;- read.table(file = &quot;datos/breast-cancer-wis.data&quot;, header = FALSE, sep = &quot;,&quot;) # Resultado head(bcancer) ## V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 ## 1 1000025 5 1 1 1 2 1 3 1 1 2 ## 2 1002945 5 4 4 5 7 10 3 2 1 2 ## 3 1015425 3 1 1 1 2 2 3 1 1 2 ## 4 1016277 6 8 8 1 3 4 3 7 1 2 ## 5 1017023 4 1 1 3 2 1 3 1 1 2 ## 6 1017122 8 10 10 8 7 10 9 7 1 4 Luce mejor, pero los nombres de las columnas son poco descriptivos. Si no damos nombres de variables, cada columna tendrá como nombre V seguida de números del 1 adelante. Para este ejemplo, contamos con un archivo de información, que describe el contenido de los datos que hemos importado. https://raw.githubusercontent.com/jboscomendoza/r-principiantes-bookdown/master/datos/breast-cancer-wis.names Si descargas este archivo, puedes abrirlo usando el bloc o navegador de internet de tu computadora. Guardaremos en un vector las abreviaturas de los nombres de columna descritos en el documento anterior. nombres &lt;- c(&quot;id&quot;, &quot;clump_t&quot;, &quot;u_csize&quot;, &quot;u_cshape&quot;, &quot;m_adh&quot;, &quot;spcs&quot;, &quot;b_nuc&quot;, &quot;b_chr&quot;, &quot;n_nuc&quot;, &quot;mit&quot;, &quot;class&quot;) Ahora usaremos este vector como argumento col.names en read.table(), para importar nuestros datos con nombres de columna. bcancer &lt;- read.table(file = &quot;datos/breast-cancer-wis.data&quot;, header = FALSE, sep = &quot;,&quot;, col.names = nombres) # Resultado head(bcancer) ## id clump_t u_csize u_cshape m_adh spcs b_nuc b_chr n_nuc mit class ## 1 1000025 5 1 1 1 2 1 3 1 1 2 ## 2 1002945 5 4 4 5 7 10 3 2 1 2 ## 3 1015425 3 1 1 1 2 2 3 1 1 2 ## 4 1016277 6 8 8 1 3 4 3 7 1 2 ## 5 1017023 4 1 1 3 2 1 3 1 1 2 ## 6 1017122 8 10 10 8 7 10 9 7 1 4 Nuestros datos han sido importados correctamente. Además, el objeto resultante es un data frame, listo para que trabajemos con él. class(bcancer) ## [1] &quot;data.frame&quot; 11.2.1 Archivos CSV Un caso particular de las tablas, son los archivos separados por comas, con extensión .csv, por Comma Separated Values, sus siglas en inglés. Este es un tipo de archivo comúnmente usado para compartir datos, pues es compatible con una amplia variedad de sistemas diferentes además de que ocupa relativamente poco espacio de almacenamiento. Este tipo de archivos también se pueden importar usando la función read.table(). Probemos descargando los mismos datos que en el ejemplo anterior, pero almacenados en un archivo con extensión .csv. download.file( url = &quot;https://raw.githubusercontent.com/jboscomendoza/r-principiantes-bookdown/master/datos/breast-cancer-wis.csv&quot;, dest = &quot;breast-cancer-wis.csv&quot; ) Podemos usar read.table() con los mismos argumentos que en el ejemplo anterior, con la excepción de que este archivo sí tiene encabezados de columna, por lo que cambiamos header de FALSE a TRUE. bcancer &lt;- read.table(file = &quot;datos/breast-cancer-wis.csv&quot;, header = TRUE, sep = &quot;,&quot;, col.names = nombres) # Resultado head(bcancer) ## id clump_t u_csize u_cshape m_adh spcs b_nuc b_chr n_nuc mit class ## 1 1000025 5 1 1 1 2 1 3 1 1 2 ## 2 1002945 5 4 4 5 7 10 3 2 1 2 ## 3 1015425 3 1 1 1 2 2 3 1 1 2 ## 4 1016277 6 8 8 1 3 4 3 7 1 2 ## 5 1017023 4 1 1 3 2 1 3 1 1 2 ## 6 1017122 8 10 10 8 7 10 9 7 1 4 Una ventaja de usar documentos con extensión .csv es la posibilidad de usar la función read.csv(). Esta es una es una versión de read.table(), optimizada para importar archivos .csv. read.csv() acepta los mismos argumentos que read.table(), pero al usarla con un archivo .csv, en casi todo los casos, no hará falta especificar nada salvo la ruta del archivo. bcancer &lt;- read.csv(&quot;datos/breast-cancer-wis.csv&quot;) # Resultado head(bcancer) ## id clump_t u_csize u_cshape m_adh spcs b_nuc b_chr n_nuc mit class ## 1 1000025 5 1 1 1 2 1 3 1 1 2 ## 2 1002945 5 4 4 5 7 10 3 2 1 2 ## 3 1015425 3 1 1 1 2 2 3 1 1 2 ## 4 1016277 6 8 8 1 3 4 3 7 1 2 ## 5 1017023 4 1 1 3 2 1 3 1 1 2 ## 6 1017122 8 10 10 8 7 10 9 7 1 4 read.csv() también devuelve un data frame como resultado "],["11-3-archivos-con-una-estructura-desconocida.html", "11.3 Archivos con una estructura desconocida", " 11.3 Archivos con una estructura desconocida Habrá ocasiones en las que no estamos seguros del contenido de los archivos que deseamos importar. En estos casos, podemos pedirle a R que intente abrir el archivo en cuestión, usando la función file.show(). Por ejemplo, intentamos abrir el archivo con extensión .csv que importamos antes. file.show(&quot;datos/breast-cancer-wis.csv&quot;) R intentará usar el programa que en nuestro equipo, por defecto, abre el tipo de archivo que le hemos indicado. Si no tenemos un programa configurado para abrir el tipo de archivo que deseamos, nuestro sistema operativo nos pedirá que elijamos uno. Lo anterior puede ocurrir si intentas abrir el archivo con extensión .data que hemos importado en este capítulo. file.show(&quot;datos/breast-cancer-wis.data&quot;) Podemos usar la función readLines() para leer un archivo línea por línea. Establecemos el argumento n = 4 para obtener sólo los primeros cuatro renglones del documento. readLines(&quot;datos/breast-cancer-wis.data&quot;, n = 4) ## [1] &quot;1000025,5,1,1,1,2,1,3,1,1,2&quot; &quot;1002945,5,4,4,5,7,10,3,2,1,2&quot; ## [3] &quot;1015425,3,1,1,1,2,2,3,1,1,2&quot; &quot;1016277,6,8,8,1,3,4,3,7,1,2&quot; La salida es una lista de vectores, uno por linea en el archivo. Observando la salida de readLines() podremos determinar si el archivo que nos interesa puede ser importado usando con los métodos que hemos revisado o necesitaremos de herramientas diferentes. El documento R Data Import/Export (R Core Team, 2018) contiene una guía avanzada sobre el proceso de importar y exportar todo tipo de datos. Puedes consultarlo en el siguiente enlace: https://cran.r-project.org/doc/manuals/r-release/R-data.pdf "],["11-4-exportar-datos.html", "11.4 Exportar datos", " 11.4 Exportar datos Un paso muy importante en el trabajo con R es exportar los datos que hemos generado, ya sea para que sean usados por otras personas o para almacenar información en nuestro disco duro en lugar de nuestro RAM. Dependiendo del tipo de estructura de dato en el que se encuentran contenidos nuestros datos son las opciones que tenemos para exportarlos. 11.4.1 Data frames y matrices Si nuestros datos se encuentran contenidos en una estructura de datos rectangular, podemos exportarlos con diferentes funciones. De manera análoga a read.table(), la función write.table() nos permite exportar matrices o data frames, como archivos de texto con distintas extensiones. Los argumentos más usados de write.table() son los siguientes. x: El nombre del data frame o matriz a exportar. file: El nombre, extensión y ruta del archivo creado con esta función. Si sólo escribimos el nombre del archivo, este será creado en nuestro directorio de trabajo. sep: El carácter que se usará como separador de columnas. row.names: Si deseamos incluir el nombre de los renglones en nuestro objeto al exportarlo, establecemos este argumento como TRUE. En general, es recomendable fijarlo como FALSE, para conservar una estructura tabular más fácil de leer. col.names: Si deseamos que el archivo incluya los nombres de las columnas en nuestro objeto, establecemos este argumento como TRUE. Es recomendable fijarlo como TRUE para evitar la necesidad de almacenar los nombres de columna en documentos distintos. Puedes consultar todos los argumentos de esta función ejecutando ?write.table. Probemos exportando el objeto iris a un documento de texto llamado iris.txt a nuestro directorio de trabajo, usando como separador la coma, con nombres de columnas y sin nombre de renglones. write.table(x = iris, file = &quot;iris.txt&quot;, sep = &quot;,&quot;, row.names = FALSE, col.names = TRUE) Importemos el archivo que hemos creado usando read.table(). iris_txt &lt;- read.table(file = &quot;iris.txt&quot;, header = TRUE, sep = &quot;,&quot;) # Resultado head(iris_txt) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa También podemos exportar datos a archivos con extensión .csv con la función write.csv(). Vamos a exportar iris como un documento .csv. En este caso, sólo especificamos que no deseamos guardar los nombres de los renglones con row.names = FALSE. write.csv(x = iris, file = &quot;iris.csv&quot;, row.names = FALSE) Importamos el archivo creado. iris_csv &lt;- read.csv(&quot;iris.csv&quot;) # Resultado head(iris_csv) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa 11.4.2 Listas La manera más sencilla de exportar listas es guardarlas en archivos RDS. Este es un tipo de archivo nativo de R que puede almacenar cualquier objeto a un archivo en nuestro disco duro. Además, RDS comprime los datos que almacena, por lo que ocupa menos espacio en disco duro que otros tipos de archivos, aunque contengan la misma información. Para exportar un objeto a un archivo RDS, usamos la función saveRDS() que siempre nos pide dos argumentos: object: El nombre del objeto a exportar. file: El nombre y ruta del archivo que crearemos. Los archivos deben tener la extensión .rds. Si no especificamos una ruta completa, el archivo será creado en nuestro directorio de trabajo. Creamos una lista de ejemplo que contiene dos vectores y dos matrices mi_lista &lt;- list(&quot;a&quot; = c(TRUE, FALSE, TRUE), &quot;b&quot; = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), &quot;c&quot; = matrix(1:4, ncol = 2), &quot;d&quot; = matrix(1:6, ncol = 3)) # Resultado mi_lista ## $a ## [1] TRUE FALSE TRUE ## ## $b ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; ## ## $c ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 ## ## $d ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 Aunque podemos intentar write.table() para exportar listas, por lo general obtendremos un error como resultado. Tratamos de exportar la lista anterior como un archivo .txt. write.table(x = mi_lista, file = &quot;mi_lista.txt&quot;) ## Error in (function (..., row.names = NULL, check.rows = FALSE, check.names = TRUE, : arguments imply differing number of rows: 3, 2 Usamos la función saveRDS() para exportar al archivo mi_lista.rds. saveRDS(object = mi_lista, file = &quot;mi_lista.rds&quot;) Si deseamos importar un archivo RDS a R, usamos la función readRDS(), indicando la ruta en la que se encuentra el archivo que deseamos. Intentemos importar el archivo mi_lista.rds. mi_lista_importado &lt;- readRDS(file = &quot;mi_lista.rds&quot;) Vamos el resultado. mi_lista_importado ## $a ## [1] TRUE FALSE TRUE ## ## $b ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; ## ## $c ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 ## ## $d ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 # El resultado es una lista, al igual que el objeto original class(mi_lista) ## [1] &quot;list&quot; Los objetos importados usando un archivo RDS conservan los tipos y clases que tenían originalmente, lo cual previene pérdida de información. "],["11-5-hojas-de-cálculo-de-excel.html", "11.5 Hojas de cálculo de Excel", " 11.5 Hojas de cálculo de Excel Un formato usado con mucha frecuencia para almacenar archivos son las hojas de cálculo, en particular las generadas por el paquete Microsoft Excel. R base no tiene una función para importar archivos almacenados en archivos con extensión .xsl y .xslx, creados con Excel. Para importar datos desde este tipo de archivos, necesitamos instalar el paquete readxl, que contiene funciones específicas para realizar esta tarea. Usamos la función installpackages(), como lo vimos en el capítulo 3 install.packages(&quot;readxl&quot;) Ya instalado, cargamos el readxl a nuestra sesión de trabajo. library(readxl) Usaremos, principalmente dos funciones de este paquete. read_excel(): Para importar archivos .xls y xlsx. excel_sheets(): Para obtener los nombres de las pestañas en una hoja de cálculo de Excel. Para probar estas funciones, descargaremos una hoja de cálculo de prueba. Nota que hemos establecido el argumento mode = \"wb\" para asegurar que el archivo se descargue correctamente. download.file( url = &quot;https://github.com/jboscomendoza/r-principiantes-bookdown/raw/master/datos/data_frames.xlsx&quot;, destfile = &quot;datos/data_frames.xlsx&quot;, mode = &quot;wb&quot; ) Si intentamos leer las primeras cinco líneas de data_frames.xlsx, confirmamos que este es un archivo que no tiene forma rectangular, de tabla. readLines(&quot;datos/data_frames.xlsx&quot;, n = 5) ## Warning in readLines(&quot;datos/data_frames.xlsx&quot;, n = 5): line 1 appears to contain ## an embedded nul ## Warning in readLines(&quot;datos/data_frames.xlsx&quot;, n = 5): incomplete final line ## found on &#39;datos/data_frames.xlsx&#39; ## [1] &quot;PK\\003\\004\\024&quot; ## [2] &quot;\\177ßYTU,B õ(±çmöL\\177¸jêd\\t\\001\\215³¹èf\\035\\200-6v¯É{ú$\\022$eµª\\235\\\\¬\\001Åpp\\177×¬=`ÂÕ\\026sQ\\021ùg)±¨ Q\\2309\\017WJ\\027&quot; En caso de que tengamos instalado Excel o algún otro programa compatible con archivos de hoja de cálculo, como LibreOffice Calc o Number, podemos pedir a R que abra este archivo con file.show(). De este modo podemos explorar su contenido. file.show(&quot;datos/data_frames.xlsx&quot;) La función excel_sheets() nos devuelve el nombre de las pestañas como un vector. excel_sheets(&quot;datos/data_frames.xlsx&quot;) ## [1] &quot;iris&quot; &quot;trees&quot; Este archivo tiene dos pestañas, llamadas iris y trees. Intentaremos importar la pestaña iris con read_excel(). Esta función tiene los siguientes argumentos principales. path: La ruta del archivo a importar. Si no especificamos una ruta completa, será buscado en nuestro directorio de trabajo. sheet: El nombre de la pestaña a importar. Si no especificamos este argumento, read_excel() intentará leer la primera pestaña de la hoja de cálculo. range: Cadena de texto con el rango de celdas a importar, escrito con el formato usado en Excel. Por ejemplo, A1:B:10. col_names: Con este argumento indicamos si la pestaña que vamos a importar tiene encabezados para usar como nombres de columna. Por defecto su valor es TRUE. Si no tenemos encabezados, podemos dar un vector con nombres para asignar a las columnas. Puedes consultar todos los argumentos de esta función ejecutando ?read_excel. Probemos read_excel(). iris_excel &lt;- read_excel(path = &quot;datos/data_frames.xlsx&quot;, sheet = &quot;iris&quot;) Nuestro resultado es un data frame. iris_excel ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # ... with 140 more rows Si los datos en la hoja de cálculo tienen forma de tabla, read_excel() no tendrá problemas para importarlos. Cuando este no es el caso, usamos el argumento range para extraer sólo la información que nos interesa. Intentamos importar la pestaña trees. trees_excel &lt;- read_excel(path = &quot;datos/data_frames.xlsx&quot;, sheet = &quot;trees&quot;) ## New names: ## * `` -&gt; ...2 ## * `` -&gt; ...3 ## * `` -&gt; ...4 ## * `` -&gt; ...5 ## * `` -&gt; ...6 # Resultado trees_excel ## # A tibble: 34 x 6 ## `Datos trees` ...2 ...3 ...4 ...5 ...6 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; ## 1 &lt;NA&gt; NA NA NA NA &lt;NA&gt; ## 2 &lt;NA&gt; 8.3 70 10.3 NA &lt;NA&gt; ## 3 &lt;NA&gt; 8.6 65 10.3 NA &lt;NA&gt; ## 4 &lt;NA&gt; 8.8 63 10.2 NA Los nombres de las variables son: Girt~ ## 5 &lt;NA&gt; 10.5 72 16.4 NA &lt;NA&gt; ## 6 &lt;NA&gt; 10.7 81 18.8 NA &lt;NA&gt; ## 7 &lt;NA&gt; 10.8 83 19.7 NA &lt;NA&gt; ## 8 &lt;NA&gt; 11 66 15.6 NA &lt;NA&gt; ## 9 &lt;NA&gt; 11 75 18.2 NA &lt;NA&gt; ## 10 &lt;NA&gt; 11.1 80 22.6 NA &lt;NA&gt; ## # ... with 24 more rows Los resultados no lucen bien porque los datos en la pestaña no tienen forma de tabla. Ajustamos los argumentos de read_excel() para leer correctamente la información de la pestaña. Al explorar manualmente el archivo data.frames.xlsx, podemos localizar el rango en el que se encuentran los datos (de las celdas B3 a D33) y los nombres de las columnas (Girth, Height y Volume). Probemos importar de nuevo con esta información. trees_excel &lt;- read_excel(path = &quot;datos/data_frames.xlsx&quot;, sheet = &quot;trees&quot;, range = &quot;B3:D33&quot;, col_names = c(&quot;Girth&quot;, &quot;Height&quot;, &quot;Volume&quot;)) # Resultado trees_excel ## # A tibble: 31 x 3 ## Girth Height Volume ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 8.3 70 10.3 ## 2 8.6 65 10.3 ## 3 8.8 63 10.2 ## 4 10.5 72 16.4 ## 5 10.7 81 18.8 ## 6 10.8 83 19.7 ## 7 11 66 15.6 ## 8 11 75 18.2 ## 9 11.1 80 22.6 ## 10 11.2 75 19.9 ## # ... with 21 more rows Esta vez hemos tenido éxito y los datos importados son los correctos. El paquete readxl tiene más funciones para trabajar con hojas de cálculo además de read_excel() y excel_sheets(), pero revisar cada una de ellas sale del alcance de este libro. Puedes conocer más sobre ellas en la documentación de readxl, llamando help(package = \"readxl\"). "],["11-6-datos-de-paquetes-estadísticos-comerciales-spss-sas-y-stata.html", "11.6 Datos de paquetes estadísticos comerciales (SPSS, SAS y STATA)", " 11.6 Datos de paquetes estadísticos comerciales (SPSS, SAS y STATA) En ciertas disciplinas, el uso de determinados paquetes estadísticos comerciales es sumamente común. Si Por ejemplo, en Psicología el paquete SPSS Statistics de IBM es el paquete estadístico comercial más usado. Si eres psicólogo o psicóloga, o colaboras con psicólogos, es altamente probable que te encuentres con datos contenidos en archivos con extensión .sav, el tipo de archivo nativo de SPSS Statistics. Por lo tanto, es conveniente ser capaces de importar y exportar datos almacenados en archivos compatibles con paquetes estadísticos comerciales, pues esto nos permitirá usar datos ya existentes compatibles con ellos y colaborar con otras personas. Para este fin, usamos el paquete haven. install.packages(&quot;haven&quot;) Para usar las funciones de haven, lo cargamos a nuestra sesión de trabajo. library(haven) Las siguientes funciones de haven son usadas para importar datos. Todas estas funciones nos piden como argumento file la ruta y nombre del archivo a importar, si no especificamos ruta, será buscado en nuestro directorio de trabajo. read_spss(): SPSS Statistics, archivos con extensión sav, zsav y por. read_sav(): SPSS Statistics, sólo archivos sav, zsav. read_sas(): SAS, archivos sas7bdat. read_xpt: SAS, archivos xpt. read_stata(): Stata, archivos dta. Todas importan los datos como un data frame. También podemos exportar nuestros data frames creados en R como archivos compatibles con estos programas con las siguientes funciones. Todas piden el argumento file, con la ruta y nombre del archivo a crear. Es muy importante que demos como nombre de archivo uno con la extensión correcta para cada paquete. write_sav(): SPSS Statistics, archivos sav, zsav o por. write_sas(): SAS, archivos sas7bda. write_xpt(): SAS, archivos xpt. write_dta(): Stata, archivos dta. Como siempre, puedes leer sobre las demás funciones en el paquete haven en su documentación, llamando help(package = \"haven\"). "],["12-gráficas.html", "Capítulo 12 Gráficas", " Capítulo 12 Gráficas R cuenta con un sistema de generación de gráficas poderoso y flexible. Sin embargo, tener estar cualidades hace que este sistema sea un tanto complejo para aprender. En este capítulo revisaremos como crear las gráficas más comunes con R base, así como algunos de los parámetros que podemos ajustar para mejorar su presentación. Al crear gráficas, notarás que ponemos en práctica todo lo que hemos visto en los capítulos anteriores, incluyendo importar datos, hacer subconjuntos de un objeto y uso de funciones. "],["12-1-datos-usados-en-el-capítulo.html", "12.1 Datos usados en el capítulo", " 12.1 Datos usados en el capítulo Para las siguientes secciones utilizaremos de nuevo una copia de los datos disponibles en el UCI Machine Learning Repository. Usaremos un conjunto de datos llamado Bank Marketing Data Set, que contiene información de personas contactadas en una campaña de marketing directo puesta en marcha por un banco de Portugal. Comenzamos con la descarga de la copia del archivo csv desde el sitio de Github de este libro. download.file( url = &quot;https://raw.githubusercontent.com/jboscomendoza/r-principiantes-bookdown/master/datos/bank.csv&quot;, destfile = &quot;datos/bank.csv&quot; ) Damos un vistazo al contenido del archivo bank.csv con readLines(). readLines(&quot;datos/bank.csv&quot;, n = 4) ## [1] &quot;\\&quot;age\\&quot;;\\&quot;job\\&quot;;\\&quot;marital\\&quot;;\\&quot;education\\&quot;;\\&quot;default\\&quot;;\\&quot;balance\\&quot;;\\&quot;housing\\&quot;;\\&quot;loan\\&quot;;\\&quot;contact\\&quot;;\\&quot;day\\&quot;;\\&quot;month\\&quot;;\\&quot;duration\\&quot;;\\&quot;campaign\\&quot;;\\&quot;pdays\\&quot;;\\&quot;previous\\&quot;;\\&quot;poutcome\\&quot;;\\&quot;y\\&quot;&quot; ## [2] &quot;30;\\&quot;unemployed\\&quot;;\\&quot;married\\&quot;;\\&quot;primary\\&quot;;\\&quot;no\\&quot;;1787;\\&quot;no\\&quot;;\\&quot;no\\&quot;;\\&quot;cellular\\&quot;;19;\\&quot;oct\\&quot;;79;1;-1;0;\\&quot;unknown\\&quot;;\\&quot;no\\&quot;&quot; ## [3] &quot;33;\\&quot;services\\&quot;;\\&quot;married\\&quot;;\\&quot;secondary\\&quot;;\\&quot;no\\&quot;;4789;\\&quot;yes\\&quot;;\\&quot;yes\\&quot;;\\&quot;cellular\\&quot;;11;\\&quot;may\\&quot;;220;1;339;4;\\&quot;failure\\&quot;;\\&quot;no\\&quot;&quot; ## [4] &quot;35;\\&quot;management\\&quot;;\\&quot;single\\&quot;;\\&quot;tertiary\\&quot;;\\&quot;no\\&quot;;1350;\\&quot;yes\\&quot;;\\&quot;no\\&quot;;\\&quot;cellular\\&quot;;16;\\&quot;apr\\&quot;;185;1;330;1;\\&quot;failure\\&quot;;\\&quot;no\\&quot;&quot; Por la estructura de los datos, podremos usar la función read.csv(), con el argumento sep = \";\" para importarlos como un data frame. banco &lt;- read.csv(file = &quot;datos/bank.csv&quot;, sep = &quot;;&quot;) Vemos las primeras líneas del conjunto con head(), el número de renglones y columnas con dim(). # Primeros datos head(banco) ## age job marital education default balance housing loan contact day ## 1 30 unemployed married primary no 1787 no no cellular 19 ## 2 33 services married secondary no 4789 yes yes cellular 11 ## 3 35 management single tertiary no 1350 yes no cellular 16 ## 4 30 management married tertiary no 1476 yes yes unknown 3 ## 5 59 blue-collar married secondary no 0 yes no unknown 5 ## 6 35 management single tertiary no 747 no no cellular 23 ## month duration campaign pdays previous poutcome y ## 1 oct 79 1 -1 0 unknown no ## 2 may 220 1 339 4 failure no ## 3 apr 185 1 330 1 failure no ## 4 jun 199 4 -1 0 unknown no ## 5 may 226 1 -1 0 unknown no ## 6 feb 141 2 176 3 failure no # Dimensiones dim(banco) ## [1] 4521 17 Usamos lapply() con la función class() para determinar el tipo de dato de cada columna en banco. Conocer esto nos será muy útil más adelante. lapply(banco, class) ## $age ## [1] &quot;integer&quot; ## ## $job ## [1] &quot;character&quot; ## ## $marital ## [1] &quot;character&quot; ## ## $education ## [1] &quot;character&quot; ## ## $default ## [1] &quot;character&quot; ## ## $balance ## [1] &quot;integer&quot; ## ## $housing ## [1] &quot;character&quot; ## ## $loan ## [1] &quot;character&quot; ## ## $contact ## [1] &quot;character&quot; ## ## $day ## [1] &quot;integer&quot; ## ## $month ## [1] &quot;character&quot; ## ## $duration ## [1] &quot;integer&quot; ## ## $campaign ## [1] &quot;integer&quot; ## ## $pdays ## [1] &quot;integer&quot; ## ## $previous ## [1] &quot;integer&quot; ## ## $poutcome ## [1] &quot;character&quot; ## ## $y ## [1] &quot;character&quot; Y por último, pedimos un resumen de nuestros datos con la función summary(). Esta función acepta cualquier tipo de objeto como argumento y nos devuelve un resumen descriptivo de los datos de cada uno de sus elementos. summary(banco) ## age job marital education ## Min. :19.00 Length:4521 Length:4521 Length:4521 ## 1st Qu.:33.00 Class :character Class :character Class :character ## Median :39.00 Mode :character Mode :character Mode :character ## Mean :41.17 ## 3rd Qu.:49.00 ## Max. :87.00 ## default balance housing loan ## Length:4521 Min. :-3313 Length:4521 Length:4521 ## Class :character 1st Qu.: 69 Class :character Class :character ## Mode :character Median : 444 Mode :character Mode :character ## Mean : 1423 ## 3rd Qu.: 1480 ## Max. :71188 ## contact day month duration ## Length:4521 Min. : 1.00 Length:4521 Min. : 4 ## Class :character 1st Qu.: 9.00 Class :character 1st Qu.: 104 ## Mode :character Median :16.00 Mode :character Median : 185 ## Mean :15.92 Mean : 264 ## 3rd Qu.:21.00 3rd Qu.: 329 ## Max. :31.00 Max. :3025 ## campaign pdays previous poutcome ## Min. : 1.000 Min. : -1.00 Min. : 0.0000 Length:4521 ## 1st Qu.: 1.000 1st Qu.: -1.00 1st Qu.: 0.0000 Class :character ## Median : 2.000 Median : -1.00 Median : 0.0000 Mode :character ## Mean : 2.794 Mean : 39.77 Mean : 0.5426 ## 3rd Qu.: 3.000 3rd Qu.: -1.00 3rd Qu.: 0.0000 ## Max. :50.000 Max. :871.00 Max. :25.0000 ## y ## Length:4521 ## Class :character ## Mode :character ## ## ## "],["12-2-la-función-plot.html", "12.2 La función plot()", " 12.2 La función plot() En R, la función plot() es usada de manera general para crear gráficos. Esta función tiene un comportamiento especial, pues dependiendo del tipo de dato que le demos como argumento, generará diferentes tipos de gráfica. Además, para cada tipo de gráfico, podremos ajustar diferentes parámetros que controlan su aspecto, dentro de esta misma función. Puedes imaginar a plot() como una especie de navaja Suiza multi-funcional, con una herramienta para cada ocasión. plot() siempre pide un argumento x, que corresponde al eje X de una gráfica. x requiere un vector y si no especificamos este argumento, obtendremos un error y no se creará una gráfica. El resto de los argumentos de plot() son opcionales, pero el más importante es y. Este argumento también requiere un vector y corresponde al eje Y de nuestra gráfica. Dependiendo del tipo de dato que demos a x y y será el gráfico que obtendremos, de acuerdo a las siguientes reglas: x y Gráfico Continuo Continuo Diagrama de dispersión (Scatterplot) Continuo Discreto Diagrama de dispersión, y coercionada a numérica Continuo Ninguno Diagrama de dispersión, por número de renglón Discreto Continuo Diagrama de caja (Box plot) Discreto Discreto Gráfico de mosaico (Diagrama de Kinneman) Discreto Ninguno Gráfica de barras Ninguno Cualquiera Error En donde los tipos de dato son: Continuo: Un vector numérico, entero, lógico o complejo. Discreto: Un vector de factores o cadenas de texto. Además de plot(), hay funciones que generan tipos específicos de gráfica. Por ejemplo, podemos crear una gráfica de barras con plot() pero existe también la función barplot(). También existen también casos como el de los histogramas, que sólo pueden ser creados con la función hist(). Cuando llamas a la función plot() o alguna otra similar, R abre una ventana mostrando ese gráfico. Si estás usando RStudio, el gráfico aparece en el panel Plot. Si llamas de nuevo la función plot(), el gráfico generado más reciente reemplazará al más antiguo y en RStudio se creará una nueva pestaña en en el panel Plot. El gráfico reemplazado se perderá. Por lo tanto, a menos que nosotros los indiquemos, nuestros gráficos se pierden al crear uno nuevo. Al final de este capítulo veremos cómo exportar gráficos de manera más permanente. "],["12-3-histogramas.html", "12.3 Histogramas", " 12.3 Histogramas Un histograma es una gráfica que nos permite observar la distribución de datos numéricos usando barras. Cada barra representa el número de veces (frecuencia) que se observaron datos en un rango determinado. Para crear un histograma usamos la función hist(), que siempre nos pide como argumento x un vector numérico. El resto de los argumentos de esta función son opcionales. Si damos un vector no numérico, se nos devolverá un error. Ya hemos trabajado con esta función en el capítulo 8, pero ahora profundizaremos sobre ella. Probemos creando un histograma con las edades (age) de las personas en nuestro data frame banco. Sabemos que age Daremos como argumento a hist() la columna age como un vector, extraído de banco usando el signo de dolar $, aunque también podemos usar corchetes e índices. hist(x = banco$age) Nuestro histograma luce bastante bien para habernos costado tan poco trabajo crearlo, aunque puede mejorar su presentación. Podemos agregar algunos argumentos a la función hist() para modificar ciertos parámetros gráficos. Vamos a cambiar el título del gráfico con el argumento main, y el nombre de los ejes X y Y con xlab y ylab, respectivamente. Estos argumentos requiere una cadena de texto y pueden agregados también a gráficos generados con plot(). hist(x = banco$age, main = &quot;Histograma de Edad&quot;, xlab = &quot;Edad&quot;, ylab = &quot;Frecuencia&quot;) Probemos cambiando el color de las barras del histograma agregando el argumento col. Este argumento acepta nombres de colores genéricos en inglés como red, blue o purple; y también acepta colores hexadecimales, como #00FFFF, #08001a o #1c48b5. Puedes ver una lista de los nombres de colores válidos en R en el siguiente enlace: http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf El tema de los colores hexadecimales sale del alcance de este libro, pero en el siguiente enlace encontrarás una web app para generar y elegir fácilmente colores de este tipo. https://www.w3schools.com/colors/colors_picker.asp Probemos con columnas de color púrpura (purple). hist(x = banco$age, main = &quot;Histograma de Edad&quot;, xlab = &quot;Edad&quot;, ylab = &quot;Frecuencia&quot;, col = &quot;purple&quot;) Nuestro histograma ya luce presentable. Creamos ahora un histograma con los mismos argumentos, pero con los datos de la columna duration, con barras de color marfil (ivory) y los títulos apropiados. hist(x = banco$duration, main = &quot;Histograma de Duration&quot;, xlab = &quot;Duration&quot;, ylab = &quot;Frecuencia&quot;, col = &quot;ivory&quot;) Como es usual, puedes consultar los demás argumentos de esta función llamando ?hist(). "],["12-4-gráficas-de-barras.html", "12.4 Gráficas de barras", " 12.4 Gráficas de barras Este es quizás el tipo de gráfico mejor conocido de todos. Una gráfica de este tipo nos muestra la frecuencia con la que se han observado los datos de una variable discreta, con una barra para cada categoría de esta variable. La función plot() puede generar gráficos de barra si damos como argumento x un vector de factor o cadena de texto, sin dar un argumento y. Por ejemplo, creamos una gráfica de barras de la variable educación (education) de banco #plot(x = banco$education) Al igual que con los histogramas, obtenemos un resultado aceptable no obstante el esfuerzo mínimo que hemos hecho para generar nuestra gráfica de barras. Podemos ajustar los parámetros gráficos con los argumentos main, xlab, ylab y col. En este caso, podemos darle a col un vector de colores, uno por barra, para que cada una sea distinta. #plot(x = banco$education, main = &quot;Gráfica de Educacíón&quot;, # xlab = &quot;Nivel educativo&quot;, ylab = &quot;Frecuencia&quot;, # col = c(&quot;royalblue&quot;, &quot;seagreen&quot;, &quot;purple&quot;, &quot;grey&quot;)) La combinación de colores puede mejorar, pero ya tenemos una gráfica de barras presentable. Sin embargo, hay ocasiones en las que deseamos usar gráficas de barras para presentar proporciones, que deseamos barras apiladas. Para esos casos, usamos la función barplot(). 12.4.1 La función barplot() Además de usar plot(), podemos crear gráficas de barra con la función barplot(). barplot pide como argumento una matriz, que represente una tabla de contingencia con los datos a graficar. Este tipo de tablas pueden ser generadas con la función table(). table() pide como argumento uno o más vectores, de preferencia variables discretas. Si damos sólo un vector como argumento, devuelve un conteo, si damos dos o más variables, devuelve tablas de contingencia. Por ejemplo, el conteo de la variable education, table(banco$education) ## ## primary secondary tertiary unknown ## 678 2306 1350 187 Si damos como argumentos la variable education y la variable loan (préstamo), obtenemos una tabla de contingencia, que asignaremos al objeto tab_banco. tab_banco &lt;- table(banco$loan, banco$education) # Resultado tab_banco ## ## primary secondary tertiary unknown ## no 584 1890 1176 180 ## yes 94 416 174 7 Damos como argumento tab_banco a barplot() y nos devuelve una gráfica de barras apiladas. barplot(tab_banco) Si deseamos graficar proporciones en lugar de conteos, usamos la función prop.table(). Esta función nos pide como argumento una tabla de contingencia generada por table(). y un número para margin. El argumento margin es similar a MARGIN de apply() (como vimos en el capítulo 10). Si damos como argumento 1, las proporciones se calcularán agrupadas por renglón. La suma de proporciones por renglón será igual a 1. Si damos como argumento 2, las proporciones se calcularán agrupadas por columna. La suma de proporciones por columna será igual a 1 Si no damos ningún argumento, las proporciones se calcularán usando toda la tabla como grupo. La suma de proporciones de todas las celdas en la tabla será igual a 1. Para ilustrar esto, veamos los tres casos para margin usando como argumento nuestro objeto tab_banco. # Proporción por renglón prop.table(tab_banco, margin = 1) ## ## primary secondary tertiary unknown ## no 0.15248042 0.49347258 0.30704961 0.04699739 ## yes 0.13603473 0.60202605 0.25180897 0.01013025 # Porporción por columna prop.table(tab_banco, margin = 2) ## ## primary secondary tertiary unknown ## no 0.86135693 0.81960104 0.87111111 0.96256684 ## yes 0.13864307 0.18039896 0.12888889 0.03743316 # Porporción por tabla prop.table(tab_banco) ## ## primary secondary tertiary unknown ## no 0.12917496 0.41804910 0.26011944 0.03981420 ## yes 0.02079186 0.09201504 0.03848706 0.00154833 Nosotros queremos obtener las proporciones por columna, así que usaremos margin = 2. ptab_banco &lt;- prop.table(tab_banco, margin = 2) Damos el resultado de la operación anterior a barplot(). barplot(ptab_banco) Hemos obtenido el resultado esperado, pero podemos mejorar la presentación. Nota que con barras apiladas el argumento col se puede usar para colorear las categorías al interior de las barras. barplot(ptab_banco, main = &quot;Préstamos por nivel educativo&quot;, xlab = &quot;Nivel educativo&quot;, ylab = &quot;Proporción&quot;, col = c(&quot;royalblue&quot;, &quot;grey&quot;)) Luce bien, pero tenemos un problema: no sabemos qué representan las categorías en nuestras barras apiladas viendo solamente nuestra gráfica. Nosotros podemos consultar directamente con los datos, pero una persona que vea por primera vez esta gráfica no tendrá esa opción, reduciendo con ello su utilidad. Para solucionar este problema, usamos leyendas. "],["12-5-leyendas.html", "12.5 Leyendas", " 12.5 Leyendas Las leyendas son usadas para identificar con mayor claridad los distintos elementos en un gráfico, tales como colores y formas. En R usamos la función legend() para generar leyendas. Esta función debe ser llamada después de crear un gráfico. En cierto modo es una anotación a un gráfico ya existente. legend() es una función relativamente compleja, así que sólo revisaremos lo esencial. legend() siempre nos pide siempre los siguientes argumentos. legend: Las etiquetas de los datos que queremos describir con la leyenda. Por ejemplo, si tenemos cuatro categorías a describir, proporcionamos un vector de cuatro cadenas de texto. fill: Los colores que acompañan a las etiquetas definidas con legend. Estos colores tienen que coincidir con los que hemos usado en el gráfico. x y y: Las coordenadas en pixeles, en las que estará ubicada la leyenda. Podemos dar como argumento a x alguno de los siguientes, para ubicar automáticamente la leyenda: bottomright, bottom, bottomleft, left, topleft, top, topright, right, center. title: Para poner título a la leyenda. Además, tenemos muchos otros argumentos opcionales, que puedes consultar en la documentación llamando ?legend(). Vamos a agregar una leyenda a la última gráfica de barras que creamos en la sección anterior de este capítulo. Entonces necesitamos conocer las etiquetas que daremos como argumento legend y a qué colores corresponden al vector banco$loan. Usamos la función unique para determinar cuántos valores únicos hay en este vector. Cada uno de estos valores corresponde a una etiqueta. Esta función, si la aplicamos a un vector de tipo factor, nos devuelve sus niveles. unique(banco$loan) ## [1] &quot;no&quot; &quot;yes&quot; Tenemos dos etiquetas, no y yes (no y sí, respectivamente), en ese orden, por lo que ese será nuestro argumento legend. Nosotros determinamos los colores en la sección anterior como royalblue y grey, en ese orden. Por lo tanto, tendremos que no será coloreado con royalblue, y yes con grey. como vamos a rellenar una barra, esto colores los daremos al argumento fill. Por último, daremos como topright como argumento x para que nuestra leyenda se unique en la parte superior derecha de nuestro gráfico. Aplicamos todo, incluido generar el gráfico al que agregaremos la leyenda. barplot(ptab_banco, main = &quot;Préstamos por nivel educativo&quot;, xlab = &quot;Nivel educativo&quot;, ylab = &quot;Proporción&quot;, col = c(&quot;royalblue&quot;, &quot;grey&quot;)) legend(x = &quot;topright&quot;, legend = c(&quot;No&quot;, &quot;Yes&quot;), fill = c(&quot;royalblue&quot;, &quot;grey&quot;), title = &quot;Loan&quot;) Se ve mucho más clara la información, pues ahora estamos mostrando a qué categoría corresponden los colores que hemos empleado en el gráfico. En las secciones siguientes agregaremos leyendas a otros gráficos, con lo cual quedará un poco más claro el uso de legend(). "],["12-6-diagramas-de-dispersión.html", "12.6 Diagramas de dispersión", " 12.6 Diagramas de dispersión Este tipo de gráfico es usado para mostrar la relación entre dos variables numéricas continuas, usando puntos. Cada punto representa la intersección entre los valores de ambas variables. Para generar un diagrama de dispersión, damos vectores numéricos como argumentos x y y a la función plot(). Veamos la relación entre las variables age y balance de banco. plot(x = banco$age, y = banco$balance) Tenemos algunos datos extremos tanto en balance. Para fines de tener una gráfica más informativa, vamos a recodificarlos usando ifelse(), cambiando todos los valores mayores a 15 000. banco$balance &lt;- ifelse(banco$balance &gt; 15000, 15000, banco$balance) plot(x = banco$age, y = banco$balance) En los diagramas de dispersión, podemos usar el argumento col para camiar el color de los puntos usando como referencia una tercera variable. La variable que usaremos será, de nuevo, loan # plot(x = banco$age, y = banco$balance, col= banco$loan) Nos sería de utilidad una leyenda para interpretar más fácilmente los colores. Ya sabemos que los niveles de loan son no y yes, además de que los colores han sido rojo y negro, así que agregar una leyenda será relativamente fácil. #plot(x = banco$age, y = banco$balance, col= banco$loan) #legend(x = &quot;topleft&quot;, legend = c(&quot;No&quot;, &quot;Yes&quot;), fill = c(&quot;Black&quot;, &quot;Red&quot;), title = &quot;Loan&quot;) Desafortunadamente esta gráfica no es muy informativa para nuestros datos. Por fortuna, podemos probar con un conjunto de datos diferente. Si usamos diagramas de dispersión con iris obtendremos gráficos mucho más interesantes. Creamos un gráfico con las medidas de pétalo, aplicando lo que hemos visto para generar diagramas de dispersión. plot(x = iris$Petal.Length, y = iris$Petal.Width, col = iris$Species, main = &quot;Iris - Pétalo&quot;, xlab = &quot;Largo&quot;, ylab = &quot;Ancho&quot;) legend(x = &quot;topleft&quot;, legend = c(&quot;Setosa&quot;, &quot;Versicolor&quot;, &quot;Virginica&quot;), fill = c(&quot;black&quot;, &quot;red&quot;, &quot;green&quot;), title = &quot;Especie&quot;) "],["12-7-diagramas-de-caja.html", "12.7 Diagramas de caja", " 12.7 Diagramas de caja Los diagrama de caja, también conocidos como de caja y bigotes son gráficos que muestra la distribución de una variable usando cuartiles, de modo que de manera visual podemos inferir algunas cosas sobre su dispersión, ubicación y simetría. Una gráfica de este tipo dibuja un rectángulo cruzado por una línea recta horizontal. Esta linea recta representa la mediana, el segundo cuartil, su base representa el primer cuartil y su parte superior el tercer cuartil. Al rango entre el primer y tercer cuartil se le conoce como intercuartílico (RIC). Esta es la caja. Además, de la caja salen dos líneas. Una que llega hasta el mínimo valor de los datos en la variable o hasta el primer cuartil menos hasta 1.5 veces el RIC; y otra que llegar hasta el valor máximo de los datos o el tercer cuartil más hasta 1.5 veces el RIC. Estos son los bigotes. Usamos la función plot() para crear este tipo de gráfico, dando como argumento x un vector de factor o cadena de texto, y como argumento y un vector numérico. Una ventaja de este tipo de gráfico es que podemos comparar las distribución de una misma variable para diferentes grupos. Vamos a ver cómo se distribuye la edad por nivel de educación en nuestro objeto banco, esto es, las variables education y age. #plot(x = banco$education, y = banco$age) Podemos ver que las personas con menor nivel educativo tienden a tener una edad mayor. La mayoría de las personas con educación primaria tienen entre 40 y 50 años, mientras que la mayoría con educación terciaria tiene entre 35 y 45 años, aproximadamente. Por supuesto, podemos cambiar los parámetros gráficos a un diagrama de caja. #plot(x = banco$education, y = banco$age, main = &quot;Edad por nivel educativo&quot;, # xlab = &quot;Nivel educativo&quot;, ylab = &quot;Edad&quot;, # col = c(&quot;orange3&quot;, &quot;yellow3&quot;, &quot;green3&quot;, &quot;grey&quot;)) También podemos crear diagramas de caja con la función boxplot(). Esta función puede generar diagramas de caja de dos maneras distintas. En la primera manera, si damos como argumento x un vector numérico, nos dará un diagrama de caja de esa variable. boxplot(x = banco$age) En la segunda manera necesitamos dar dos argumentos: formula: Para esta función las fórmulas tienen el formato y ~ x, donde x es el nombre de la variable continua a graficar, y la x es la variable que usaremos como agrupación. data: Es el data frame del que serán tomadas las variables. Por ejemplo, para mostrar diagramas de caja por nivel educativo, nuestra variable y es age y nuestra variable x es education, por lo tanto, formula será age ~ education. boxplot(formula = age ~ education, data = banco) "],["12-8-gráficos-de-mosaico.html", "12.8 Gráficos de mosaico", " 12.8 Gráficos de mosaico Los gráficos de mosaico o diagramas de Marimekko son usados para mostrar la relación entre dos variables discretas, ya sean factores o cadenas de texto. Este tipo de gráfico recibe su nombre porque consiste en una cuadricula, en la que cada rectángulo representa el numero de casos que corresponden a un cruce específico de variables. Entre más casos se encuentren en ese cruce, más grande será el rectángulo. Para obtener un gráfico de mosaico, damos como vectores de factor o cadena de texto como argumentos x y y a la función plot(). Por ejemplo, intentemos graficar el estado marital con el nivel educativo de las personas en banco #plot(x = banco$marital, y = banco$education) Podemos cambiar el color de los mosaicos con el argumento col. Debemos proporcionar un color por cada nivel del vector en el eje Y. #plot(x = banco$marital, y = banco$education, # col = c(&quot;#99cc99&quot;, &quot;#cc9999&quot;, &quot;#9999cc&quot;, &quot;#9c9c9c&quot;)) De esta manera es más claro que el grupo más numeroso de personas son las casadas con educación secundaria y el más pequeño, divorciadas con educación primaria. "],["12-9-exportar-gráficos.html", "12.9 Exportar gráficos", " 12.9 Exportar gráficos Exportar los gráficos que hemos creado es un proceso que puede parecer un poco confuso. Cuando llamamos una de estas funciones, le estamos indicando a R que mande nuestro gráfico a un dispositivo gráfico (graphic device) en nuestra computadora, donde podemos verlo, que por defecto es una ventana en nuestro escritorio o el panel Plot si estás usando RStudio. Una consecuencia de esto es que si creas y lo mandas a un dispositivo gráfico en uso, el gráfico nuevo reemplazará al anterior. Por ejemplo, si usas plot() para crear un gráfico, se mostrará en una ventana de tu escritorio, pero si usas plot() de generar un gráfico distinto, el contenido de esta ventana será reemplazada con este nuevo gráfico. Lo mismo pasa con todos los dispositivos gráficos. Además, los gráficos no pueden ser guardados en un objetos para después ser exportados. Es necesario mandar nuestros gráficos a un dispositivo como JPG, PNG o algún otro tipo de archivo que pueda ser almacenado en nuestro disco duro. Para exportar un gráfico usamos alguna de las siguientes funciones, cada una corresponde con un tipo de archivo distinto. No son las únicas, pero son las más usadas. bpm() jpeg() pdf() png() tiff() Cada una de estas funciones tiene los siguientes argumentos tres argumentos principales. filename: El nombre y ruta del archivo de imagen a crear. Si no especificamos una ruta completa, entonces el el archivo será creado en nuestro directorio de trabajo. width: El ancho del archivo de imagen a crear, por defecto en pixeles. height: El alto del archivo de imagen a crear, por defecto en pixeles. La manera de utilizar estas funciones llamándolas antes de llamar a una función que genere una gráfica. Al hacer esto, le indicamos a R que en lugar de mandar nuestro gráfico a una ventana del escritorio, lo mande a un dispositivo gráfico distinto. Finalmente, llamamos a la función dev.off(), para cerrar el dispositivo gráfico que hemos elegido, de este modo se creará un archivo y podremos crear más gráficos después. Por ejemplo, para exportar un gráfico con leyenda como un archivo PNG llamamos lo siguiente. Nota que tenemos que dar la misma extensión de archivo que la función que estamos llamando, en este caso .png. #png(filename = &quot;loan_age.png&quot;, width = 800, height = 600) #plot(x = banco$age, y = banco$duration, col = banco$loan, # main = &quot;Edad y Duración&quot;, xlab = &quot;Edad&quot;, ylab = &quot;Duración&quot;) #legend(x = &quot;top&quot;, legend = c(&quot;No&quot;, &quot;Yes&quot;), fill = c(&quot;Black&quot;, &quot;Red&quot;), # title = &quot;Loan&quot;) #dev.off() Si aparece un mensaje como el siguiente, es que hemos tenido éxito. null device 1 Podemos ver el resultado usando file.show(). file.show(&quot;loan_age.png&quot;) De esta manera podemos exportar cualquier tipo de gráfico generado con R. "],["13-conclusión.html", "Capítulo 13 Conclusión", " Capítulo 13 Conclusión Al haber concluido este libro tendrás las herramientas básicas para utilizar R como un lenguaje de programación. Desde los conceptos más básicos, hasta la definición de funciones para exportar gráficos. Lo que hemos visto en este libro te será de utilidad sin importar el uso de R que tengas previsto, pues son conceptos fundamentales que te permitirán acceder a otros más complejos y avanzado. El siguiente paso es aplicar lo que hemos revisado hasta este momento, ya sea en algún proyecto propio o para aprender más sobre R y sus aplicaciones. En caso de que desees profundizar más sobre el uso de R, la siguiente sección te presenta una selección de materiales de referencia que te facilitarán continuar con tu aprendizaje, dependiendo de tus intereses personales. Todas las referencias incluyen un enlace para que las consultes en línea, sin costo y legalmente. ¡Suerte! Material de referencia Probabilidad y estadística (introductorio a medio) Diez, D., Barr, C., Çetinkaya-Rundel, M. (2015). OpenIntro Statistics, segunda edición. OpenIntro. https://www.openintro.org/stat/textbook.php?stat_book=os Navarro, D. (2015). Learning statistics with R: A tutorial for psychology students and other beginners. University of Adelaide. http://www.fon.hum.uva.nl/paul/lot2015/Navarro2014.pdf R aplicado a Data Science (medio a avanzado) Peng, R. D. (2016). R Programming for Data Science. Leanpub. https://leanpub.com/rprogramming Wickham. H. y Grolemund, G. (2017). R for Data Science. OReilly. http://r4ds.had.co.nz/ Programación con R (avanzado) Wickham, H. (2014). Tidy Data. Journal of Statistical Software. https://www.jstatsoft.org/article/view/v059i10/v59i10.pdf Wickham, H. (2014). Advanced R. OReilly. http://adv-r.had.co.nz/ Referencias sobre RStudio RStudio Cheat Sheets. https://www.rstudio.com/resources/cheatsheets/ "],["14-introducción.html", "Capítulo 14 # Introducción", " Capítulo 14 # Introducción Este es un libro introductorio al análisis de datos con R. Este libro ha sido escrito en R-Markdown empleando el paquete bookdown y está disponible en el repositorio Github: rubenfcasal/intror. Se puede acceder a la versión en línea a través del siguiente enlace: https://rubenfcasal.github.io/intror. donde puede descargarse en formato pdf. Para ejecutar los ejemplos mostrados en el libro será necesario tener instalados los siguientes paquetes: lattice, ggplot2, foreign, car, leaps, MASS, RcmdrMisc, lmtest, glmnet, mgcv, rmarkdown, knitr, dplyr. Por ejemplo mediante el comando: pkgs &lt;- c(&quot;lattice&quot;, &quot;ggplot2&quot;, &quot;foreign&quot;, &quot;car&quot;, &quot;leaps&quot;, &quot;MASS&quot;, &quot;RcmdrMisc&quot;, &quot;lmtest&quot;, &quot;glmnet&quot;, &quot;mgcv&quot;, &quot;rmarkdown&quot;, &quot;knitr&quot;, &quot;dplyr&quot;) install.packages(setdiff(pkgs, installed.packages()[,&quot;Package&quot;]), dependencies = TRUE) # Si aparecen errores debidos a incompatibilidades entre las versiones de los paquetes, # probar a ejecutar en lugar de lo anterior: # install.packages(pkgs, dependencies=TRUE) # Instala todos... Para generar el libro (compilar) serán necesarios paquetes adicionales, para lo que se recomendaría consultar el libro de Escritura de libros con bookdown en castellano. Este obra está bajo una licencia de Creative Commons Reconocimiento-NoComercial-SinObraDerivada 4.0 Internacional (esperamos poder liberarlo bajo una licencia menos restrictiva más adelante). El entorno estadístico R puede ser una herramienta de gran utilidad a lo largo de todo el proceso de obtención de información a partir de datos (normalmente con el objetivo final de ayudar a tomar decisiones). Figura 14.1: Etapas del proceso "],["14-1-el-lenguaje-y-entorno-estadístico-r.html", "14.1 El lenguaje y entorno estadístico R", " 14.1 El lenguaje y entorno estadístico R R es un lenguaje de programación desarrollado específicamente para el análisis estadístico y la visualización de datos. El lenguaje R es interpretado (similar a Matlab o Phyton) pero orientado al análisis estadístico (fórmulas modelos, factores,). derivado del S (Laboratorios Bell). R es un Software Libre bajo las condiciones de licencia GPL de GNU, con código fuente de libre acceso. Además de permitir crear nuevas funciones, se pueden examinar y modificar las ya existentes. Multiplataforma, disponible para los sistemas operativos más populares (Linux, Windows, MacOS X, ). 14.1.1 Principales características Se pueden destacar las siguientes características del entorno R: Dispone de numerosos complementos (librerías, paquetes) que cubren literalmente todos los campos del análisis de datos. Repositorios: CRAN (9705, 14972, ) Bioconductor (1289, 1741, ), GitHub,  Existe una comunidad de usuarios (programadores) muy dinámica (multitud de paquetes adicionales). Muy bien documentado y con numerosos foros de ayuda. Puntos débiles (a priori): velocidad, memoria,  Aunque inicialmente fue un lenguaje desarrollado por estadísticos para estadísticos: Figura 14.2: Rexer Data Miner Survey 2007-2015 Hoy en día es muy popular: Figura 14.3: IEEE Spectrum Top Programming Languages, 2019 R destaca especialmente en: Representaciones gráficas. Métodos estadísticos avanzados: Data Science: Statistical Learning, Data Mining, Machine Learning, Business Intelligence,  Datos funcionales. Estadística espacial.  Análisis de datos complejos: Big Data. Lenguaje natural (Text Mining). Análisis de redes.  14.1.2 Interfaces gráficas El programa R utiliza una interfaz de comandos donde se teclean las instrucciones que se pretenden ejecutar (ver Figura 14.4). Por ejemplo, para obtener una secuencia de números desde el 1 hasta el 10, se utilizará la sentencia: 1:10 obteniéndose el resultado ## [1] 1 2 3 4 5 6 7 8 9 10 En el Apéndice B se detallan los pasos para la instalación de R, y en el Apéndice C los de otras interfaces gráficas. "],["14-2-entorno-de-trabajo.html", "14.2 Entorno de trabajo", " 14.2 Entorno de trabajo 14.2.1 Ventana de Consola Al abrir el programa R, tal y como ya se ha visto, aparece la siguiente ventana de consola para trabajar de modo interactivo en modo comando (ver Figura 14.4). Figura 14.4: Consola de R en Windows (modo MDI). En la ventana de consola cada línea en que el usuario puede introducir información se inicia con el carácter &gt; que pone el sistema R. Para ejecutar las instrucciones que están en una línea, se pulsa la tecla de Retorno (y por defecto se imprime el resultado). 2+2 ## [1] 4 1+2*4 ## [1] 9 Se pueden escribir varias instrucción en una misma línea separándolas por ; 2+2;1+2*4 ## [1] 4 ## [1] 9 Se pueden recuperar líneas de instrucciones introducidas anteriormente pulsando la tecla con la flecha ascendente del teclado, a fin de re-ejecutarlas o modificarlas. 14.2.2 Ventana de Script La ventana consola ejecuta de forma automática cada línea de comando. Sin embargo, suele interesar guardar un conjunto de instrucciones en un único archivo de texto para formar lo que se conoce como un script. Las instrucciones del script se copian y pegan en la ventana de comandos para ser ejecutadas. Para crear un fichero de script se selecciona el submenú Nuevo script dentro del menú Archivo. Otra posibilidad es utilizar directamente la combinación de teclas Ctrl+N. Figura 14.5: Ventanas de la consola y de script en Windows (modo MDI). Para guardar este tipo de fichero se utiliza directamente el menú Archivo &gt; Guardar como y se elige a continuación la ubicación en el disco duro del ordenador. De igual modo para abrir un script existente se hace a través del menú Archivo &gt; Abrir script. En el Apéndice A se incluyen enlaces a numerosos recursos para el aprendizaje de R, incluyendo manuales y libros, además de otros recursos para la obtención de ayuda. 14.2.3 Ayuda dentro del programa Como ya se ha comentado con anterioridad, hay manuales de R alos que se puede acceder a través del menú Ayuda &gt; Manuales (en PDF) Puede empezarse utilizando help.start() o demo(). Todas las funciones de R tienen su documentación integrada en el programa. Para obtener la ayuda de una determinada función se utilizará help (función) o de forma equivalente ?función. Por ejemplo, la ayuda de la función rnorm (utilizada para la generación de datos normales) se obtiene con el código help(rnorm) ?rnorm En muchas ocasiones no se conoce el nombre exacto de la función de la que queremos obtener la documentación. En estos casos, la función help.search() realiza búsquedas en la documentación en todos los paquetes instalados, estén cargados o no. Por ejemplo, si no conocemos la función que permite calcular la mediana de un conjunto de datos, se puede utilizar help.search(&quot;median&quot;) Para más detalles véase ?help.search "],["14-3-librerías.html", "14.3 Librerías", " 14.3 Librerías 14.3.1 Paquetes Al iniciar el programa R se cargan por defecto una serie de librerías básicas con las que se pueden realizar una gran cantidad de operaciones básicas. Estas librerías conforman el llamado paquete base. En otras ocasiones es necesario cargar otras librerías distintas a las anteriores. Esto se hace a través de los llamados paquetes (packages) que pueden ser descargados directamente de la web http://cran.r-project.org/web/packages/ o directamente a través del menú Paquetes. 14.3.2 Instalación La instalación de un paquete, bajo el sistema operativo Windows, se puede hacer de varias formas: Desde el menú Paquetes &gt; Instalar paquete(s) Desde la ventana de consola utilizando la instrucción install.packages(&quot;nombre del paquete&quot;) Este proceso sólo es necesario realizarlo la primera vez que se utilice el paquete. 14.3.3 Carga Para utilizar un paquete ya instalado será necesario cargarlo, lo cual se puede hacer de varias formas: Desde el menú Paquetes &gt; Cargar paquete(s)... Por consola, utilizando library(nombre del paquete) Esta operación será necesario realizarla cada vez que se inicie una sesión de R. Finalmente, la ayuda de un paquete se puede obtener con la sentencia library(help = &quot;nombre del paquete&quot;) "],["14-4-una-primera-sesión.html", "14.4 Una primera sesión", " 14.4 Una primera sesión 14.4.1 Inicio de una sesión El programa R (bajo Windows) se arranca al hacer un doble-click sobre el icono del programa. Entonces aparecerá la ventana de consola donde se escribirán los comandos y los resultados serán mostrados inmediatamente por pantalla. Veamos algún ejemplo: 3+5 ## [1] 8 sqrt(16) # raiz cuadrada de 16 ## [1] 4 pi # R reconoce el número pi ## [1] 3.141593 Nótese que en los comandos se pueden hacer comentarios utilizando el símbolo #. Los resultados obtenidos pueden guardarse en objetos. Por ejemplo, al escribir a &lt;- 3 + 5 el resultado de la suma se guarda en a. Se puede comprobar si la asignación se ha realizado correctamente escribiendo el nombre del objeto a ## [1] 8 La asignación anterior se puede hacer del siguiente modo ejemplo, al escribir a &lt;- 3 + 5; a ## [1] 8 Nota: Habitualmente no habrá diferencia entre la utilización de las asignaciones hechas con = y &lt;- (aunque nosotros emplearemos el segundo). Las diferencias aparecen a nivel de programación y se tratarán en el Capítulo 24. Veamos ahora un ejemplo un poco más avanzado. Con el siguiente código Se obtienen 200 datos simulados siguiendo una distribución gaussiana de media 105 y desviación típica 2 Se hace un resumen estadístico de los valores obtenidos Se hace el correspondiente histograma y gráfico de cajas x &lt;- rnorm(n = 200, mean = 105, sd = 2) #datos normales summary(x) # resumen estadístico ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 99.15 103.63 104.93 104.94 106.37 111.34 hist(x) # histograma boxplot(x) # gráfico de cajas Es importante señalar que R diferencia entre mayúsculas y minúsculas, de modo que los objetos a y A serán diferentes. a &lt;- 1:10 # secuencia de números A &lt;- &quot;casa&quot; a ## [1] 1 2 3 4 5 6 7 8 9 10 A ## [1] &quot;casa&quot; "],["14-5-objetos-básicos.html", "14.5 Objetos básicos", " 14.5 Objetos básicos R es un lenguaje orientado a objetos lo que significa que las variables, datos, funciones, resultados, etc., se guardan en la memoria del ordenador en forma de objetos con un nombre específico. Los principales tipos de valores básicos de R son: numéricos, cadenas de caracteres, y lógicos 14.5.1 Objetos numéricos Los valores numéricos adoptan la notación habitual en informática: punto decimal, notacion científica,  pi ## [1] 3.141593 1e3 ## [1] 1000 Con este tipo de objetos se pueden hacer operaciones aritméticas utilizando el operador correspondiente. a &lt;- 3.4 b &lt;- 4.5 a * b ## [1] 15.3 a / b ## [1] 0.7555556 a + b ## [1] 7.9 min(a, b) ## [1] 3.4 14.5.2 Objetos tipo carácter Las cadenas de caracteres se introducen delimitadas por comillas (nombre) o por apóstrofos (nombre). a &lt;- &quot;casa grande&quot; a ## [1] &quot;casa grande&quot; a &lt;- &#39;casa grande&#39; a ## [1] &quot;casa grande&quot; a &lt;- &#39;casa &quot;grande&quot;&#39; a ## [1] &quot;casa \\&quot;grande\\&quot;&quot; 14.5.3 Objetos lógicos Los objetos lógicos sólo pueden tomar dos valores TRUE (numéricamente toma el valor 1) y FALSE (valor 0). A &lt;- TRUE B &lt;- FALSE A ## [1] TRUE B ## [1] FALSE # valores numéricos as.numeric(A) ## [1] 1 as.numeric(B) ## [1] 0 14.5.4 Operadores lógicos Existen varios operadores en R que devuelven un valor de tipo lógico. Veamos algún ejemplo a &lt;- 2 b &lt;- 3 a == b # compara a y b ## [1] FALSE a == a # compara a y a ## [1] TRUE a &lt; b ## [1] TRUE b &lt; a ## [1] FALSE ! (b &lt; a) # ! niega la condición ## [1] TRUE 2**2 == 2^2 ## [1] TRUE 3*2 == 3^2 ## [1] FALSE Nótese la diferencia entre = (asignación) y == (operador lógico) 2 == 3 ## [1] FALSE # 2 = 3 # produce un error: # Error en 2 = 3 : lado izquierdo de la asignación inválida (do_set) Se pueden encadenar varias condiciones lógicas utiilizando los operadores &amp; (y lógico) y | (o lógico). TRUE &amp; TRUE ## [1] TRUE TRUE | TRUE ## [1] TRUE TRUE &amp; FALSE ## [1] FALSE TRUE | FALSE ## [1] TRUE 2 &lt; 3 &amp; 3 &lt; 1 ## [1] FALSE 2 &lt; 3 | 3 &lt; 1 ## [1] TRUE "],["14-6-área-de-trabajo.html", "14.6 Área de trabajo", " 14.6 Área de trabajo Como ya se ha comentado con anterioridad es posible guardar los comandos que se han utilizado en una sesión en ficheros llamados script. En ocasiones interesará además guardar todos los objetos que han sido generados a lo largo de una sesión de trabajo. El Workspace o Área de Trabajo es el entorno en el que se puede guardar todo el trabajo realizado en una sesión. De este modo, la próxima vez que se inicie el programa, al cargar dicho entorno, se podrá acceder a lo objetos almacenados en él. En primer lugar, para saber los objetos que tenemos en memoria se utiliza la función ls. Por ejemplo, supongamos que acabamos de iniciar una sesión de R y hemos escrito a &lt;- 1:10 b &lt;- log(50) Entonces al utilizar ls se obtendrá la siguiente lista de objetos en memoria ls() ## [1] &quot;a&quot; &quot;b&quot; También es posible borrar objetos a través de la función rm rm(b) ls() ## [1] &quot;a&quot; Para borrar todos los objetos en memoria se puede utilizar rm(list=ls()) rm(list = ls()) ## character(0) character(0) (lista vacía) significa que no hay objetos en memoria. 14.6.1 Guardar y cargar resultados Para guardar el área de trabajo (Workspace) con todos los objetos de memoria (es decir, los que figuran al utilizar ls()) se utiliza la función save.image(nombre archivo). rm(list = ls()) # primero borramos toda la mamoria x &lt;- 20 y &lt;- 34 z &lt;- &quot;casa&quot; save.image(file = &quot;prueba.RData&quot;) # guarda area de trabajo en prueba.RData La función save permite guardar los objetos seleccionados. save(x, y, file = &quot;prueba2.RData&quot;) # guarda los objetos x e y Para cargar una ára de trabajo ya exitente se utiliza la función load(). load(&quot;prueba2.RData&quot;) # carga área de trabajo 14.6.2 Directorio de trabajo Por defecto R utiliza una carpeta de trabajo donde guardará toda la información. Dicha carpeta se puede obtener con la función getwd() ## [1] &quot;d:/&quot; El directorio de trabajo se puede cambiar utilizando setwd(carpeta). Por ejemplo, para cambiar el directorio de trabajo a c:\\datos, se utiliza el comando setwd(&quot;c:/datos&quot;) # Importante la barra utilizada # NO funciona setwd(&quot;c:\\datos&quot;) "],["15-estructuras-de-datos-1.html", "Capítulo 15 Estructuras de datos", " Capítulo 15 Estructuras de datos En los ejemplos que hemos visto hasta ahora los objetos de R almacenaban un único valor cada uno. Sin embargo, las estructuras de datos que proporciona R permiten almacenar en un mismo objeto varios valores. Las principales estructuras son: Vectores Matrices y Arrays Data Frames Listas "],["15-1-vectores-2.html", "15.1 Vectores", " 15.1 Vectores Un vector es un conjunto de valores básicos del mismo tipo. La forma más sencilla de crear vectores es a través de la función c() que se usa para combinar (concatenar) valores. x &lt;- c(3, 5, 7) x ## [1] 3 5 7 y &lt;- c(8, 9) y ## [1] 8 9 c(x, y) ## [1] 3 5 7 8 9 z &lt;- c(&quot;Hola&quot;, &quot;Adios&quot;) z ## [1] &quot;Hola&quot; &quot;Adios&quot; 15.1.1 Generación de secuencias Existen varias funciones que pemiten obtener secuencias de números x &lt;- 1:5 x ## [1] 1 2 3 4 5 seq(1, 5, 0.5) ## [1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 seq(from=1, to=5, length=9) ## [1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 rep(1, 5) ## [1] 1 1 1 1 1 15.1.2 Generación secuencias aleatorias A continuación se obtiene una simulación de 10 lanzamientos de un dado sample(1:6, size=10, replace = T) #lanzamiento de un dado ## [1] 2 6 2 5 4 2 3 4 6 1 Para simular el lanzamiento de una moneda podemos escribir resultado &lt;- c(cara=1,cruz=0) # se le han asignado nombres al objeto print(resultado) ## cara cruz ## 1 0 class(resultado) ## [1] &quot;numeric&quot; attributes(resultado) ## $names ## [1] &quot;cara&quot; &quot;cruz&quot; names(resultado) ## [1] &quot;cara&quot; &quot;cruz&quot; lanz &lt;- sample(resultado, size=10, replace = T) lanz ## cruz cruz cara cara cruz cara cara cruz cara cruz ## 0 0 1 1 0 1 1 0 1 0 table(lanz) ## lanz ## 0 1 ## 5 5 Otros ejemplos rnorm(10) # rnorm(10, mean = 0, sd = 1) ## [1] 1.00843853 1.93329133 -2.28990896 1.50803374 -0.16744042 0.50024132 ## [7] -2.04885063 -0.63305352 -0.07330854 -0.82789775 runif(15, min = 2, max = 10) ## [1] 8.146354 5.303966 3.590824 2.732927 8.881598 2.513958 6.646036 7.696940 ## [9] 6.386299 3.814317 2.481628 4.821822 5.274310 3.556691 9.339315 El lector puede utilizar la función help() para obtener la ayuda de las funciones anteriores. 15.1.3 Selección de elementos de un vector Para acceder a los elementos de un vector se indica entre corchetes el correspondiente vector de subíndices (enteros positivos). x &lt;- seq(-3, 3, 1) x ## [1] -3 -2 -1 0 1 2 3 x[1] # primer elemento ## [1] -3 ii &lt;- c(1, 5, 7) x[ii] #posiciones 1, 5 y 7 ## [1] -3 1 3 ii &lt;- x&gt;0; ii ## [1] FALSE FALSE FALSE FALSE TRUE TRUE TRUE x[ii] # valores positivos ## [1] 1 2 3 ii &lt;- 1:3 x[-ii] # elementos de x salvo los 3 primeros ## [1] 0 1 2 3 15.1.4 Ordenación de vectores x &lt;- c(65, 18, 59, 18, 6, 94, 26) sort(x) ## [1] 6 18 18 26 59 65 94 sort(x, decreasing = T) ## [1] 94 65 59 26 18 18 6 Otra posibilidad es utilizar un índice de ordenación. ii &lt;- order(x) ii # índice de ordenación ## [1] 5 2 4 7 3 1 6 x[ii] # valores ordenados ## [1] 6 18 18 26 59 65 94 La función rev() devuelve los valores del vector en orden inverso. rev(x) ## [1] 26 94 6 18 59 18 65 15.1.5 Valores perdidos Los valore perdidos aparecen normalmente cuando algún dato no ha sido registrado. Este tipo de valores se registran como NA (abreviatura de Not Available). Por ejemplo, supongamos que tenemos registrado las alturas de 5 personas pero desconocemos la altura de la cuarta persona. El vector sería registrado como sigue: altura &lt;- c(165, 178, 184, NA, 175) altura ## [1] 165 178 184 NA 175 Es importante notar que cualquier operación aritmética sobre un vector que contiene algún NA dará como resultado otro NA. mean(altura) ## [1] NA Para forzar a R a que ignore los valores perdidos se utliza la opción na.rm = TRUE. mean(altura, na.rm = TRUE) ## [1] 175.5 R permite gestionar otros tipos de valores especiales: NaN (Not a Number): es resultado de una indeterminación. Inf: R representa valores no finitos \\(\\pm \\infty\\) como Inf y -Inf. 5/0 # Infinito ## [1] Inf log(0) # -Infinito ## [1] -Inf 0/0 # Not a Number ## [1] NaN 15.1.6 Vectores no numéricos Los vectores pueden ser no numéricos, aunque todas las componentes deben ser del mismo tipo: a &lt;- c(&quot;A Coruña&quot;, &quot;Lugo&quot;, &quot;Ourense&quot;, &quot;Pontevedra&quot;) a ## [1] &quot;A Coruña&quot; &quot;Lugo&quot; &quot;Ourense&quot; &quot;Pontevedra&quot; letters[1:10] # primeras 10 letas del abecedario ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; LETTERS[1:10] # lo mismo en mayúscula ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;F&quot; &quot;G&quot; &quot;H&quot; &quot;I&quot; &quot;J&quot; month.name[1:6] # primeros 6 meses del año en inglés ## [1] &quot;January&quot; &quot;February&quot; &quot;March&quot; &quot;April&quot; &quot;May&quot; &quot;June&quot; 15.1.7 Factores Los factores se utilizan para representar datos categóricos. Se puede pensar en ellos como vectores de enteros en los que cada entero tiene asociada una etiqueta (label). Los factores son muy importantes en la modelización estadística ya que Rlos trata de forma especial. Utilizar factores con etiquetas es preferible a utilizar enteros porque las etiquetas son auto-descriptivas. Veamos un ejemplo. Supongamos que el vector sexo indica el sexo de un persona codificado como 0 si hombre y 1 si mujer sexo &lt;- c(0, 1, 1, 1, 0, 0, 1, 0, 1) sexo ## [1] 0 1 1 1 0 0 1 0 1 table(sexo) ## sexo ## 0 1 ## 4 5 El problema de introducir así los datos es que no queda reflejado la etiquetación de los mismos. Para ello guardaremos los datos en una estructura tipo factor: sexo2 &lt;- factor(sexo, labels = c(&quot;hombre&quot;, &quot;mujer&quot;)); sexo2 ## [1] hombre mujer mujer mujer hombre hombre mujer hombre mujer ## Levels: hombre mujer levels(sexo2) # devuelve los niveles de un factor ## [1] &quot;hombre&quot; &quot;mujer&quot; unclass(sexo2) # representación subyacente del factor ## [1] 1 2 2 2 1 1 2 1 2 ## attr(,&quot;levels&quot;) ## [1] &quot;hombre&quot; &quot;mujer&quot; table(sexo2) ## sexo2 ## hombre mujer ## 4 5 Veamos otro ejemplo, en el que inicialmente tenemos datos categóricos. Los niveles se toman automáticamente por orden alfabético respuestas &lt;- factor(c(&#39;si&#39;, &#39;si&#39;, &#39;no&#39;, &#39;si&#39;, &#39;si&#39;, &#39;no&#39;, &#39;no&#39;)) respuestas ## [1] si si no si si no no ## Levels: no si Si deseásemos otro orden (lo cual puede ser importante en algunos casos, por ejemplo para representaciones gráficas), habría que indicarlo expresamente respuestas &lt;- factor(c(&#39;si&#39;, &#39;si&#39;, &#39;no&#39;, &#39;si&#39;, &#39;si&#39;, &#39;no&#39;, &#39;no&#39;), levels = c(&#39;si&#39;, &#39;no&#39;)) respuestas ## [1] si si no si si no no ## Levels: si no "],["15-2-matrices-y-arrays-1.html", "15.2 Matrices y arrays", " 15.2 Matrices y arrays 15.2.1 Matrices Las matrices son la extensión natural de los vectores a dos dimensiones. Su generalización a más dimensiones se llama array. Las matrices se pueden crear concatenando vectores con las funciones cbind o rbind: x &lt;- c(3, 7, 1, 8, 4) y &lt;- c(7, 5, 2, 1, 0) cbind(x, y) # por columnas ## x y ## [1,] 3 7 ## [2,] 7 5 ## [3,] 1 2 ## [4,] 8 1 ## [5,] 4 0 rbind(x, y) # por filas ## [,1] [,2] [,3] [,4] [,5] ## x 3 7 1 8 4 ## y 7 5 2 1 0 Una matriz se puede crear con la función matrix donde el parámetro nrow indica el número de filas y ncol el número de columnas. Por defecto, los valores se colocan por columnas. matrix(1:8, nrow = 2, ncol = 4) # equivalente a matrix(1:8, nrow=2) ## [,1] [,2] [,3] [,4] ## [1,] 1 3 5 7 ## [2,] 2 4 6 8 Los nombres de los parámetros se pueden acortar siempre y cuando no haya ambigüedad, por lo que es habitual escribir matrix(1:8, nr = 2, nc = 4) ## [,1] [,2] [,3] [,4] ## [1,] 1 3 5 7 ## [2,] 2 4 6 8 Si queremos indicar que los valores se escriban por filas matrix(1:8, nr = 2, byrow = TRUE) ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 15.2.2 Nombres en matrices Se pueden dar nombres a las filas y columnas de una matriz. x &lt;- matrix(c(1, 2, 3, 11, 12, 13), nrow = 2, byrow = TRUE) x ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 11 12 13 rownames(x) &lt;- c(&quot;fila 1&quot;, &quot;fila 2&quot;) colnames(x) &lt;- c(&quot;col 1&quot;, &quot;col 2&quot;, &quot;col 3&quot;) x ## col 1 col 2 col 3 ## fila 1 1 2 3 ## fila 2 11 12 13 Obtenemos el mismo resultado si escribimos colnames(x) &lt;- paste(&quot;col&quot;, 1:ncol(x), sep=&quot; &quot;) Internamente, las matrices son vectores con un atributo especial: la dimensión. dim(x) ## [1] 2 3 attributes(x) ## $dim ## [1] 2 3 ## ## $dimnames ## $dimnames[[1]] ## [1] &quot;fila 1&quot; &quot;fila 2&quot; ## ## $dimnames[[2]] ## [1] &quot;col 1&quot; &quot;col 2&quot; &quot;col 3&quot; 15.2.3 Acceso a los elementos de una matriz El acceso a los elementos de una matriz se realiza de forma análoga al acceso ya comentado para los vectores. x &lt;- matrix(1:6, 2, 3); x ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 x[1, 1] ## [1] 1 x[2, 2] ## [1] 4 x[2, ] # segunda fila ## [1] 2 4 6 x[ ,2] # segunda columna ## [1] 3 4 x[1, 1:2] # primera fila, columnas 1ª y 2ª ## [1] 1 3 15.2.4 Ordenación por filas y columnas En ocasiones, interesará ordenar los elementos de una matriz por los valores de una determinada columna o fila. Por ejemplo, supongamos la matriz x &lt;- c(79, 100, 116, 121, 52, 134, 123, 109, 80, 107, 66, 118) x &lt;- matrix(x, ncol=4, byrow=T); x ## [,1] [,2] [,3] [,4] ## [1,] 79 100 116 121 ## [2,] 52 134 123 109 ## [3,] 80 107 66 118 La matriz ordenada por los valores de la primera columna viene dada por ii &lt;- order(x[ ,1]) x[ii, ] # ordenación columna 1 ## [,1] [,2] [,3] [,4] ## [1,] 52 134 123 109 ## [2,] 79 100 116 121 ## [3,] 80 107 66 118 De igual modo, si queremos ordenar por los valores de la cuarta columna: ii &lt;- order(x[ ,4]); x[ii, ] # ordenación columna 4 ## [,1] [,2] [,3] [,4] ## [1,] 52 134 123 109 ## [2,] 80 107 66 118 ## [3,] 79 100 116 121 15.2.5 Operaciones con Matrices y Arrays A continuación se muestran algunas funciones que se pueden emplear con matrices Función Descripción dim(),nrow(),ncol() número de filas y/o columnas diag() diagonal de una matrix * multiplicación elemento a elemento %*% multiplicación matricial de matrices cbind(),rbind() encadenamiento de columnas o filas t() transpuesta solve(A) inversa de la matriz A solve(A,b) solución del sistema de ecuaciones \\(Ax=b\\) qr() descomposición de Cholesky eigen() autovalores y autovectores svd() descomposición singular 15.2.6 Ejemplos x &lt;- matrix(1:6, ncol = 3) x ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 t(x) # matriz transpuesta ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 4 ## [3,] 5 6 dim(x) # dimensiones de la matriz ## [1] 2 3 15.2.7 Inversión de una matriz A &lt;- matrix(c(2, 4, 0, 2), nrow = 2); A ## [,1] [,2] ## [1,] 2 0 ## [2,] 4 2 B &lt;- solve(A) B # inversa ## [,1] [,2] ## [1,] 0.5 0.0 ## [2,] -1.0 0.5 A %*% B # comprobamos que está bien ## [,1] [,2] ## [1,] 1 0 ## [2,] 0 1 "],["15-3-data-frames-3.html", "15.3 Data frames", " 15.3 Data frames Los data.fames (marcos de datos) son el objeto más habitual para el almacenamiento de datos. En este tipo de objetos cada individuo de la muestra se corresponde con una fila y cada una de las variables con una columna. Para la creación de estas estructuras se utiliza la función data.frame(). Este tipo de estructuras son en apariencia muy similares a las matrices, con la ventaja de que permiten que los valores de las distintas columnas sean de tipos diferentes. Por ejemplo, supongamos que tenemos registrados los siguientes valores Producto &lt;- c(&quot;Zumo&quot;, &quot;Queso&quot;, &quot;Yogourt&quot;) Seccion &lt;- c(&quot;Bebidas&quot;, &quot;Lácteos&quot;, &quot;Lácteos&quot;) Unidades &lt;- c(2, 1, 10) Los valores anteriores se podrían guardar en una única matriz x &lt;- cbind(Producto, Seccion, Unidades) class(x) ## [1] &quot;matrix&quot; &quot;array&quot; x ## Producto Seccion Unidades ## [1,] &quot;Zumo&quot; &quot;Bebidas&quot; &quot;2&quot; ## [2,] &quot;Queso&quot; &quot;Lácteos&quot; &quot;1&quot; ## [3,] &quot;Yogourt&quot; &quot;Lácteos&quot; &quot;10&quot; Sin embargo, el resultado anterior no es satisfactorio ya que todos los valores se han transformado en caracteres. Una solución mejor es utilizar un data.frame, con lo cual se mantiene el tipo original de las variables. lista.compra &lt;- data.frame(Producto, Seccion, Unidades) class(lista.compra) ## [1] &quot;data.frame&quot; lista.compra ## Producto Seccion Unidades ## 1 Zumo Bebidas 2 ## 2 Queso Lácteos 1 ## 3 Yogourt Lácteos 10 A continuación se muestran ejemplos que ilustran la manera de acceder a los valores de un data.frame. lista.compra$Unidades ## [1] 2 1 10 lista.compra[ ,3] # de manera equivalente ## [1] 2 1 10 lista.compra$Seccion ## [1] &quot;Bebidas&quot; &quot;Lácteos&quot; &quot;Lácteos&quot; lista.compra$Unidades[1:2] # primeros dos valores de Unidades ## [1] 2 1 lista.compra[2,] # segunda fila ## Producto Seccion Unidades ## 2 Queso Lácteos 1 La función summary() permite hacer un resumen estadístico de las variables (columnas) del data.frame. summary(lista.compra) ## Producto Seccion Unidades ## Length:3 Length:3 Min. : 1.000 ## Class :character Class :character 1st Qu.: 1.500 ## Mode :character Mode :character Median : 2.000 ## Mean : 4.333 ## 3rd Qu.: 6.000 ## Max. :10.000 "],["15-4-listas-3.html", "15.4 Listas", " 15.4 Listas Las listas son colecciones ordenadas de cualquier tipo de objetos (en R las listas son un tipo especial de vectores). Así, mientras que los elementos de los vectores, matrices y arrays deben ser del mismo tipo, en el caso de las listas se pueden tener elementos de tipos distintos. x &lt;- c(1, 2, 3, 4) y &lt;- c(&quot;Hombre&quot;, &quot;Mujer&quot;) z &lt;- matrix(1:12, ncol = 4) datos &lt;- list(A=x, B=y, C=z) datos ## $A ## [1] 1 2 3 4 ## ## $B ## [1] &quot;Hombre&quot; &quot;Mujer&quot; ## ## $C ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 "],["16-gráficos.html", "Capítulo 16 Gráficos", " Capítulo 16 Gráficos R dispone de varias funciones que permiten la realización de gráficos. Estas funciones se dividen en dos grandes grupos Gráficos de alto nivel: Crean un gráfico nuevo. plot, hist, boxplot, ... Gráficos de bajo nivel: Permiten añadir elementos (líneas, puntos, ) a un gráfico ya existente points, lines, legend, text, ... El parámetro add=TRUE convierte una función de nivel alto a bajo. Dentro de las funciones gráficas de alto nivel destaca la función plot que tiene muchas variantes y dependiendo del tipo de datos que se le pasen como argumento actuará de modo distinto. "],["16-1-el-comando-plot.html", "16.1 El comando plot", " 16.1 El comando plot Si escribimos plot(x, y) donde x e y son vectores, entonces R hará directamente el conocido como gráfico de dispersión que representa en un sistema coordenado los pares de valores \\((x,y)\\). Por ejemplo, utilizando el siguiente código data(cars) plot(cars$speed, cars$dist) # otra posibilidad plot(cars) Figura 16.1: Gráfico de dispersión de distancia frente a velocidad [Figura 16.1] El comando plot incluye por defecto una elección automáticas de títulos, ejes, escalas, etiquetas, etc., que pueden ser modificados añadiendo parámetros gráficos al comando: Parámetro Descripción type tipo de gráfico: p: puntos, l: líneas, b: puntos y líneas, n: gráfico en blanco,  xlim, ylim | límites de los ejes (e.g. xlim=c(1, 10) o xlim=range(x)) xlab, ylab | títulos de los ejes main, sub | título principal y subtítulo col | color de los símbolos (véase colors()) véase col.axis, col.lab, col.main, col.sub lty | tipo de línea lwd | anchura de línea pch | tipo de símbolo cex | tamaño de los símbolos bg | color de relleno (para pch = 21:25) Para obtener ayuda sobre estos parámetros ejecutar help(par). Veamos algún ejemplo: plot(cars, xlab = &quot;velocidad&quot;, ylab = &quot;distancia&quot;, main = &quot;Título&quot;) Figura 16.2: Gráfico de dispersión de distancia frente a velocidad, especificando título y etiquetas de los ejes [Figura 16.2] plot(cars, pch = 16, col = &#39;blue&#39;, main = &#39;pch=16&#39;) Figura 16.3: Gráfico de dispersión de distancia frente a velocidad, cambiando el color y el tipo de símbolo [Figura 16.3] "],["16-2-funciones-gráficas-de-bajo-nivel.html", "16.2 Funciones gráficas de bajo nivel", " 16.2 Funciones gráficas de bajo nivel Las principales funciones gráficas de bajo nivel son: Función Descripción points y lines agregan puntos y líneas text agrega un texto mtext agrega texto en los márgenes segments dibuja línea desde el punto inicial al final abline dibuja líneas rect dibuja rectángulos polygon dibuja polígonos legend agrega una leyenda axis agrega ejes locator devuelve coordenadas de puntos identify similar a locator "],["16-3-ejemplos-1.html", "16.3 Ejemplos", " 16.3 Ejemplos plot(cars) abline(h = c(20, 40), lty = 2) # líneas verticales discontinuas (lty=2) # selecciona puntos y los dibuja en azul sólido points(subset(cars, dist &gt; 20 &amp; dist &lt; 40), pch = 16, col = &#39;blue&#39;) x &lt;- seq(0, 2 * pi, length = 100) y1 &lt;- cos(x) y2 &lt;- sin(x) plot( x, y1, type = &quot;l&quot;, col = 2, lwd = 3, xlab = &quot;[0,2pi]&quot;, ylab = &quot;&quot;, main = &quot;Seno y Coseno&quot;) lines(x, y2, col = 3, lwd = 3, lty = 2) points(pi, 0, pch = 17, col = 4) legend(0, -0.5, c(&quot;Coseno&quot;, &quot;Seno&quot;), col = 2:3, lty = 1:2, lwd = 3) abline(v = pi, lty = 3) abline(h = 0, lty = 3) text(pi, 0, &quot;(pi,0)&quot;, adj = c(0, 0)) Alternativamente se podría usar curve(): curve(cos, 0, 2*pi, col = 2, lwd = 3, xlab = &quot;[0,2pi]&quot;, ylab = &quot;&quot;, main = &quot;Seno y Coseno&quot;) curve(sin, col = 3, lwd = 3, lty = 2, add = TRUE) "],["16-4-parámetros-gráficos.html", "16.4 Parámetros gráficos", " 16.4 Parámetros gráficos Como ya hemos visto, muchas funciones gráficas permiten establecer (temporalmente) opciones gráficas mediante estos parámetros. Con la función par() se pueden obtener y establecer (de forma permanente) todas las opciones gráficas. Algunas más de estas opciones son: Parámetro Descripción adj justificación del texto axes si es FALSE no dibuja los ejes ni la caja bg color del fondo bty tipo de caja alrededor del gráfico font estilo del texto \\ (1: normal, 2: cursiva, 3:negrita, 4: negrita cursiva) las orientación de los caracteres en los ejes mar márgenes mfcol divide la pantalla gráfica por columnas mfrow lo mismo que mfcol pero por filas Ejecutar help(par) para obtener la lista completa. "],["16-5-múltiples-gráficos-por-ventana.html", "16.5 Múltiples gráficos por ventana", " 16.5 Múltiples gráficos por ventana En R se pueden hacer varios gráficos por ventana. Para ello, antes de ejecutar la función plot, se puede utilizar la función: par(mfrow = c(filas, columnas)) Los gráficos se irán mostrando en pantalla por filas. En caso de que se quieran mostrar por columnas en la función anterior se sustituye mfrow por mfcol. Por ejemplo, con el siguiente código se obtiene el gráfico de la siguiente transparencia. par(mfrow = c(2, 3)) plot(cars, pch = 1, main = &quot;pch = 1&quot;) plot(cars, pch = 2, main = &quot;pch = 2&quot;) plot(cars, pch = 3, main = &quot;pch = 3&quot;) plot(cars, col = &quot;red&quot;, main = &quot;col = red&quot;) plot(cars, col = &quot;blue&quot;, main = &quot;col = blue&quot;) plot(cars, col = &quot;brown&quot;, main = &quot;col = brown&quot;) Para estructuras gráficas más complicadas véase help(layout). "],["16-6-exportar-gráficos-1.html", "16.6 Exportar gráficos", " 16.6 Exportar gráficos Para guardar gráficos, en Windows, se puede usar el menú Archivo -&gt; Guardar como de la ventana gráfica (seleccionando el formato deseado: bitmap, postscript,) y también mediante código ejecutando savePlot(filename, type). Alternativamente, se pueden emplear ficheros como dispositivos gráficos. Por ejemplo, a continuación guardamos un gráfico en el fichero car.pdf: pdf(&quot;cars.pdf&quot;) # abrimos el dispositivo gráfico plot(cars) dev.off() # cerramos el dispositivo Con el siguiente código guardaremos el gráfico en formato jpeg: jgeg(&quot;cars.jpg&quot;) # abrimos el dispositivo gráfico plot(cars) dev.off() # cerramos el dispositivo Otros formatos disponibles son bmp, png y tiff. Para más detalles ejecutar: help(Devices) "],["16-7-otras-librerías-gráficas.html", "16.7 Otras librerías gráficas", " 16.7 Otras librerías gráficas Además de los gráficos estándar, en R están disponibles muchas librerías gráficas adicionales: Gráficos Lattice (Trellis) Especialmente adecuados para gráficas condicionales múltiples. No se pueden combinar con las funciones estándar. Generalmente el argumento principal es una formula: y ~ x | a gráficas de y sobre x condicionadas por a y ~ x | a*b gráficas condicionadas por a y b Devuelven un objeto con el que se puede interactuar. Más librerías gráficas: ggplot2 rgl rggobi Para más detalles ver CRAN Task View: Graphics 16.7.1 Ejemplos load(&quot;datos/empleados.RData&quot;) library(lattice) xyplot(log(salario) ~ log(salini) | sexoraza, data = empleados) # Equivalente a xyplot(log(salario) ~ log(salini) | sexo*minoria, data = empleados) library(ggplot2) ggplot(empleados, aes(log(salini), log(salario), col = sexo)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) ggplot(empleados, aes(salario, fill = sexo)) + geom_density(alpha=.5) "],["17-manipulación-de-datos-con-r.html", "Capítulo 17 Manipulación de datos con R", " Capítulo 17 Manipulación de datos con R La mayoría de los estudios estadísticos requieren disponer de un conjunto de datos. "],["17-1-lectura-importación-y-exportación-de-datos.html", "17.1 Lectura, importación y exportación de datos", " 17.1 Lectura, importación y exportación de datos Además de la introducción directa, R es capaz de importar datos externos en múltiples formatos: bases de datos disponibles en librerías de R archivos de texto en formato ASCII archivos en otros formatos: Excel, SPSS,  bases de datos relacionales: MySQL, Oracle,  formatos web: HTML, XML, JSON,  . 17.1.1 Formato de datos de R El formato de archivo en el que habitualmente se almacena objetos (datos) R es binario y está comprimido (en formato \"gzip\" por defecto). Para cargar un fichero de datos se emplea normalmente load(): res &lt;- load(&quot;datos/empleados.RData&quot;) res ## [1] &quot;empleados&quot; ls() ## [1] &quot;citefig&quot; &quot;citefig2&quot; &quot;citepkg&quot; &quot;citepkgs&quot; &quot;empleados&quot; &quot;fig.path&quot; ## [7] &quot;inline&quot; &quot;inline2&quot; &quot;is_html&quot; &quot;is_latex&quot; &quot;latexfig&quot; &quot;res&quot; y para guardar save(): # Guardar save(empleados, file = &quot;datos/empleados_new.RData&quot;) El objeto empleado normalmente en R para almacenar datos en memoria es el data.frame. 17.1.2 Acceso a datos en paquetes R dispone de múltiples conjuntos de datos en distintos paquetes, especialmente en el paquete datasets que se carga por defecto al abrir R. Con el comando data() podemos obtener un listado de las bases de datos disponibles. Para cargar una base de datos concreta se utiliza el comando data(nombre) (aunque en algunos casos se cargan automáticamente al emplearlos). Por ejemplo, data(cars) carga la base de datos llamada cars en el entorno de trabajo (\".GlobalEnv\") y ?cars muestra la ayuda correspondiente con la descripición de la base de datos. 17.1.3 Lectura de archivos de texto En R para leer archivos de texto se suele utilizar la función read.table(). Supóngase, por ejemplo, que en el directorio actual está el fichero empleados.txt. La lectura de este fichero vendría dada por el código: # Session &gt; Set Working Directory &gt; To Source...? datos &lt;- read.table(file = &quot;datos/empleados.txt&quot;, header = TRUE) # head(datos) str(datos) ## &#39;data.frame&#39;: 474 obs. of 10 variables: ## $ id : int 1 2 3 4 5 6 7 8 9 10 ... ## $ sexo : chr &quot;Hombre&quot; &quot;Hombre&quot; &quot;Mujer&quot; &quot;Mujer&quot; ... ## $ fechnac : chr &quot;2/3/1952&quot; &quot;5/23/1958&quot; &quot;7/26/1929&quot; &quot;4/15/1947&quot; ... ## $ educ : int 15 16 12 8 15 15 15 12 15 12 ... ## $ catlab : chr &quot;Directivo&quot; &quot;Administrativo&quot; &quot;Administrativo&quot; &quot;Administrativo&quot; ... ## $ salario : num 57000 40200 21450 21900 45000 ... ## $ salini : int 27000 18750 12000 13200 21000 13500 18750 9750 12750 13500 ... ## $ tiempemp: int 98 98 98 98 98 98 98 98 98 98 ... ## $ expprev : int 144 36 381 190 138 67 114 0 115 244 ... ## $ minoria : chr &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; ... Si el fichero estuviese en el directorio c:\\datos bastaría con especificar file = \"c:/datos/empleados.txt\". Nótese también que para la lectura del fichero anterior se ha establecido el argumento header=TRUE para indicar que la primera línea del fichero contiene los nombres de las variables. Los argumentos utilizados habitualmente para esta función son: header: indica si el fichero tiene cabecera (header=TRUE) o no (header=FALSE). Por defecto toma el valor header=FALSE. sep: carácter separador de columnas que por defecto es un espacio en blanco (sep=\"\"). Otras opciones serían: sep=\",\" si el separador es un ; sep=\"*\" si el separador es un *, etc. dec: carácter utilizado en el fichero para los números decimales. Por defecto se establece dec = \".\". Si los decimales vienen dados por , se utiliza dec = \",\" Resumiendo, los (principales) argumentos por defecto de la función read.table son los que se muestran en la siguiente línea: read.table(file, header = FALSE, sep = &quot;&quot;, dec = &quot;.&quot;) Para más detalles sobre esta función véase help(read.table). Estan disponibles otras funciones con valores por defecto de los parámetros adecuados para otras situaciones. Por ejemplo, para ficheros separados por tabuladores se puede utilizar read.delim() o read.delim2(): read.delim(file, header = TRUE, sep = &quot;\\t&quot;, dec = &quot;.&quot;) read.delim2(file, header = TRUE, sep = &quot;\\t&quot;, dec = &quot;,&quot;) 17.1.4 Alternativa tidyverse Para leer archivos de texto en distintos formatos también se puede emplear el paquete readr (colección tidyverse), para lo que se recomienda consultar el Capítulo 11 del libro R for Data Science. 17.1.5 Importación desde SPSS El programa R permite lectura de ficheros de datos en formato SPSS (extensión .sav) sin necesidad de tener instalado dicho programa en el ordenador. Para ello se necesita: cargar la librería foreign utilizar la función read.spss Por ejemplo: library(foreign) datos &lt;- read.spss(file = &quot;datos/Employee data.sav&quot;, to.data.frame = TRUE) # head(datos) str(datos) ## &#39;data.frame&#39;: 474 obs. of 10 variables: ## $ id : num 1 2 3 4 5 6 7 8 9 10 ... ## $ sexo : Factor w/ 2 levels &quot;Hombre&quot;,&quot;Mujer&quot;: 1 1 2 2 1 1 1 2 2 2 ... ## $ fechnac : num 1.17e+10 1.19e+10 1.09e+10 1.15e+10 1.17e+10 ... ## $ educ : Factor w/ 10 levels &quot;8&quot;,&quot;12&quot;,&quot;14&quot;,..: 4 5 2 1 4 4 4 2 4 2 ... ## $ catlab : Factor w/ 3 levels &quot;Administrativo&quot;,..: 3 1 1 1 1 1 1 1 1 1 ... ## $ salario : Factor w/ 221 levels &quot;15750&quot;,&quot;15900&quot;,..: 179 137 28 31 150 101 121 31 71 45 ... ## $ salini : Factor w/ 90 levels &quot;9000&quot;,&quot;9750&quot;,..: 60 42 13 21 48 23 42 2 18 23 ... ## $ tiempemp: Factor w/ 36 levels &quot;63&quot;,&quot;64&quot;,&quot;65&quot;,..: 36 36 36 36 36 36 36 36 36 36 ... ## $ expprev : Factor w/ 208 levels &quot;Ausente&quot;,&quot;10&quot;,..: 38 131 139 64 34 181 13 1 14 91 ... ## $ minoria : Factor w/ 2 levels &quot;No&quot;,&quot;Sí&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## - attr(*, &quot;variable.labels&quot;)= Named chr [1:10] &quot;Código de empleado&quot; &quot;Sexo&quot; &quot;Fecha de nacimiento&quot; &quot;Nivel educativo&quot; ... ## ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;id&quot; &quot;sexo&quot; &quot;fechnac&quot; &quot;educ&quot; ... ## - attr(*, &quot;codepage&quot;)= int 1252 Nota: Si hay fechas, puede ser recomendable emplear la función spss.get() del paquete Hmisc. 17.1.6 Importación desde Excel Se pueden leer fichero de Excel (con extensión .xlsx) utilizando por ejemplo los paquetes openxlsx, readxl (colección tidyverse), XLConnect o RODBC (este paquete se empleará más adelante para acceder a bases de datos), entre otros. Sin embargo, un procedimiento sencillo consiste en exportar los datos desde Excel a un archivo de texto separado por tabuladores (extensión .csv). Por ejemplo, supongamos que queremos leer el fichero coches.xls: Desde Excel se selecciona el menú Archivo -&gt; Guardar como -&gt; Guardar como y en Tipo se escoge la opción de archivo CSV. De esta forma se guardarán los datos en el archivo coches.csv. El fichero coches.csv es un fichero de texto plano (se puede editar con Notepad), con cabecera, las columnas separadas por ; y siendo , el carácter decimal. Por lo tanto, la lectura de este fichero se puede hacer con: datos &lt;- read.table(&quot;coches.csv&quot;, header = TRUE, sep = &quot;;&quot;, dec = &quot;,&quot;) Otra posibilidad es utilizar la función read.csv2, que es una adaptación de la función general read.table con las siguientes opciones: read.csv2(file, header = TRUE, sep = &quot;;&quot;, dec = &quot;,&quot;) Por lo tanto, la lectura del fichero coches.csv se puede hacer de modo más directo con: datos &lt;- read.csv2(&quot;coches.csv&quot;) 17.1.7 Exportación de datos Puede ser de interés la exportación de datos para que puedan leídos con otros programas. Para ello, se puede emplear la función write.table(). Esta función es similar, pero funcionando en sentido inverso, a read.table() (Sección 17.1.3). Veamos un ejemplo: tipo &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;) longitud &lt;- c(120.34, 99.45, 115.67) datos &lt;- data.frame(tipo, longitud) datos ## tipo longitud ## 1 A 120.34 ## 2 B 99.45 ## 3 C 115.67 Para guardar el data.frame datos en un fichero de texto se puede utilizar: write.table(datos, file = &quot;datos.txt&quot;) Otra posibilidad es utilizar la función: write.csv2(datos, file = &quot;datos.csv&quot;) que dará lugar al fichero datos.csv importable directamente desde Excel. "],["17-2-manipulación-de-datos.html", "17.2 Manipulación de datos", " 17.2 Manipulación de datos Una vez cargada una (o varias) bases de datos hay una series de operaciones que serán de interés para el tratamiento de datos: Operaciones con variables: crear recodificar (e.g. categorizar)  Operaciones con casos: ordenar filtrar  A continuación se tratan algunas operaciones básicas. 17.2.1 Operaciones con variables 17.2.1.1 Creación (y eliminación) de variables Consideremos de nuevo la base de datos cars incluida en el paquete datasets: data(cars) # str(cars) head(cars) ## speed dist ## 1 4 2 ## 2 4 10 ## 3 7 4 ## 4 7 22 ## 5 8 16 ## 6 9 10 Utilizando el comando help(cars) se obtiene que cars es un data.frame con 50 observaciones y dos variables: speed: Velocidad (millas por hora) dist: tiempo hasta detenerse (pies) Recordemos que, para acceder a la variable speed se puede hacer directamente con su nombre o bien utilizando notación matricial. cars$speed ## [1] 4 4 7 7 8 9 10 10 10 11 11 12 12 12 12 13 13 13 13 14 14 14 14 15 15 ## [26] 15 16 16 17 17 17 18 18 18 18 19 19 19 20 20 20 20 20 22 23 24 24 24 24 25 cars[, 1] # Equivalente ## [1] 4 4 7 7 8 9 10 10 10 11 11 12 12 12 12 13 13 13 13 14 14 14 14 15 15 ## [26] 15 16 16 17 17 17 18 18 18 18 19 19 19 20 20 20 20 20 22 23 24 24 24 24 25 Supongamos ahora que queremos transformar la variable original speed (millas por hora) en una nueva variable velocidad (kilómetros por hora) y añadir esta nueva variable al data.frame cars. La transformación que permite pasar millas a kilómetros es kilómetros=millas/0.62137 que en R se hace directamente con: cars$speed/0.62137 Finalmente, incluimos la nueva variable que llamaremos velocidad en cars: cars[, c(&quot;velocidad&quot;)] &lt;- cars[, 1]/0.62137 head(cars) ## speed dist velocidad ## 1 4 2 6.437388 ## 2 4 10 6.437388 ## 3 7 4 11.265430 ## 4 7 22 11.265430 ## 5 8 16 12.874777 ## 6 9 10 14.484124 También transformaremos la variable dist (en pies) en una nueva variable distancia (en metros). Ahora la transformación deseada es metros=pies/3.2808: cars[, c(&quot;distancia&quot;)] &lt;- cars[, 2]/3.2808 head(cars) ## speed dist velocidad distancia ## 1 4 2 6.437388 0.6096074 ## 2 4 10 6.437388 3.0480371 ## 3 7 4 11.265430 1.2192148 ## 4 7 22 11.265430 6.7056815 ## 5 8 16 12.874777 4.8768593 ## 6 9 10 14.484124 3.0480371 Ahora, eliminaremos las variables originales speed y dist, y guardaremos el data.frame resultante con el nombre coches. En primer lugar, veamos varias formas de acceder a las variables de interés: cars[, c(3, 4)] cars[, c(&quot;velocidad&quot;, &quot;distancia&quot;)] cars[, -c(1, 2)] Utilizando alguna de las opciones anteriores se obtiene el data.frame deseado: coches &lt;- cars[, c(&quot;velocidad&quot;, &quot;distancia&quot;)] # head(coches) str(coches) ## &#39;data.frame&#39;: 50 obs. of 2 variables: ## $ velocidad: num 6.44 6.44 11.27 11.27 12.87 ... ## $ distancia: num 0.61 3.05 1.22 6.71 4.88 ... Finalmente los datos anteriores podrían ser guardados en un fichero exportable a Excel con el siguiente comando: write.csv2(coches, file = &quot;coches.csv&quot;) 17.2.2 Operaciones con casos 17.2.2.1 Ordenación Continuemos con el data.frame cars. Se puede comprobar que los datos disponibles están ordenados por los valores de speed. A continuación haremos la ordenación utilizando los valores de dist. Para ello utilizaremos el conocido como vector de índices de ordenación. Este vector establece el orden en que tienen que ser elegidos los elementos para obtener la ordenación deseada. Veamos un ejemplo sencillo: x &lt;- c(2.5, 4.3, 1.2, 3.1, 5.0) # valores originales ii &lt;- order(x) ii # vector de ordenación ## [1] 3 1 4 2 5 x[ii] # valores ordenados ## [1] 1.2 2.5 3.1 4.3 5.0 En el caso de vectores, el procedimiento anterior se podría hacer directamente con: sort(x) Sin embargo, para ordenar data.frames será necesario la utilización del vector de índices de ordenación. A continuación, los datos de cars ordenados por dist: ii &lt;- order(cars$dist) # Vector de índices de ordenación cars2 &lt;- cars[ii, ] # Datos ordenados por dist head(cars2) ## speed dist velocidad distancia ## 1 4 2 6.437388 0.6096074 ## 3 7 4 11.265430 1.2192148 ## 2 4 10 6.437388 3.0480371 ## 6 9 10 14.484124 3.0480371 ## 12 12 14 19.312165 4.2672519 ## 5 8 16 12.874777 4.8768593 17.2.2.2 Filtrado El filtrado de datos consiste en elegir una submuestra que cumpla determinadas condiciones. Para ello se puede utilizar la función subset() (que además permite seleccionar variables). A continuación se muestran un par de ejemplos: subset(cars, cars$dist &gt; 85) # datos con dis&gt;85 ## speed dist velocidad distancia ## 47 24 92 38.62433 28.04194 ## 48 24 93 38.62433 28.34674 ## 49 24 120 38.62433 36.57644 subset(cars, cars$speed &gt; 10 &amp; cars$speed &lt; 15 &amp; cars$dist &gt; 45) # speed en (10,15) y dist&gt;45 ## speed dist velocidad distancia ## 19 13 46 20.92151 14.02097 ## 22 14 60 22.53086 18.28822 ## 23 14 80 22.53086 24.38430 También se pueden hacer el filtrado empleando directamente los correspondientes vectores de índices: ii &lt;- cars$dist &gt; 85 cars[ii, ] # dis&gt;85 ## speed dist velocidad distancia ## 47 24 92 38.62433 28.04194 ## 48 24 93 38.62433 28.34674 ## 49 24 120 38.62433 36.57644 ii &lt;- cars$speed &gt; 10 &amp; cars$speed &lt; 15 &amp; cars$dist &gt; 45 cars[ii, ] # speed en (10,15) y dist&gt;45 ## speed dist velocidad distancia ## 19 13 46 20.92151 14.02097 ## 22 14 60 22.53086 18.28822 ## 23 14 80 22.53086 24.38430 En este caso puede ser de utilidad la función which(): it &lt;- which(ii) str(it) ## int [1:3] 19 22 23 cars[it, 1:2] ## speed dist ## 19 13 46 ## 22 14 60 ## 23 14 80 # rownames(cars[it, 1:2]) id &lt;- which(!ii) str(cars[id, 1:2]) ## &#39;data.frame&#39;: 47 obs. of 2 variables: ## $ speed: num 4 4 7 7 8 9 10 10 10 11 ... ## $ dist : num 2 10 4 22 16 10 18 26 34 17 ... # Se podría p.e. emplear cars[id, ] para predecir cars[it, ]$speed # ?which.min "],["18-análisis-exploratorio-de-datos.html", "Capítulo 18 Análisis exploratorio de datos", " Capítulo 18 Análisis exploratorio de datos El objetivo del análisis exploratorio de datos es presentar una descripción de los mismos que faciliten su análisis mediante procedimientos que permitan: Organizar los datos Resumirlos Representarlos gráficamente Análizar la información "],["18-1-medidas-resumen.html", "18.1 Medidas resumen", " 18.1 Medidas resumen 18.1.1 Datos de ejemplo El fichero empleados.RData contiene datos de empleados de un banco que utilizaremos, entre otros, a modo de ejemplo. load(&quot;datos/empleados.RData&quot;) data.frame(Etiquetas = attr(empleados, &quot;variable.labels&quot;)) # Listamos las etiquetas ## Etiquetas ## id Código de empleado ## sexo Sexo ## fechnac Fecha de nacimiento ## educ Nivel educativo (años) ## catlab Categoría Laboral ## salario Salario actual ## salini Salario inicial ## tiempemp Meses desde el contrato ## expprev Experiencia previa (meses) ## minoria Clasificación étnica ## sexoraza Clasificación por sexo y raza Para hacer referencia directamente a las variables de empleados attach(empleados) 18.1.2 Tablas de frecuencias table(sexo) ## sexo ## Hombre Mujer ## 258 216 prop.table(table(sexo)) ## sexo ## Hombre Mujer ## 0.5443038 0.4556962 table(sexo,catlab) ## catlab ## sexo Administrativo Seguridad Directivo ## Hombre 157 27 74 ## Mujer 206 0 10 prop.table(table(sexo,catlab)) ## catlab ## sexo Administrativo Seguridad Directivo ## Hombre 0.33122363 0.05696203 0.15611814 ## Mujer 0.43459916 0.00000000 0.02109705 prop.table(table(sexo,catlab), 1) ## catlab ## sexo Administrativo Seguridad Directivo ## Hombre 0.6085271 0.1046512 0.2868217 ## Mujer 0.9537037 0.0000000 0.0462963 prop.table(table(sexo,catlab), 2) ## catlab ## sexo Administrativo Seguridad Directivo ## Hombre 0.4325069 1.0000000 0.8809524 ## Mujer 0.5674931 0.0000000 0.1190476 table(catlab,educ,sexo) ## , , sexo = Hombre ## ## educ ## catlab 8 12 14 15 16 17 18 19 20 21 ## Administrativo 10 48 6 78 10 2 2 1 0 0 ## Seguridad 13 13 0 1 0 0 0 0 0 0 ## Directivo 0 1 0 4 25 8 7 26 2 1 ## ## , , sexo = Mujer ## ## educ ## catlab 8 12 14 15 16 17 18 19 20 21 ## Administrativo 30 128 0 33 14 1 0 0 0 0 ## Seguridad 0 0 0 0 0 0 0 0 0 0 ## Directivo 0 0 0 0 10 0 0 0 0 0 round(prop.table(table(catlab,educ,sexo)),2) ## , , sexo = Hombre ## ## educ ## catlab 8 12 14 15 16 17 18 19 20 21 ## Administrativo 0.02 0.10 0.01 0.16 0.02 0.00 0.00 0.00 0.00 0.00 ## Seguridad 0.03 0.03 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## Directivo 0.00 0.00 0.00 0.01 0.05 0.02 0.01 0.05 0.00 0.00 ## ## , , sexo = Mujer ## ## educ ## catlab 8 12 14 15 16 17 18 19 20 21 ## Administrativo 0.06 0.27 0.00 0.07 0.03 0.00 0.00 0.00 0.00 0.00 ## Seguridad 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## Directivo 0.00 0.00 0.00 0.00 0.02 0.00 0.00 0.00 0.00 0.00 Si la variable es ordinal, entonces también son de interés las frecuencias acumuladas table(educ) ## educ ## 8 12 14 15 16 17 18 19 20 21 ## 53 190 6 116 59 11 9 27 2 1 prop.table(table(educ)) ## educ ## 8 12 14 15 16 17 ## 0.111814346 0.400843882 0.012658228 0.244725738 0.124472574 0.023206751 ## 18 19 20 21 ## 0.018987342 0.056962025 0.004219409 0.002109705 cumsum(table(educ)) ## 8 12 14 15 16 17 18 19 20 21 ## 53 243 249 365 424 435 444 471 473 474 cumsum(prop.table(table(educ))) ## 8 12 14 15 16 17 18 19 ## 0.1118143 0.5126582 0.5253165 0.7700422 0.8945148 0.9177215 0.9367089 0.9936709 ## 20 21 ## 0.9978903 1.0000000 18.1.3 Media y varianza La media es la medida de centralización por excelencia. Para su cálculo se utiliza la instrucción mean consumo&lt;-c(6.9, 6.3, 6.2, 6.5 ,6.4, 6.8, 6.6) mean(consumo) ## [1] 6.528571 dotchart(consumo,pch=16) text(mean(consumo),2.5, pos=3,expression(bar(X)==6.53)) arrows(mean(consumo),0,mean(consumo),2.5,length = 0.15,col=&#39;red&#39;) mean(salario) ## [1] 34419.57 mean(subset(empleados,catlab==&#39;Directivo&#39;)$salario) ## [1] 63977.8 También se puede utilizar la función tapply, que se estudiará con detalle más adelante tapply(salario, catlab, mean) ## Administrativo Seguridad Directivo ## 27838.54 30938.89 63977.80 La principal medida de dispersión es la varianza. En la práctica, cuando se trabaja con datos muestrales, se sustituye por la cuasi-varianza (también llamada varianza muestral corregida), que se calcula mediante el comando var var(consumo) ## [1] 0.06571429 var(salario) ## [1] 291578214 La cuasi-desviación típica se calcula sd(consumo) ## [1] 0.256348 sd(salario) ## [1] 17075.66 o, equivalentemente, sqrt(var(consumo)) ## [1] 0.256348 sqrt(var(salario)) ## [1] 17075.66 La media de dispersión adimensional (relativa) más utilizada es el coeficiente de variación (de Pearson) sd(consumo)/abs(mean(consumo)) ## [1] 0.03926555 que también podemos expresar en tanto por cien 100*sd(consumo)/abs(mean(consumo)) ## [1] 3.926555 El coeficiente de variación nos permite, entre otras cosas, comparar dispersiones de variables medidas en diferentes unidades 100*sd(salini)/abs(mean(salini)) ## [1] 46.2541 100*sd(salario)/abs(mean(salario)) ## [1] 49.61033 100*sd(expprev)/abs(mean(expprev)) ## [1] 109.1022 18.1.4 Mediana y cuantiles La mediana es una medida de centralización robusta. Se calcula mediante median diametro &lt;- c(3.88,4.09,3.92,3.97,4.02,3.95, 4.03,3.92,3.98,5.60) dotchart(diametro,pch=16,xlab=&quot;diámetro&quot;) abline(v=mean(diametro),col=&#39;red&#39;,lwd=2) abline(v=median(diametro),col=&#39;blue&#39;,lty=2,lwd=2) legend(&quot;bottomright&quot;,c(&quot;media&quot;,&quot;mediana&quot;), col=c(&quot;red&quot;,&quot;blue&quot;),lty=c(1,2),lwd=c(2,2),box.lty=0,cex=1.5) Podemos comprobar que la variable salario presenta una asimetría derecha mean(salario); median(salario) ## [1] 34419.57 ## [1] 28875 Calculemos cuántos empleados tienen un salario inferior al salario medio mean(salario &lt; mean(salario)) ## [1] 0.6940928 paste(&#39;El &#39;, round(100*mean(salario &lt; mean(salario)),0), &#39;%&#39;, &#39; de los empleados tienen un salario inferior al salario medio&#39;, sep=&#39;&#39;) ## [1] &quot;El 69% de los empleados tienen un salario inferior al salario medio&quot; Como sabemos, la mitad de los empleados tienen un salario inferior a la mediana mean(salario &lt; median(salario)) ## [1] 0.5 Los cuantiles son una generalización de la mediana, que se corresponde con el cuantil de orden 0.5. R contempla distintas formas de calcular los cuantiles median(c(1,2,3,4)) ## [1] 2.5 quantile(c(1,2,3,4),0.5) ## 50% ## 2.5 quantile(c(1,2,3,4),0.5,type=1) ## 50% ## 2 Calculemos los cuartiles y los deciles de la variable salario quantile(salario) ## 0% 25% 50% 75% 100% ## 15750.0 24000.0 28875.0 36937.5 135000.0 quantile(salario, probs=c(0.25,0.5,0.75)) ## 25% 50% 75% ## 24000.0 28875.0 36937.5 quantile(salario, probs=seq(0.1, 0.9, 0.1)) ## 10% 20% 30% 40% 50% 60% 70% 80% 90% ## 21045.0 22950.0 24885.0 26700.0 28875.0 30750.0 34500.0 40920.0 59392.5 El rango y el rango intercuartílico data.frame(Rango=max(salario)-min(salario), RI=as.numeric(quantile(salario, 0.75) - quantile(salario, 0.25))) ## Rango RI ## 1 119250 12937.5 18.1.5 Summary summary(empleados) ## id sexo fechnac educ ## Min. : 1.0 Hombre:258 Min. :1929-02-10 Min. : 8.00 ## 1st Qu.:119.2 Mujer :216 1st Qu.:1948-01-03 1st Qu.:12.00 ## Median :237.5 Median :1962-01-23 Median :12.00 ## Mean :237.5 Mean :1956-10-08 Mean :13.49 ## 3rd Qu.:355.8 3rd Qu.:1965-07-06 3rd Qu.:15.00 ## Max. :474.0 Max. :1971-02-10 Max. :21.00 ## NA&#39;s :1 ## catlab salario salini tiempemp ## Administrativo:363 Min. : 15750 Min. : 9000 Min. :63.00 ## Seguridad : 27 1st Qu.: 24000 1st Qu.:12488 1st Qu.:72.00 ## Directivo : 84 Median : 28875 Median :15000 Median :81.00 ## Mean : 34420 Mean :17016 Mean :81.11 ## 3rd Qu.: 36938 3rd Qu.:17490 3rd Qu.:90.00 ## Max. :135000 Max. :79980 Max. :98.00 ## ## expprev minoria sexoraza ## Min. : 0.00 No:370 Blanca varón :194 ## 1st Qu.: 19.25 Sí:104 Minoría varón: 64 ## Median : 55.00 Blanca mujer :176 ## Mean : 95.86 Minoría mujer: 40 ## 3rd Qu.:138.75 ## Max. :476.00 ## summary(subset(empleados,catlab==&#39;Directivo&#39;)) ## id sexo fechnac educ ## Min. : 1.0 Hombre:74 Min. :1937-07-12 Min. :12.00 ## 1st Qu.:102.5 Mujer :10 1st Qu.:1954-08-09 1st Qu.:16.00 ## Median :233.5 Median :1961-05-29 Median :17.00 ## Mean :234.1 Mean :1958-11-26 Mean :17.25 ## 3rd Qu.:344.2 3rd Qu.:1963-10-03 3rd Qu.:19.00 ## Max. :468.0 Max. :1966-04-05 Max. :21.00 ## catlab salario salini tiempemp ## Administrativo: 0 Min. : 34410 Min. :15750 Min. :64.00 ## Seguridad : 0 1st Qu.: 51956 1st Qu.:23063 1st Qu.:73.00 ## Directivo :84 Median : 60500 Median :28740 Median :81.00 ## Mean : 63978 Mean :30258 Mean :81.15 ## 3rd Qu.: 71281 3rd Qu.:34058 3rd Qu.:91.00 ## Max. :135000 Max. :79980 Max. :98.00 ## expprev minoria sexoraza ## Min. : 3.00 No:80 Blanca varón :70 ## 1st Qu.: 19.75 Sí: 4 Minoría varón: 4 ## Median : 52.00 Blanca mujer :10 ## Mean : 77.62 Minoría mujer: 0 ## 3rd Qu.:125.25 ## Max. :285.00 "],["18-2-gráficos-1.html", "18.2 Gráficos", " 18.2 Gráficos 18.2.1 Diagrama de barras y gráfico de sectores table(catlab) ## catlab ## Administrativo Seguridad Directivo ## 363 27 84 par(mfrow = c(1, 3)) barplot(table(catlab),main=&quot;frecuencia absoluta&quot;) barplot(100*prop.table(table(catlab)),main=&quot;frecuencia relativa (%)&quot;) pie(table(catlab)) nj &lt;- table(educ) fj &lt;- prop.table(nj) Nj &lt;- cumsum(nj) Fj &lt;- cumsum(fj) layout(matrix(c(1,2,5,3,4,5), 2, 3, byrow=TRUE), respect=TRUE) barplot(nj,main=&quot;frecuencia absolutas&quot;,xlab=&#39;años de estudio&#39;) barplot(fj,main=&quot;frecuencia relativas&quot;,xlab=&#39;años de estudio&#39;) barplot(Nj,main=&quot;frecuencia absolutas acumuladas&quot;,xlab=&#39;años de estudio&#39;) barplot(Fj,main=&quot;frecuencia relativas acumuladas&quot;,xlab=&#39;años de estudio&#39;) pie(nj,col=rainbow(6),main=&#39;años de estudio&#39;) par(mfrow = c(1, 1)) Con datos continuos, podemos hacer uso de la función cut (más adelante veremos como se representa el histograma) table(cut(expprev, breaks=5)) ## ## (-0.476,95.2] (95.2,190] (190,286] (286,381] (381,476] ## 312 81 46 22 13 barplot(table(cut(expprev,breaks=5)),xlab=&quot;Experiencia previa&quot;, main=&quot;Categorización en 5 clases&quot;) Debemos ser muy cuidadosos a la hora de valorar gráficas como la siguiente tt &lt;- table(cut(expprev, breaks=c(0,40,80,150,250,400))) barplot(tt,xlab=&quot;Experiencia previa&quot;, main=&quot;Categorización en 5 clases&quot;) 18.2.2 Gráfico de puntos dotchart(salario, xlab=&#39;salarios&#39;) stripchart(salario~sexo, method=&#39;jitter&#39;) 18.2.3 Árbol de tallo y hojas Esta representación puede ser útil cuando se dispone de pocos datos. stem(salario) ## ## The decimal point is 4 digit(s) to the right of the | ## ## 1 | 666666777777777778888999 ## 2 | 00000000000000111111111111111111122222222222222222222222233333333333+148 ## 3 | 00000000000000000001111111111111111111111111122222222222223333333333+36 ## 4 | 0000000001112222334445555666778899 ## 5 | 0111123344555556677778999 ## 6 | 0001122355566777888999 ## 7 | 00134455889 ## 8 | 01346 ## 9 | 1127 ## 10 | 044 ## 11 | 1 ## 12 | ## 13 | 5 stem(tiempemp) ## ## The decimal point is at the | ## ## 62 | 000 ## 64 | 00000000000000000000000 ## 66 | 000000000000000000000000000000000 ## 68 | 0000000000000000000000000000000 ## 70 | 0000000000000000 ## 72 | 00000000000000000000000000 ## 74 | 000000000000000 ## 76 | 00000000000000000000000 ## 78 | 000000000000000000000000000000000000 ## 80 | 00000000000000000000000000000000000000 ## 82 | 0000000000000000000000000000000000 ## 84 | 000000000000000000000000 ## 86 | 000000000000000000000000 ## 88 | 00000000000000000000 ## 90 | 00000000000000000000000000000 ## 92 | 00000000000000000000000000000000000000 ## 94 | 00000000000000000000 ## 96 | 000000000000000000000000000 ## 98 | 00000000000000 18.2.4 Histograma Este gráfico es uno de los más habituales para representar datos continuos hist(salario, main=&#39;número de clases por defecto&#39;) hist(salario, breaks=3, main=&#39;3 intervalos de clase&#39;) hist(salario, breaks=100, main=&#39;100 intervalos de clase&#39;) cl1 &lt;- seq(15000,40000,5000) cl2 &lt;- seq(50000,80000,10000) cl3 &lt;- seq(100000,140000,20000) hist(salario, breaks=c(cl1,cl2,cl3),main=&#39;intervalos de clase de distinta amplitud&#39;) 18.2.5 Gráfico de densidad Es una versión suavizada del histograma. plot(density(salario)) hist(salario, freq=F, main=&#39;&#39;) lines(density(salario), lwd=3, col=&#39;red&#39;) El paquete car nos da acceso a la instrucción densityPlot: library(car) # help(car) densityPlot(salario~sexo) 18.2.6 Diagrama de cajas Se trata de un gráfico muy polivalente boxplot(salario, horizontal=T, axes=F) axis(1) par(mfrow=c(1,2)) boxplot(salario~catlab) boxplot(salario~sexo) par(mfrow=c(1,1)) boxplot(salario~sexo*catlab) boxplot(salini, salario) hist(salario,probability=T,ylab=&quot;&quot;,col=&#39;grey&#39;,axes=F,main=&quot;&quot;); axis(1) lines(density(salario),col=&#39;red&#39;,lwd=2) par(new=T) boxplot(salario,horizontal=T,axes=F,lwd=2) 18.2.7 Gráfica de dispersión Permite ver la relación entre dos variables: plot(educ,salario) plot(tiempemp,salario) plot(salini,salario) En el caso de una serie temporal AirPassengers ## Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec ## 1949 112 118 132 129 121 135 148 148 136 119 104 118 ## 1950 115 126 141 135 125 149 170 170 158 133 114 140 ## 1951 145 150 178 163 172 178 199 199 184 162 146 166 ## 1952 171 180 193 181 183 218 230 242 209 191 172 194 ## 1953 196 196 236 235 229 243 264 272 237 211 180 201 ## 1954 204 188 235 227 234 264 302 293 259 229 203 229 ## 1955 242 233 267 269 270 315 364 347 312 274 237 278 ## 1956 284 277 317 313 318 374 413 405 355 306 271 306 ## 1957 315 301 356 348 355 422 465 467 404 347 305 336 ## 1958 340 318 362 348 363 435 491 505 404 359 310 337 ## 1959 360 342 406 396 420 472 548 559 463 407 362 405 ## 1960 417 391 419 461 472 535 622 606 508 461 390 432 plot(AirPassengers) Y un último ejemplo utilizando los datos iris de Fisher: plot(iris[,3],iris[,4],main=&quot;Longitud y anchura de pétalos de lirios&quot;, xlab=&quot;Longitud de pétalo&quot;,ylab=&quot;Anchura de pétalo&quot;) iris.color&lt;-c(&quot;red&quot;,&quot;green&quot;,&quot;blue&quot;)[iris$Species] plot(iris[,3],iris[,4],col=iris.color,main=&quot;Longitud y anchura de pétalo según especies&quot;,xlab=&quot;Longitud de pétalo&quot;, ylab=&quot;Anchura de pétalo&quot;) legend(&quot;topleft&quot;,c(&quot;Setosa&quot;,&quot;Versicolor&quot;,&quot;Virginica&quot;),pch=1, col=c(&quot;red&quot;,&quot;green&quot;,&quot;blue&quot;),box.lty=0) pairs(iris[,1:4],col=iris.color) "],["19-inferencia-estadística.html", "Capítulo 19 Inferencia estadística", " Capítulo 19 Inferencia estadística El objetivo de este capítulo es ofrecer un primer acercamiento a la inferencia estadística, cubriendo de forma somera los siguientes apartados: contrastes de normalidad contrastes paramétricos y no paramétricos, con una y dos muestras regresión y correlación análisis de la varianza con un factor En este capítulo utilizaremos como ejemplo los datos de clientes de una compañía de distribución industrial (HATCO) contenidos en el fichero hatco.RData. load(&#39;datos/hatco.RData&#39;) Listado de etiquetas as.data.frame(attr(hatco, &quot;variable.labels&quot;)) ## attr(hatco, &quot;variable.labels&quot;) ## empresa Empresa ## tamano Tamaño de la empresa ## adquisic Estructura de adquisición ## tindustr Tipo de industria ## tsitcomp Tipo de situación de compra ## velocida Velocidad de entrega ## precio Nivel de precios ## flexprec Flexibilidad de precios ## imgfabri Imagen del fabricante ## servconj Servicio conjunto ## imgfvent Imagen de fuerza de ventas ## calidadp Calidad de producto ## fidelida Porcentaje de compra a HATCO ## satisfac Satisfacción global ## nfidelid Nivel de compra a HATCO ## nsatisfa Nivel de satisfacción "],["19-1-normalidad.html", "19.1 Normalidad", " 19.1 Normalidad Queremos hacer un estudio inferencial de la variable satisfac (satisfacción global). Lo primero que vamos a hacer es comprobar si, visualmente, los datos parecen razonablemente simétricos y si se pueden ajustar por una distribución normal hist(hatco$satisfac) qqnorm(hatco$satisfac) shapiro.test(hatco$satisfac) ## ## Shapiro-Wilk normality test ## ## data: hatco$satisfac ## W = 0.97608, p-value = 0.06813 "],["19-2-contrastes.html", "19.2 Contrastes", " 19.2 Contrastes 19.2.1 Una muestra Obtenemos un intervalo de confianza de satisfac t.test(hatco$satisfac) # with(hatco, t.test(satisfac)) ## ## One Sample t-test ## ## data: hatco$satisfac ## t = 55.301, df = 98, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 4.603406 4.946089 ## sample estimates: ## mean of x ## 4.774747 Contrastamos si es razonable suponer que la media es 5 t.test(hatco$satisfac, mu=5) ## ## One Sample t-test ## ## data: hatco$satisfac ## t = -2.6089, df = 98, p-value = 0.01051 ## alternative hypothesis: true mean is not equal to 5 ## 95 percent confidence interval: ## 4.603406 4.946089 ## sample estimates: ## mean of x ## 4.774747 Utilizando una confianza del 99% t.test(hatco$satisfac, mu=5, conf.level=0.99) ## ## One Sample t-test ## ## data: hatco$satisfac ## t = -2.6089, df = 98, p-value = 0.01051 ## alternative hypothesis: true mean is not equal to 5 ## 99 percent confidence interval: ## 4.547935 5.001560 ## sample estimates: ## mean of x ## 4.774747 Veamos si podemos afirmar que la media es menor que 5 t.test(hatco$satisfac, mu=5, alternative = &#39;less&#39;) ## ## One Sample t-test ## ## data: hatco$satisfac ## t = -2.6089, df = 98, p-value = 0.005253 ## alternative hypothesis: true mean is less than 5 ## 95 percent confidence interval: ## -Inf 4.918122 ## sample estimates: ## mean of x ## 4.774747 ¿Y mayor que 4.65? t.test(hatco$satisfac, mu=4.65, alternative = &#39;greater&#39;) ## ## One Sample t-test ## ## data: hatco$satisfac ## t = 1.4448, df = 98, p-value = 0.07585 ## alternative hypothesis: true mean is greater than 4.65 ## 95 percent confidence interval: ## 4.631373 Inf ## sample estimates: ## mean of x ## 4.774747 El test de los rangos con signo de Wilcoxon es un contraste no paramétrico (exige que la distribución sea simétrica) que se puede utilizar como alternativa al contraste t de Student with(hatco, wilcox.test(satisfac, mu=5)) ## ## Wilcoxon signed rank test with continuity correction ## ## data: satisfac ## V = 1574, p-value = 0.01303 ## alternative hypothesis: true location is not equal to 5 19.2.2 Dos muestras Disponemos de dos muestras independientes, el porcentaje de compra en las empresas con nivel de satisfacción bajo y alto, y asumimos que las varianzas son iguales t.test(fidelida ~ nsatisfa, data = hatco, var.equal=TRUE) ## ## Two Sample t-test ## ## data: fidelida by nsatisfa ## t = -6.5833, df = 97, p-value = 2.363e-09 ## alternative hypothesis: true difference in means between group bajo and group alto is not equal to 0 ## 95 percent confidence interval: ## -12.915013 -6.931653 ## sample estimates: ## mean in group bajo mean in group alto ## 41.72778 51.65111 Si no se asume igualdad de varianzas, se calcula la variante Welch del test t t.test(fidelida ~ nsatisfa, data = hatco) ## ## Welch Two Sample t-test ## ## data: fidelida by nsatisfa ## t = -6.6901, df = 96.995, p-value = 1.437e-09 ## alternative hypothesis: true difference in means between group bajo and group alto is not equal to 0 ## 95 percent confidence interval: ## -12.86727 -6.97940 ## sample estimates: ## mean in group bajo mean in group alto ## 41.72778 51.65111 Comparemos visualmente las varianzas boxplot(fidelida ~ nsatisfa, data = hatco) La comparación de las varianzas puede hacerse con el test F var.test(fidelida ~ nsatisfa, data = hatco) ## ## F test to compare two variances ## ## data: fidelida by nsatisfa ## F = 1.4248, num df = 53, denom df = 44, p-value = 0.2292 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.797925 2.505462 ## sample estimates: ## ratio of variances ## 1.424804 Una alternativa no paramétrica bartlett.test(fidelida ~ nsatisfa, data = hatco) ## ## Bartlett test of homogeneity of variances ## ## data: fidelida by nsatisfa ## Bartlett&#39;s K-squared = 1.4675, df = 1, p-value = 0.2257 También puede utilizarse el test de Wilcoxon como alternativa al test t wilcox.test(fidelida ~ nsatisfa, data = hatco) ## ## Wilcoxon rank sum test with continuity correction ## ## data: fidelida by nsatisfa ## W = 430.5, p-value = 3.504e-08 ## alternative hypothesis: true location shift is not equal to 0 Si disponemos de datos apareados, por ejemplo nivel de precios e imagen de fuerza de ventas with(hatco, t.test(precio, imgfvent, paired = TRUE)) ## ## Paired t-test ## ## data: precio and imgfvent ## t = -2.2347, df = 98, p-value = 0.02771 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.55114759 -0.03269079 ## sample estimates: ## mean of the differences ## -0.2919192 Y la correspondiente alternativa no paramétrica with(hatco, wilcox.test(precio, imgfvent, paired = TRUE)) ## ## Wilcoxon signed rank test with continuity correction ## ## data: precio and imgfvent ## V = 1789.5, p-value = 0.02431 ## alternative hypothesis: true location shift is not equal to 0 "],["19-3-regresión-y-correlación.html", "19.3 Regresión y correlación", " 19.3 Regresión y correlación 19.3.1 Regresión lineal simple Utilizando la función lm (modelo lineal) se puede llevar a cabo, entre otras muchas cosas, una regresión lineal simple lm(satisfac ~ fidelida, data = hatco) ## ## Call: ## lm(formula = satisfac ~ fidelida, data = hatco) ## ## Coefficients: ## (Intercept) fidelida ## 1.6074 0.0685 modelo &lt;- lm(satisfac ~ fidelida, data = hatco, na.action=na.exclude) summary(modelo) ## ## Call: ## lm(formula = satisfac ~ fidelida, data = hatco, na.action = na.exclude) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.47492 -0.37341 0.09358 0.38258 1.25258 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.607399 0.322436 4.985 2.71e-06 *** ## fidelida 0.068500 0.006848 10.003 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6058 on 97 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.5078, Adjusted R-squared: 0.5027 ## F-statistic: 100.1 on 1 and 97 DF, p-value: &lt; 2.2e-16 plot(hatco$fidelida, hatco$satisfac) # Cuidado con el orden de las variables # with(hatco, plot(fidelida, satisfac)) # Alternativa empleando with # plot(satisfac ~ fidelida, data = hatco) # Alternativa empleando fórmulas abline(modelo) Valores ajustados fitted(modelo) ## 1 2 3 4 5 6 7 8 ## 3.799412 4.552917 4.895419 3.799412 5.580423 4.689918 4.758418 4.621417 ## 9 10 11 12 13 14 15 16 ## 5.922925 5.306421 3.799412 4.826919 4.278915 4.210415 5.306421 4.963919 ## 17 18 19 20 21 22 23 24 ## 4.210415 4.347416 5.306421 5.374922 4.415916 4.004913 5.374922 4.073414 ## 25 26 27 28 29 30 31 32 ## 4.963919 4.963919 4.073414 5.306421 4.963919 4.758418 4.552917 5.237921 ## 33 34 35 36 37 38 39 40 ## 5.717424 4.847469 4.004913 4.278915 4.621417 4.758418 3.593911 3.525410 ## 41 42 43 44 45 46 47 48 ## 4.347416 5.580423 5.237921 4.895419 4.210415 5.306421 5.374922 4.552917 ## 49 50 51 52 53 54 55 56 ## 5.511923 5.237921 4.415916 5.237921 5.032420 3.799412 4.278915 4.826919 ## 57 58 59 60 61 62 63 64 ## 5.854425 6.059926 4.758418 5.032420 5.306421 5.717424 4.826919 4.073414 ## 65 66 67 68 69 70 71 72 ## 4.347416 4.689918 5.648924 4.758418 5.580423 4.963919 5.032420 5.374922 ## 73 74 75 76 77 78 79 80 ## 5.100920 5.717424 4.415916 4.963919 4.484416 4.826919 4.278915 5.443422 ## 81 82 83 84 85 86 87 88 ## 5.648924 4.847469 4.415916 4.141914 5.237921 4.552917 5.100920 4.073414 ## 89 90 91 92 93 94 95 96 ## 3.936413 5.717424 4.963919 4.278915 4.552917 4.073414 3.730912 3.319909 ## 97 98 99 100 ## 5.717424 4.210415 4.484416 NA Residuos head(resid(modelo)) ## 1 2 3 4 5 6 ## 0.4005878 -0.2529168 0.3045811 0.1005878 1.2195769 -0.2899177 qqnorm(resid(modelo)) shapiro.test(resid(modelo)) ## ## Shapiro-Wilk normality test ## ## data: resid(modelo) ## W = 0.98515, p-value = 0.3325 plot(hatco$fidelida, hatco$satisfac) abline(modelo) # segments(hatco$fidelida, fitted(modelo), hatco$fidelida, hatco$satisfac) with(hatco, segments(fidelida, fitted(modelo), fidelida, satisfac)) plot(fitted(modelo), resid(modelo)) Banda de confianza predict(modelo, interval=&#39;confidence&#39;) ## fit lwr upr ## 1 3.799412 3.571263 4.027561 ## 2 4.552917 4.424306 4.681528 ## 3 4.895419 4.772225 5.018613 ## 4 3.799412 3.571263 4.027561 ## 5 5.580423 5.380031 5.780815 ## 6 4.689918 4.567906 4.811929 ## 7 4.758418 4.637529 4.879307 ## 8 4.621417 4.496801 4.746033 ## 9 5.922925 5.665048 6.180803 ## 10 5.306421 5.146011 5.466832 ## 11 3.799412 3.571263 4.027561 ## 12 4.826919 4.705631 4.948206 ## 13 4.278915 4.123089 4.434741 ## 14 4.210415 4.045670 4.375159 ## 15 5.306421 5.146011 5.466832 ## 16 4.963919 4.837379 5.090459 ## 17 4.210415 4.045670 4.375159 ## 18 4.347416 4.199793 4.495038 ## 19 5.306421 5.146011 5.466832 ## 20 5.374922 5.205264 5.544580 ## 21 4.415916 4.275658 4.556174 ## 22 4.004913 3.810147 4.199680 ## 23 5.374922 5.205264 5.544580 ## 24 4.073414 3.889113 4.257714 ## 25 4.963919 4.837379 5.090459 ## 26 4.963919 4.837379 5.090459 ## 27 4.073414 3.889113 4.257714 ## 28 5.306421 5.146011 5.466832 ## 29 4.963919 4.837379 5.090459 ## 30 4.758418 4.637529 4.879307 ## 31 4.552917 4.424306 4.681528 ## 32 5.237921 5.086103 5.389740 ## 33 5.717424 5.494745 5.940103 ## 34 4.847469 4.725765 4.969172 ## 35 4.004913 3.810147 4.199680 ## 36 4.278915 4.123089 4.434741 ## 37 4.621417 4.496801 4.746033 ## 38 4.758418 4.637529 4.879307 ## 39 3.593911 3.330292 3.857530 ## 40 3.525410 3.249642 3.801179 ## 41 4.347416 4.199793 4.495038 ## 42 5.580423 5.380031 5.780815 ## 43 5.237921 5.086103 5.389740 ## 44 4.895419 4.772225 5.018613 ## 45 4.210415 4.045670 4.375159 ## 46 5.306421 5.146011 5.466832 ## 47 5.374922 5.205264 5.544580 ## 48 4.552917 4.424306 4.681528 ## 49 5.511923 5.322196 5.701650 ## 50 5.237921 5.086103 5.389740 ## 51 4.415916 4.275658 4.556174 ## 52 5.237921 5.086103 5.389740 ## 53 5.032420 4.901205 5.163635 ## 54 3.799412 3.571263 4.027561 ## 55 4.278915 4.123089 4.434741 ## 56 4.826919 4.705631 4.948206 ## 57 5.854425 5.608471 6.100378 ## 58 6.059926 5.777748 6.342104 ## 59 4.758418 4.637529 4.879307 ## 60 5.032420 4.901205 5.163635 ## 61 5.306421 5.146011 5.466832 ## 62 5.717424 5.494745 5.940103 ## 63 4.826919 4.705631 4.948206 ## 64 4.073414 3.889113 4.257714 ## 65 4.347416 4.199793 4.495038 ## 66 4.689918 4.567906 4.811929 ## 67 5.648924 5.437531 5.860316 ## 68 4.758418 4.637529 4.879307 ## 69 5.580423 5.380031 5.780815 ## 70 4.963919 4.837379 5.090459 ## 71 5.032420 4.901205 5.163635 ## 72 5.374922 5.205264 5.544580 ## 73 5.100920 4.963837 5.238003 ## 74 5.717424 5.494745 5.940103 ## 75 4.415916 4.275658 4.556174 ## 76 4.963919 4.837379 5.090459 ## 77 4.484416 4.350544 4.618289 ## 78 4.826919 4.705631 4.948206 ## 79 4.278915 4.123089 4.434741 ## 80 5.443422 5.263964 5.622881 ## 81 5.648924 5.437531 5.860316 ## 82 4.847469 4.725765 4.969172 ## 83 4.415916 4.275658 4.556174 ## 84 4.141914 3.967647 4.316181 ## 85 5.237921 5.086103 5.389740 ## 86 4.552917 4.424306 4.681528 ## 87 5.100920 4.963837 5.238003 ## 88 4.073414 3.889113 4.257714 ## 89 3.936413 3.730815 4.142011 ## 90 5.717424 5.494745 5.940103 ## 91 4.963919 4.837379 5.090459 ## 92 4.278915 4.123089 4.434741 ## 93 4.552917 4.424306 4.681528 ## 94 4.073414 3.889113 4.257714 ## 95 3.730912 3.491126 3.970697 ## 96 3.319909 3.006980 3.632839 ## 97 5.717424 5.494745 5.940103 ## 98 4.210415 4.045670 4.375159 ## 99 4.484416 4.350544 4.618289 ## 100 NA NA NA Banda de predicción head(predict(modelo, interval=&#39;prediction&#39;)) ## fit lwr upr ## 1 3.799412 2.575563 5.023261 ## 2 4.552917 3.343663 5.762171 ## 3 4.895419 3.686729 6.104109 ## 4 3.799412 2.575563 5.023261 ## 5 5.580423 4.361444 6.799403 ## 6 4.689918 3.481348 5.898487 Representación gráfica de las bandas bandas.frame &lt;- data.frame(fidelida=24:66) bc &lt;- predict(modelo, interval = &#39;confidence&#39;, newdata = bandas.frame) bp &lt;- predict(modelo, interval = &#39;prediction&#39;, newdata = bandas.frame) plot(hatco$fidelida, hatco$satisfac, ylim = range(hatco$satisfac, bp, na.rm = TRUE)) matlines(bandas.frame$fidelida, bc, lty=c(1,2,2), col=&#39;black&#39;) matlines(bandas.frame$fidelida, bp, lty=c(0,3,3), col=&#39;red&#39;) 19.3.2 Correlación Coeficiente de correlación de Pearson cor(hatco$fidelida, hatco$satisfac, use=&#39;complete.obs&#39;) ## [1] 0.712581 cor(hatco[,6:14], use=&#39;complete.obs&#39;) ## velocida precio flexprec imgfabri servconj ## velocida 1.00000000 -0.35439461 0.51879732 0.04885481 0.60908594 ## precio -0.35439461 1.00000000 -0.48550163 0.27150666 0.51134698 ## flexprec 0.51879732 -0.48550163 1.00000000 -0.11472112 0.07496499 ## imgfabri 0.04885481 0.27150666 -0.11472112 1.00000000 0.29800272 ## servconj 0.60908594 0.51134698 0.07496499 0.29800272 1.00000000 ## imgfvent 0.08084452 0.18873090 -0.03801323 0.79015164 0.24641510 ## calidadp -0.48984768 0.46822563 -0.44542562 0.19904126 -0.06152068 ## fidelida 0.67428681 0.07682487 0.57807750 0.22442574 0.69802972 ## satisfac 0.64981476 0.02636286 0.53057615 0.47553688 0.63054720 ## imgfvent calidadp fidelida satisfac ## velocida 0.08084452 -0.48984768 0.67428681 0.64981476 ## precio 0.18873090 0.46822563 0.07682487 0.02636286 ## flexprec -0.03801323 -0.44542562 0.57807750 0.53057615 ## imgfabri 0.79015164 0.19904126 0.22442574 0.47553688 ## servconj 0.24641510 -0.06152068 0.69802972 0.63054720 ## imgfvent 1.00000000 0.18052945 0.26674626 0.34349253 ## calidadp 0.18052945 1.00000000 -0.20401261 -0.28687427 ## fidelida 0.26674626 -0.20401261 1.00000000 0.71258104 ## satisfac 0.34349253 -0.28687427 0.71258104 1.00000000 cor.test(hatco$fidelida, hatco$satisfac) ## ## Pearson&#39;s product-moment correlation ## ## data: hatco$fidelida and hatco$satisfac ## t = 10.003, df = 97, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.5995024 0.7977691 ## sample estimates: ## cor ## 0.712581 El coeficiente de correlación de Spearman es una variante no paramétrica cor.test(hatco$fidelida, hatco$satisfac, method=&#39;spearman&#39;) ## ## Spearman&#39;s rank correlation rho ## ## data: hatco$fidelida and hatco$satisfac ## S = 46601, p-value &lt; 2.2e-16 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.7118039 "],["19-4-análisis-de-la-varianza.html", "19.4 Análisis de la varianza", " 19.4 Análisis de la varianza 19.4.1 ANOVA con un factor Vamos a estudiar si hay diferencias en las medias de la variable satisfac (satisfacción global) entre los diferentes grupos definidos por nfidelid (nivel de compra), utilizando el procedimiento clásico de análisis de la varianza. Este procedimiento exige normalidad y homocedasticidad. table(hatco$nfidelid) ## ## bajo medio alto ## 3 64 33 tapply(hatco$satisfac, hatco$nfidelid, mean, na.rm = TRUE) ## bajo medio alto ## 3.533333 4.498437 5.443750 La variable explicativa tiene que ser obligatoriamente de tipo factor. Por coherencia con la función (general) lm, la variación entre grupos está etiquetada nfidelid, y la variación dentro de los grupos como Residuals anova(lm(satisfac~nfidelid, data = hatco)) ## Analysis of Variance Table ## ## Response: satisfac ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## nfidelid 2 23.832 11.9158 23.588 4.647e-09 *** ## Residuals 96 48.495 0.5052 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Como alternativa, se puede utilizar la función aov aov(satisfac~nfidelid, data = hatco) ## Call: ## aov(formula = satisfac ~ nfidelid, data = hatco) ## ## Terms: ## nfidelid Residuals ## Sum of Squares 23.83161 48.49526 ## Deg. of Freedom 2 96 ## ## Residual standard error: 0.7107454 ## Estimated effects may be unbalanced ## 1 observation deleted due to missingness summary(aov(satisfac~nfidelid, data = hatco)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## nfidelid 2 23.83 11.916 23.59 4.65e-09 *** ## Residuals 96 48.50 0.505 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## 1 observation deleted due to missingness Comparaciones entre pares de variables pairwise.t.test(hatco$satisfac, hatco$nfidelid) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: hatco$satisfac and hatco$nfidelid ## ## bajo medio ## medio 0.024 - ## alto 4.6e-05 5.5e-08 ## ## P value adjustment method: holm Relajamos la hipótesis de varianzas iguales oneway.test(satisfac~nfidelid, data = hatco) ## ## One-way analysis of means (not assuming equal variances) ## ## data: satisfac and nfidelid ## F = 35.013, num df = 2.0000, denom df = 6.7661, p-value = 0.0002697 Podemos utilizar el test de Bartlett para contrastar la igualdad de varianzas bartlett.test(satisfac~nfidelid, data = hatco) ## ## Bartlett test of homogeneity of variances ## ## data: satisfac by nfidelid ## Bartlett&#39;s K-squared = 1.4922, df = 2, p-value = 0.4742 Representación gráfica medias &lt;- tapply(hatco$satisfac, hatco$nfidelid, mean, na.rm = TRUE) desviaciones &lt;- tapply(hatco$satisfac, hatco$nfidelid, sd, na.rm = TRUE) n &lt;- tapply(hatco$satisfac[!is.na(hatco$satisfac)], hatco$nfidelid[!is.na(hatco$satisfac)], length) errores &lt;- desviaciones/sqrt(n) stripchart(hatco$satisfac~hatco$nfidelid, method=&#39;jitter&#39;, jit=0.01, pch=18, col=&#39;grey&#39;, vertical = TRUE) arrows(1:3, medias+errores, 1:3, medias-errores, angle=90, code=3, lwd=2, col=&#39;orange&#39;) points(1:3, medias, pch=4, lwd=2, cex=2, col=&#39;orange&#39;) 19.4.2 Test de Kruskal-Wallis Alternativa no paramétrica al análisis de la varianza con un factor kruskal.test(satisfac~nfidelid, data = hatco) ## ## Kruskal-Wallis rank sum test ## ## data: satisfac by nfidelid ## Kruskal-Wallis chi-squared = 31.073, df = 2, p-value = 1.789e-07 "],["20-modelado-de-datos.html", "Capítulo 20 Modelado de datos", " Capítulo 20 Modelado de datos La realidad puede ser muy compleja por lo que es habitual emplear un modelo para tratar de explicarla. Modelos estocásticos (con componente aleatoria). Tienen en cuenta la incertidumbre debida a no disponer de la suficiente información sobre las variables que influyen en el fenómeno en estudio. La inferencia estadística proporciona herramientas para ajustar y contrastar la validez del modelo a partir de los datos observados. Sin embargo resultaría muy extraño que la realidad coincida exactamente con un modelo concreto. George Box afirmó en su famoso aforismo: En esencia, todos los modelos son falsos, pero algunos son útiles. El objetivo de un modelo es disponer de una aproximación simple de la realidad que sea útil. "],["20-1-modelos-de-regresión.html", "20.1 Modelos de regresión", " 20.1 Modelos de regresión Nos centraremos en los modelos de regresión: \\[Y=f(X_{1},\\cdots,X_{p})+\\varepsilon\\] donde: \\(Y\\equiv\\) variable respuesta (o dependiente). \\(\\left( X_{1},\\cdots,X_{p}\\right) \\equiv\\) variables explicativas (independientes, o covariables). \\(\\varepsilon\\equiv\\) error aleatorio. 20.1.1 Herramientas disponibles en R R dispone de múltiples herramientas para trabajar con modelos de este tipo. Algunas de las funciones y paquetes disponibles se muestran a continuación: Modelos paramétricos: Modelos lineales: Regresión lineal: lm() (aov(), lme(), biglm, ). Regresión lineal robusta: MASS::rlm(). Métodos de regularización (Ridge regression, Lasso): glmnet,  Modelos lineales generalizados: glm() (bigglm, ). Modelos paramétricos no lineales: nls() (nlme, ). Modelos no paramétricos: Regresión local (métodos de suavizado): loess(), KernSmooth, sm,  Modelos aditivos generalizados (GAM): gam, mgcv,  Arboles de decisión (Random Forest, Boosting): rpart, randomForest, xgboost,  Redes neuronales,  Desde el punto de vista de la programación, con todos estos modelos se trabaja de una forma muy similar en R. "],["20-2-fórmulas.html", "20.2 Fórmulas", " 20.2 Fórmulas En R para especificar un modelo estadístico (realmente una familia) se suelen emplear fórmulas (también para generar gráficos). Son de la forma: respuesta ~ modelo modelo especifica los términos mediante operadores (tienen un significado especial en este contexto): Operador Descripción a+b incluye a y b (efectos principales) -b excluye b del modelo a:b interacción de a y b \\ b %in% a efectos de b anidados en a (a:b) \\ a/b = a + b %in% a = a + a:b a*b = a+b+a:b efectos principales más interacciones ^n interacciones hasta nivel n ((a+b)^2 = a+b+a:b) poly(a, n) polinomios de a hasta grado n 1 término constante . todas las variables disponibles o modelo actual en actualizaciones Para realizar operaciones aritméticas (que incluyan +, -, *, ^, 1, ) es necesario aislar la operación dentro una función (e.g. log(abs(x) + 1)). Por ejemplo, para realizar un ajuste cuadrático se debería utilizar y ~ x + I(x^2), ya que y ~ x + x^2 = y ~ x (la interacción x:x = x). I() función identidad. "],["20-3-ejemplo-regresión-lineal-simple.html", "20.3 Ejemplo: regresión lineal simple", " 20.3 Ejemplo: regresión lineal simple Introducido en descriptiva y con referencias al tema siguiente "],["21-modelos-lineales.html", "Capítulo 21 Modelos lineales", " Capítulo 21 Modelos lineales Suponen que la función de regresión es lineal: \\[Y=\\beta_{0}+\\beta_{1}X_{1}+\\beta_{2}X_{2}+\\cdots+\\beta_{p}X_{p}+\\varepsilon\\] El efecto de las variables explicativas sobre la respuesta es simple (proporcional a su valor). "],["21-1-ejemplo.html", "21.1 Ejemplo", " 21.1 Ejemplo El fichero hatco.RData contiene observaciones de clientes de la compañía de distribución industrial (Compañía Hair, Anderson y Tatham). Las variables se pueden clasificar en tres grupos: load(&#39;datos/hatco.RData&#39;) as.data.frame(attr(hatco, &quot;variable.labels&quot;)) ## attr(hatco, &quot;variable.labels&quot;) ## empresa Empresa ## tamano Tamaño de la empresa ## adquisic Estructura de adquisición ## tindustr Tipo de industria ## tsitcomp Tipo de situación de compra ## velocida Velocidad de entrega ## precio Nivel de precios ## flexprec Flexibilidad de precios ## imgfabri Imagen del fabricante ## servconj Servicio conjunto ## imgfvent Imagen de fuerza de ventas ## calidadp Calidad de producto ## fidelida Porcentaje de compra a HATCO ## satisfac Satisfacción global ## nfidelid Nivel de compra a HATCO ## nsatisfa Nivel de satisfacción Consideraremos como respuesta la variable fidelida y como variables explicativas el resto de variables continuas menos satisfac. datos &lt;- hatco[, 6:13] # Nota: realmente no copia el objeto... plot(datos) # cor(datos, use = &quot;complete&quot;) # Por defecto 8 decimales... print(cor(datos, use = &quot;complete&quot;), digits = 2) ## velocida precio flexprec imgfabri servconj imgfvent calidadp fidelida ## velocida 1.000 -0.354 0.519 0.049 0.609 0.081 -0.490 0.674 ## precio -0.354 1.000 -0.486 0.272 0.511 0.189 0.468 0.077 ## flexprec 0.519 -0.486 1.000 -0.115 0.075 -0.038 -0.445 0.578 ## imgfabri 0.049 0.272 -0.115 1.000 0.298 0.790 0.199 0.224 ## servconj 0.609 0.511 0.075 0.298 1.000 0.246 -0.062 0.698 ## imgfvent 0.081 0.189 -0.038 0.790 0.246 1.000 0.181 0.267 ## calidadp -0.490 0.468 -0.445 0.199 -0.062 0.181 1.000 -0.204 ## fidelida 0.674 0.077 0.578 0.224 0.698 0.267 -0.204 1.000 "],["21-2-ajuste-función-lm.html", "21.2 Ajuste: función lm", " 21.2 Ajuste: función lm Para el ajuste (estimación de los parámetros) de un modelo lineal a un conjunto de datos (por mínimos cuadrados) se emplea la función lm: ajuste &lt;- lm(formula, datos, seleccion, pesos, na.action) formula fórmula que especifica el modelo. datos data.frame opcional con las variables de la formula. seleccion especificación opcional de un subconjunto de observaciones. pesos vector opcional de pesos (WLS). na.action opción para manejar los datos faltantes (na.omit). modelo &lt;- lm(fidelida ~ servconj, datos) modelo ## ## Call: ## lm(formula = fidelida ~ servconj, data = datos) ## ## Coefficients: ## (Intercept) servconj ## 21.98 8.30 Al imprimir el ajuste resultante se muestra un pequeño resumen del ajuste (aunque el objeto que contiene los resultados es una lista). Para obtener un resumen más completo se puede utilizar la función summary(). summary(modelo) ## ## Call: ## lm(formula = fidelida ~ servconj, data = datos) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.1956 -4.0655 0.2944 4.5945 11.9744 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 21.9754 2.6086 8.424 3.34e-13 *** ## servconj 8.3000 0.8645 9.601 9.76e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.432 on 97 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.4872, Adjusted R-squared: 0.482 ## F-statistic: 92.17 on 1 and 97 DF, p-value: 9.765e-16 plot(fidelida ~ servconj, datos) abline(modelo) 21.2.1 Extracción de información Para la extracción de información se pueden acceder a los componentes del modelo ajustado o emplear funciones (genéricas). Algunas de las más utilizadas son las siguientes: Función Descripción fitted valores ajustados coef coeficientes estimados (y errores estándar) confint intervalos de confianza para los coeficientes residuals residuos plot gráficos de diagnóstico termplot gráfico de efectos parciales anova calcula tablas de análisis de varianza (también permite comparar modelos) predict calcula predicciones para nuevos datos Ejemplo: modelo2 &lt;- lm(fidelida ~ servconj + flexprec, data = hatco) summary(modelo2) ## ## Call: ## lm(formula = fidelida ~ servconj + flexprec, data = hatco) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.2549 -2.2850 0.3411 3.3260 7.0853 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.4617 2.9734 -1.164 0.247 ## servconj 7.8287 0.5897 13.276 &lt;2e-16 *** ## flexprec 3.4017 0.3191 10.661 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.375 on 96 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.7652, Adjusted R-squared: 0.7603 ## F-statistic: 156.4 on 2 and 96 DF, p-value: &lt; 2.2e-16 confint(modelo2) ## 2.5 % 97.5 % ## (Intercept) -9.363813 2.440344 ## servconj 6.658219 8.999274 ## flexprec 2.768333 4.035030 anova(modelo2) ## Analysis of Variance Table ## ## Response: fidelida ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## servconj 1 3813.6 3813.6 199.23 &lt; 2.2e-16 *** ## flexprec 1 2175.6 2175.6 113.66 &lt; 2.2e-16 *** ## Residuals 96 1837.6 19.1 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # anova(modelo2, modelo) # termplot(modelo2, partial.resid = TRUE) Muchas de estas funciones genéricas son válidas para otros tipos de modelos (glm, ). Algunas funciones como summary() devuelven información adicional: res &lt;- summary(modelo2) names(res) ## [1] &quot;call&quot; &quot;terms&quot; &quot;residuals&quot; &quot;coefficients&quot; ## [5] &quot;aliased&quot; &quot;sigma&quot; &quot;df&quot; &quot;r.squared&quot; ## [9] &quot;adj.r.squared&quot; &quot;fstatistic&quot; &quot;cov.unscaled&quot; &quot;na.action&quot; res$sigma ## [1] 4.375074 res$adj.r.squared ## [1] 0.7603292 "],["21-3-predicción.html", "21.3 Predicción", " 21.3 Predicción Para calcular predicciones (estimaciones de la media condicionada) se puede emplear la función predict() (ejecutar help(predict.lm) para ver todas las opciones disponibles). Por defecto obtiene las predicciones correspondientes a las observaciones (modelo$fitted.values). Para otros casos hay que emplear el argumento newdata: data.frame con los valores de (todas) las covariables, sus nombres deben coincidir con los originales. Ejemplo: valores &lt;- 0:5 pred &lt;- predict(modelo, newdata = data.frame(servconj = valores)) pred ## 1 2 3 4 5 6 ## 21.97544 30.27548 38.57552 46.87556 55.17560 63.47564 plot(fidelida ~ servconj, datos) lines(valores, pred) Esta función también permite obtener intervalos de confianza y de predicción: valores &lt;- seq(0, 5, len = 100) newdata &lt;- data.frame(servconj = valores) pred &lt;- predict(modelo, newdata = newdata, interval = c(&quot;confidence&quot;)) head(pred) ## fit lwr upr ## 1 21.97544 16.79816 27.15272 ## 2 22.39463 17.30126 27.48800 ## 3 22.81383 17.80427 27.82338 ## 4 23.23302 18.30718 28.15886 ## 5 23.65221 18.80999 28.49444 ## 6 24.07141 19.31269 28.83013 plot(fidelida ~ servconj, datos) matlines(valores, pred, lty = c(1, 2, 2), col = 1) pred2 &lt;- predict(modelo, newdata = newdata, interval = c(&quot;prediction&quot;)) matlines(valores, pred2[, -1], lty = 3, col = 1) legend(&quot;topleft&quot;, c(&quot;Ajuste&quot;, &quot;Int. confianza&quot;, &quot;Int. predicción&quot;), lty = c(1, 2, 3)) "],["21-4-selección-de-variables-explicativas.html", "21.4 Selección de variables explicativas", " 21.4 Selección de variables explicativas Cuando se dispone de un conjunto grande de posibles variables explicativas suele ser especialmente importante determinar cuales de estas deberían ser incluidas en el modelo de regresión. Si alguna de las variables no contiene información relevante sobre la respuesta no se debería incluir (se simplificaría la interpretación del modelo, aumentaría la precisión de la estimación y se evitarían problemas como la multicolinealidad). Se trataría entonces de conseguir un buen ajuste con el menor número de variables explicativas posible. Para actualizar un modelo (p.e. eliminando o añadiendo variables) se puede emplear la función update: modelo.completo &lt;- lm(fidelida ~ . , data = datos) summary(modelo.completo) ## ## Call: ## lm(formula = fidelida ~ ., data = datos) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.3351 -2.0733 0.5224 2.9218 6.7106 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -9.5935 4.8213 -1.990 0.0496 * ## velocida -0.6023 1.9590 -0.307 0.7592 ## precio -1.0771 2.0283 -0.531 0.5967 ## flexprec 3.4616 0.3997 8.660 1.62e-13 *** ## imgfabri -0.1735 0.6472 -0.268 0.7892 ## servconj 9.0919 3.8023 2.391 0.0189 * ## imgfvent 1.5596 0.9221 1.691 0.0942 . ## calidadp 0.4874 0.3451 1.412 0.1613 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.281 on 91 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.7869, Adjusted R-squared: 0.7705 ## F-statistic: 48 on 7 and 91 DF, p-value: &lt; 2.2e-16 modelo.reducido &lt;- update(modelo.completo, . ~ . - imgfabri) summary(modelo.reducido) ## ## Call: ## lm(formula = fidelida ~ velocida + precio + flexprec + servconj + ## imgfvent + calidadp, data = datos) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.2195 -2.0022 0.4724 2.9514 6.8328 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -9.9900 4.5656 -2.188 0.0312 * ## velocida -0.5207 1.9254 -0.270 0.7874 ## precio -1.0017 1.9986 -0.501 0.6174 ## flexprec 3.4709 0.3962 8.761 9.23e-14 *** ## servconj 8.9111 3.7230 2.394 0.0187 * ## imgfvent 1.3699 0.5883 2.329 0.0221 * ## calidadp 0.4844 0.3432 1.411 0.1615 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.26 on 92 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.7867, Adjusted R-squared: 0.7728 ## F-statistic: 56.56 on 6 and 92 DF, p-value: &lt; 2.2e-16 Para obtener el modelo óptimo lo ideal sería evaluar todos los modelos posibles. 21.4.1 Búsqueda exhaustiva La función regsubsets del paquete leaps permite seleccionar los mejores modelos fijando el número de variables explicativas. Por defecto, evalúa todos los modelos posibles con un determinado número de parámetros (variando desde 1 hasta un máximo de nvmax=8) y selecciona el mejor (nbest=1). library(leaps) res &lt;- regsubsets(fidelida ~ . , data = datos) summary(res) ## Subset selection object ## Call: regsubsets.formula(fidelida ~ ., data = datos) ## 7 Variables (and intercept) ## Forced in Forced out ## velocida FALSE FALSE ## precio FALSE FALSE ## flexprec FALSE FALSE ## imgfabri FALSE FALSE ## servconj FALSE FALSE ## imgfvent FALSE FALSE ## calidadp FALSE FALSE ## 1 subsets of each size up to 7 ## Selection Algorithm: exhaustive ## velocida precio flexprec imgfabri servconj imgfvent calidadp ## 1 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; ## 3 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; ## 4 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 5 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 6 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 7 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; # names(summary(res)) Al representar el resultado se obtiene un gráfico con los mejores modelos ordenados según el criterio determinado por el argumento scale = c(\"bic\", \"Cp\", \"adjr2\", \"r2\"). Por ejemplo, en este caso, empleando el coeficiente de determinación ajustado, obtendríamos: plot(res, scale = &quot;adjr2&quot;) En este caso (considerando que una mejora del 2% no es significativa), el modelo resultante sería: lm(fidelida ~ servconj + flexprec, data = hatco) ## ## Call: ## lm(formula = fidelida ~ servconj + flexprec, data = hatco) ## ## Coefficients: ## (Intercept) servconj flexprec ## -3.462 7.829 3.402 Notas: Si se emplea alguno de los criterios habituales, el mejor modelo con un determinado número de variables no depende del criterio empleado. Pero estos criterios pueden diferir al comparar modelos con distinto número de variables explicativas. Si el número de variables explicativas es grande, en lugar de emplear una búsqueda exhaustiva se puede emplear un criterio por pasos, mediante el argumento method = c(\"backward\", \"forward\", \"seqrep\"), pero puede ser recomendable emplear el paquete MASS para obtener directamente el modelo final. 21.4.2 Selección por pasos Si el número de variables es grande (no sería práctico evaluar todas las posibilidades) se suele utilizar alguno (o varios) de los siguientes métodos: Selección progresiva (forward): Se parte de una situación en la que no hay ninguna variable y en cada paso se incluye una aplicando un criterio de entrada (hasta que ninguna de las restantes lo verifican). Eliminación progresiva (backward): Se parte del modelo con todas las variables y en cada paso se elimina una aplicando un criterio de salida (hasta que ninguna de las incluidas lo verifican). Regresión paso a paso (stepwise): El más utilizado, se combina un criterio de entrada y uno de salida. Normalmente se parte sin ninguna variable y en cada paso puede haber una inclusión y una exclusión (forward/backward). La función stepAIC del paquete MASS permite seleccionar el modelo por pasos, hacia delante o hacia atrás según criterio AIC o BIC (también esta disponible una función step del paquete base stats con menos opciones). La función stepwise del paquete RcmdrMisc es una interfaz de stepAIC que facilita su uso: library(MASS) library(RcmdrMisc) modelo &lt;- stepwise(modelo.completo, direction = &quot;forward/backward&quot;, criterion = &quot;BIC&quot;) ## ## Direction: forward/backward ## Criterion: BIC ## ## Start: AIC=437.24 ## fidelida ~ 1 ## ## Df Sum of Sq RSS AIC ## + servconj 1 3813.6 4013.2 375.71 ## + velocida 1 3558.5 4268.2 381.81 ## + flexprec 1 2615.5 5211.3 401.57 ## + imgfvent 1 556.9 7269.9 434.53 ## + imgfabri 1 394.2 7432.5 436.72 ## &lt;none&gt; 7826.8 437.24 ## + calidadp 1 325.8 7501.0 437.63 ## + precio 1 46.2 7780.6 441.25 ## ## Step: AIC=375.71 ## fidelida ~ servconj ## ## Df Sum of Sq RSS AIC ## + flexprec 1 2175.6 1837.6 302.97 ## + precio 1 831.5 3181.7 357.32 ## + velocida 1 772.3 3240.9 359.15 ## + calidadp 1 203.8 3809.4 375.15 ## &lt;none&gt; 4013.2 375.71 ## + imgfvent 1 74.8 3938.4 378.44 ## + imgfabri 1 2.3 4010.9 380.25 ## - servconj 1 3813.6 7826.8 437.24 ## ## Step: AIC=302.97 ## fidelida ~ servconj + flexprec ## ## Df Sum of Sq RSS AIC ## + imgfvent 1 129.8 1707.7 300.31 ## &lt;none&gt; 1837.6 302.97 ## + imgfabri 1 69.3 1768.3 303.76 ## + calidadp 1 50.7 1786.9 304.80 ## + precio 1 0.2 1837.4 307.56 ## + velocida 1 0.0 1837.5 307.57 ## - flexprec 1 2175.6 4013.2 375.71 ## - servconj 1 3373.7 5211.3 401.57 ## ## Step: AIC=300.31 ## fidelida ~ servconj + flexprec + imgfvent ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 1707.7 300.31 ## - imgfvent 1 129.82 1837.6 302.97 ## + calidadp 1 24.70 1683.0 303.47 ## + precio 1 0.96 1706.8 304.85 ## + imgfabri 1 0.66 1707.1 304.87 ## + velocida 1 0.41 1707.3 304.88 ## - flexprec 1 2230.67 3938.4 378.44 ## - servconj 1 2850.14 4557.9 392.91 summary(modelo) ## ## Call: ## lm(formula = fidelida ~ servconj + flexprec + imgfvent, data = datos) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.9301 -2.1395 0.0695 2.9632 7.4286 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -6.7761 3.1343 -2.162 0.0331 * ## servconj 7.4320 0.5902 12.592 &lt;2e-16 *** ## flexprec 3.4503 0.3097 11.140 &lt;2e-16 *** ## imgfvent 1.5369 0.5719 2.687 0.0085 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.24 on 95 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.7818, Adjusted R-squared: 0.7749 ## F-statistic: 113.5 on 3 and 95 DF, p-value: &lt; 2.2e-16 Los métodos disponibles son \"backward/forward\", \"forward/backward\", \"backward\" y \"forward\". Cuando el número de variables explicativas es muy grande (o si el tamaño de la muestra es pequeño en comparación) pueden aparecer problemas al emplear los métodos anteriores (incluso pueden no ser aplicables). Una alternativa son los métodos de regularización (Ridge regression, Lasso) disponibles en el paquete glmnet. "],["21-5-regresión-con-variables-categóricas.html", "21.5 Regresión con variables categóricas", " 21.5 Regresión con variables categóricas La función lm() admite también variables categóricas (factores), lo que equivaldría a modelos de análisis de la varianza o de la covarianza. Como ejemplo, en el resto del tema emplearemos los datos de empleados: load(&quot;datos/empleados.RData&quot;) datos &lt;- with(empleados, data.frame(lnsal = log(salario), lnsalini = log(salini), catlab, sexo)) Al incluir variables categóricas la función lm() genera las variables indicadoras (variables dummy) que sean necesarias. Por ejemplo, la función model.matrix() construye la denominada matriz de diseño \\(X\\) de un modelo lineal: \\[\\mathbf{Y}=X\\mathbf{\\beta}+\\mathbf{\\varepsilon}\\] En el caso de una variable categórica, por defecto se toma la primera categoría como referencia y se generan variables indicadoras del resto de categorías: X &lt;- model.matrix(lnsal ~ catlab, datos) head(X) ## (Intercept) catlabSeguridad catlabDirectivo ## 1 1 0 1 ## 2 1 0 0 ## 3 1 0 0 ## 4 1 0 0 ## 5 1 0 0 ## 6 1 0 0 En el correspondiente ajuste (análisis de la varianza de un factor): modelo &lt;- lm(lnsal ~ catlab, datos) summary(modelo) ## ## Call: ## lm(formula = lnsal ~ catlab, data = datos) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.58352 -0.15983 -0.01012 0.13277 1.08725 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.20254 0.01280 797.245 &lt; 2e-16 *** ## catlabSeguridad 0.13492 0.04864 2.774 0.00576 ** ## catlabDirectivo 0.82709 0.02952 28.017 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2438 on 471 degrees of freedom ## Multiple R-squared: 0.625, Adjusted R-squared: 0.6234 ## F-statistic: 392.6 on 2 and 471 DF, p-value: &lt; 2.2e-16 el nivel de referencia no tiene asociado un coeficiente (su efecto se corresponde con (Intercept)). Los coeficientes del resto de niveles miden el cambio que se produce en la media al cambiar desde la categoría de referencia (diferencias de efectos respecto al nivel de referencia). Para contrastar el efecto de los factores, es preferible emplear la función anova: modelo &lt;- lm(lnsal ~ catlab + sexo, datos) anova(modelo) ## Analysis of Variance Table ## ## Response: lnsal ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## catlab 2 46.674 23.3372 489.59 &lt; 2.2e-16 *** ## sexo 1 5.596 5.5965 117.41 &lt; 2.2e-16 *** ## Residuals 470 22.404 0.0477 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Notas: Para centrarse en las efectos de los factores, se puede emplear la función aov (analysis of variance; ver también model.tables() y TukeyHSD()). Esta función llama internamente a lm() (utilizando la misma parametrización). Para utilizar distintas parametrizaciones de los efectos se puede emplear el argumento contrasts = c(\"contr.treatment\", \"contr.poly\") (ver help(contrasts)). "],["21-6-interacciones.html", "21.6 Interacciones", " 21.6 Interacciones Al emplear el operador + se considera que los efectos de las covariables son aditivos (independientes): modelo &lt;- lm(lnsal ~ lnsalini + catlab, datos) anova(modelo) ## Analysis of Variance Table ## ## Response: lnsal ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## lnsalini 1 58.668 58.668 1901.993 &lt; 2.2e-16 *** ## catlab 2 1.509 0.755 24.465 7.808e-11 *** ## Residuals 470 14.497 0.031 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 plot(lnsal ~ lnsalini, data = datos, pch = as.numeric(catlab), col = &#39;darkgray&#39;) parest &lt;- coef(modelo) abline(a = parest[1], b = parest[2], lty = 1) abline(a = parest[1] + parest[3], b = parest[2], lty = 2) abline(a = parest[1] + parest[4], b = parest[2], lty = 3) legend(&quot;bottomright&quot;, levels(datos$catlab), pch = 1:3, lty = 1:3) Para especificar que el efecto de una covariable depende de otra (interacción), se pueden emplear los operadores * ó :. modelo2 &lt;- lm(lnsal ~ lnsalini*catlab, datos) summary(modelo2) ## ## Call: ## lm(formula = lnsal ~ lnsalini * catlab, data = datos) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.37440 -0.11335 -0.00524 0.10459 0.97018 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.66865 0.43820 3.808 0.000159 *** ## lnsalini 0.89512 0.04595 19.479 &lt; 2e-16 *** ## catlabSeguridad 8.31808 3.01827 2.756 0.006081 ** ## catlabDirectivo 3.01268 0.79509 3.789 0.000171 *** ## lnsalini:catlabSeguridad -0.85864 0.31392 -2.735 0.006470 ** ## lnsalini:catlabDirectivo -0.27713 0.07924 -3.497 0.000515 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1727 on 468 degrees of freedom ## Multiple R-squared: 0.8131, Adjusted R-squared: 0.8111 ## F-statistic: 407.3 on 5 and 468 DF, p-value: &lt; 2.2e-16 anova(modelo2) ## Analysis of Variance Table ## ## Response: lnsal ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## lnsalini 1 58.668 58.668 1967.6294 &lt; 2.2e-16 *** ## catlab 2 1.509 0.755 25.3090 3.658e-11 *** ## lnsalini:catlab 2 0.543 0.272 9.1097 0.0001315 *** ## Residuals 468 13.954 0.030 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 En este caso las pendientes también varían dependiendo del nivel del factor: plot(lnsal ~ lnsalini, data = datos, pch = as.numeric(catlab), col = &#39;darkgray&#39;) parest &lt;- coef(modelo2) abline(a = parest[1], b = parest[2], lty = 1) abline(a = parest[1] + parest[3], b = parest[2] + parest[5], lty = 2) abline(a = parest[1] + parest[4], b = parest[2] + parest[6], lty = 3) legend(&quot;bottomright&quot;, levels(datos$catlab), pch = 1:3, lty = 1:3) Por ejemplo, empleando la fórmula lnsal ~ lnsalini:catlab se considerarían distintas pendientes pero el mismo término independiente. "],["21-7-diagnosis-del-modelo.html", "21.7 Diagnosis del modelo", " 21.7 Diagnosis del modelo Las conclusiones obtenidas con este método se basan en las hipótesis básicas del modelo: Linealidad. Normalidad (y homogeneidad). Homocedasticidad. Independencia. Ninguna de las variables explicativas es combinación lineal de las demás. Si alguna de estas hipótesis no es cierta, las conclusiones obtenidas pueden no ser fiables, o incluso totalmente erróneas. En el caso de regresión múltiple es de especial interés el fenómeno de la multicolinealidad (o colinearidad) relacionado con la última de estas hipótesis. En esta sección consideraremos como ejemplo el modelo: modelo &lt;- lm(salario ~ salini + expprev, data = empleados) summary(modelo) ## ## Call: ## lm(formula = salario ~ salini + expprev, data = empleados) ## ## Residuals: ## Min 1Q Median 3Q Max ## -32263 -4219 -1332 2673 48571 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3850.71760 900.63287 4.276 2.31e-05 *** ## salini 1.92291 0.04548 42.283 &lt; 2e-16 *** ## expprev -22.44482 3.42240 -6.558 1.44e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7777 on 471 degrees of freedom ## Multiple R-squared: 0.7935, Adjusted R-squared: 0.7926 ## F-statistic: 904.8 on 2 and 471 DF, p-value: &lt; 2.2e-16 21.7.1 Gráficas básicas de diagnóstico Con la función plot se pueden generar gráficos de interés para la diagnosis del modelo: oldpar &lt;- par( mfrow=c(2,2)) plot(modelo) par(oldpar) Por defecto se muestran cuatro gráficos (ver help(plot.lm) para más detalles). El primero (residuos frente a predicciones) permite detectar falta de linealidad o heterocedasticidad (o el efecto de un factor omitido: mala especificación del modelo), lo ideal sería no observar ningún patrón. El segundo gráfico (gráfico QQ), permite diagnosticar la normalidad, los puntos del deberían estar cerca de la diagonal. El tercer gráfico de dispersión-nivel permite detectar heterocedasticidad y ayudar a seleccionar una transformación para corregirla (más adelante, en la sección Alternativas, se tratará este tema), la pendiente de los datos debería ser nula. El último gráfico permite detectar valores atípicos o influyentes. Representa los residuos estandarizados en función del valor de influencia (a priori) o leverage (\\(hii\\) que depende de los valores de las variables explicativas, debería ser \\(&lt; 2(p+1)/2\\)) y señala las observaciones atípicas (residuos fuera de [-2,2]) e influyentes a posteriori (estadístico de Cook &gt;0.5 y &gt;1). Si las conclusiones obtenidas dependen en gran medida de una observación (normalmente atípica), esta se denomina influyente (a posteriori) y debe ser examinada con cuidado por el experimentador. Para recalcular el modelo sin una de las observaciones puede ser útil la función update: # which.max(cooks.distance(modelo)) modelo2 &lt;- update(modelo, data = empleados[-29, ]) Si hay datos atípicos o influyentes, puede ser recomendable emplear regresión lineal robusta, por ejemplo mediante la función rlm del paquete MASS. En el ejemplo anterior, se observa claramente heterogeneidad de varianzas y falta de normalidad. Aparentemente no hay observaciones influyentes (a posteriori) aunque si algún dato atípico. 21.7.2 Gráficos parciales de residuos En regresión lineal múltiple, en lugar de generar gráficos de dispersión simple (p.e. gráficos de dispersión matriciales) para detectar problemas (falta de linealidad, ) y analizar los efectos de las variables explicativas, se pueden generar gráficos parciales de residuos, por ejemplo con el comando: termplot(modelo, partial.resid = TRUE) Aunque puede ser preferible emplear las funciones crPlots ó avPlots del paquete car: library(car) crPlots(modelo) # avPlots(modelo) Estas funciones permitirían además detectar puntos atípicos o influyentes (mediante los argumentos id.method e id.n). 21.7.3 Estadísticos Para obtener medidas de diagnosis o resúmenes numéricos de interés se pueden emplear las siguientes funciones: Función Descripción rstandard residuos estandarizados rstudent residuos estudentizados (eliminados) cooks.distance valores del estadístico de Cook influence valores de influencia, cambios en coeficientes y varianza residual al eliminar cada dato. Ejecutar help(influence.measures) para ver un listado de medidas de diagnóstico adicionales. Hay muchas herramientas adicionales disponibles en otros paquetes. Por ejemplo, para la detección de multicolinealidad, se puede emplear la función vif del paquete car para calcular los factores de inflación de varianza para las variables del modelo: # library(car) vif(modelo) ## salini expprev ## 1.002041 1.002041 Valores grandes, por ejemplo &gt; 10, indican la posible presencia de multicolinealidad. Nota: Las tolerancias (proporciones de variabilidad no explicada por las demás covariables) se pueden calcular con 1/vif(modelo). 21.7.4 Contrastes 21.7.4.1 Normalidad Para realizar el contraste de normalidad de Shapiro-Wilk se puede emplear: shapiro.test(residuals(modelo)) ## ## Shapiro-Wilk normality test ## ## data: residuals(modelo) ## W = 0.85533, p-value &lt; 2.2e-16 hist(residuals(modelo)) 21.7.4.2 Homocedasticidad La librería lmtest proporciona herramientas adicionales para la diagnosis de modelos lineales, por ejemplo el test de Breusch-Pagan para heterocedasticidad: library(lmtest) bptest(modelo, studentize = FALSE) ## ## Breusch-Pagan test ## ## data: modelo ## BP = 290.37, df = 2, p-value &lt; 2.2e-16 Si el p-valor es grande aceptaríamos que hay igualdad de varianzas. 21.7.4.3 Autocorrelación Contraste de Durbin-Watson para detectar si hay correlación serial entre los errores: dwtest(modelo, alternative= &quot;two.sided&quot;) ## ## Durbin-Watson test ## ## data: modelo ## DW = 1.8331, p-value = 0.06702 ## alternative hypothesis: true autocorrelation is not 0 Si el p-valor es pequeño rechazaríamos la hipótesis de independencia. "],["21-8-métodos-de-regularización.html", "21.8 Métodos de regularización", " 21.8 Métodos de regularización [[Pasar a selección de variables explicativas?]] Estos métodos emplean también un modelo lineal: \\[Y=\\beta_{0}+\\beta_{1}X_{1}+\\beta_{2}X_{2}+\\cdots+\\beta_{p}X_{p}+\\varepsilon\\] En lugar de ajustarlo por mínimos cuadrados (estándar), minimizando: \\[ RSS = \\sum\\limits_{i=1}^{n}\\left( y_{i} - \\beta_0 - \\beta_1 x_{1i} - \\cdots - \\beta_p x_{pi} \\right)^{2}\\] Se imponen restricciones adicionales a los parámetros que los retraen (shrink) hacia cero: Produce una reducción en la varianza de predicción (a costa del sesgo). En principio se consideran todas las variables explicativas. Ridge regression Penalización cuadrática: \\(RSS+\\lambda\\sum_{j=1}^{p}\\beta_{j}^{2}\\). Lasso Penalización en valor absoluto: \\(RSS+\\lambda\\sum_{j=1}^{p}|\\beta_{j}|\\). Normalmente asigna peso nulo a algunas variables (selección de variables). El parámetro de penalización se selecciona por validación cruzada. Normalmente estandarizan las variables explicativas (coeficientes en la misma escala). 21.8.1 Datos El fichero hatco.RData contiene observaciones de clientes de la compañía de distribución industrial (Compañía Hair, Anderson y Tatham). Las variables se pueden clasificar en tres grupos: load(&#39;datos/hatco.RData&#39;) as.data.frame(attr(hatco, &quot;variable.labels&quot;)) ## attr(hatco, &quot;variable.labels&quot;) ## empresa Empresa ## tamano Tamaño de la empresa ## adquisic Estructura de adquisición ## tindustr Tipo de industria ## tsitcomp Tipo de situación de compra ## velocida Velocidad de entrega ## precio Nivel de precios ## flexprec Flexibilidad de precios ## imgfabri Imagen del fabricante ## servconj Servicio conjunto ## imgfvent Imagen de fuerza de ventas ## calidadp Calidad de producto ## fidelida Porcentaje de compra a HATCO ## satisfac Satisfacción global ## nfidelid Nivel de compra a HATCO ## nsatisfa Nivel de satisfacción Consideraremos como respuesta la variable fidelida y como variables explicativas el resto de variables continuas menos satisfac. library(glmnet) El paquete glmnet no emplea formulación de modelos, hay que establecer la respuesta y y las variables explicativas x (se puede emplear la función model.matrix() para construir x, la matriz de diseño, a partir de una fórmula). En este caso, eliminamos también la última fila por tener datos faltantes: x &lt;- as.matrix(hatco[-100, 6:12]) y &lt;- hatco$fidelida[-100] 21.8.2 Ridge Regression Ajustamos un modelo de regresión ridge con la función glmnet con alpha=0 (ridge penalty). fit.ridge &lt;- glmnet(x, y, alpha = 0) plot(fit.ridge, xvar = &quot;lambda&quot;, label = TRUE) Para seleccionar el parámetro de penalización por validación cruzada se puede emplear la función cv.glmnet. cv.ridge &lt;- cv.glmnet(x, y, alpha = 0) plot(cv.ridge) En este caso el parámetro sería: cv.ridge$lambda.1se ## [1] 4.378567 y el modelo resultante contiene todas las variables explicativas: coef(cv.ridge) ## 8 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s1 ## (Intercept) 7.28558079 ## velocida 1.55290772 ## precio 0.65329920 ## flexprec 2.10876330 ## imgfabri 0.33897337 ## servconj 3.61576314 ## imgfvent 1.03776789 ## calidadp 0.02802458 21.8.3 Lasso Ajustamos un modelo lasso también con la función glmnet (con la opción por defecto alpha=1, lasso penalty). fit.lasso &lt;- glmnet(x,y) plot(fit.lasso, xvar = &quot;lambda&quot;, label = TRUE) Seleccionamos el parámetro de penalización por validación cruzada. cv.lasso &lt;- cv.glmnet(x,y) plot(cv.lasso) En este caso el modelo resultante solo contiene 4 variables explicativas: coef(cv.lasso) ## 8 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s1 ## (Intercept) 5.5514217 ## velocida 0.1044538 ## precio . ## flexprec 2.6524703 ## imgfabri . ## servconj 6.3120283 ## imgfvent 0.3600231 ## calidadp . "],["21-9-alternativas.html", "21.9 Alternativas", " 21.9 Alternativas 21.9.1 Transformación (modelos linealizables) Cuando no se satisfacen los supuestos básicos se puede intentar transformar los datos para corregir la falta de linealidad, la heterocedasticidad y/o la falta de normalidad (normalmente estas últimas suelen ocurrir en la misma escala). Por ejemplo, la función boxcox del paquete MASS permite seleccionar la transformación de Box-Cox más adecuada: \\[Y^{(\\lambda)} = \\begin{cases} \\dfrac{Y^\\lambda - 1}{\\lambda} &amp; \\text{si } \\lambda \\neq 0 \\\\ \\ln{(Y)} &amp; \\text{si } \\lambda = 0 \\end{cases}\\] # library(MASS) boxcox(modelo) En este caso una transformación logarítmica parece adecuada. En ocasiones para obtener una relación lineal (o heterocedasticidad) también es necesario transformar las covariables además de la respuesta. Algunas de las relaciones fácilmente linealizables se muestran a continuación: modelo ecuación covariable respuesta logarítmico \\(y = a + b\\text{ }log(x)\\) \\(log(x)\\) _ inverso \\(y = a + b/x\\) \\(1/x\\) _ potencial \\(y = ax^b\\) \\(log(x)\\) \\(log(y)\\) exponencial \\(y = ae^{bx}\\) _ \\(log(y)\\) curva-S \\(y = ae^{b/x}\\) \\(1/x\\) \\(log(y)\\) 21.9.1.1 Ejemplo: plot(salario ~ salini, data = empleados, col = &#39;darkgray&#39;) # Ajuste lineal abline(lm(salario ~ salini, data = empleados)) # Modelo exponencial modelo1 &lt;- lm(log(salario) ~ salini, data = empleados) parest &lt;- coef(modelo1) curve(exp(parest[1] + parest[2]*x), lty = 2, add = TRUE) # Modelo logarítmico modelo2 &lt;- lm(log(salario) ~ log(salini), data = empleados) parest &lt;- coef(modelo2) curve(exp(parest[1]) * x^parest[2], lty = 3, add = TRUE) legend(&quot;bottomright&quot;, c(&quot;Lineal&quot;,&quot;Exponencial&quot;,&quot;Logarítmico&quot;), lty = 1:3) Con estos datos de ejemplo, el principal problema es la falta de homogeneidad de varianzas (y de normalidad) y se corrige sustancialmente con el segundo modelo: plot(log(salario) ~ log(salini), data = empleados) abline(modelo2) 21.9.2 Ajuste polinómico En este apartado utilizaremos como ejemplo el conjunto de datos Prestige de la librería car. Al tratar de explicar prestige (puntuación de ocupaciones obtenidas a partir de una encuesta ) a partir de income (media de ingresos en la ocupación), un ajuste cuadrático puede parecer razonable: # library(car) plot(prestige ~ income, data = Prestige, col = &#39;darkgray&#39;) # Ajuste lineal abline(lm(prestige ~ income, data = Prestige)) # Ajuste cuadrático modelo &lt;- lm(prestige ~ income + I(income^2), data = Prestige) parest &lt;- coef(modelo) curve(parest[1] + parest[2]*x + parest[3]*x^2, lty = 2, add = TRUE) legend(&quot;bottomright&quot;, c(&quot;Lineal&quot;,&quot;Cuadrático&quot;), lty = 1:2) Alternativamente se podría emplear la función poly: plot(prestige ~ income, data = Prestige, col = &#39;darkgray&#39;) # Ajuste cúbico modelo &lt;- lm(prestige ~ poly(income, 3), data = Prestige) valores &lt;- seq(0, 26000, len = 100) pred &lt;- predict(modelo, newdata = data.frame(income = valores)) lines(valores, pred, lty = 3) 21.9.3 Ajuste polinómico local (robusto) Si no se logra un buen ajuste empleando los modelos anteriores se puede pensar en utilizar métodos no paramétricos (p.e. regresión aditiva no paramétrica). Por ejemplo, enR es habitual emplear la función loess (sobre todo en gráficos): plot(prestige ~ income, Prestige, col = &#39;darkgray&#39;) fit &lt;- loess(prestige ~ income, Prestige, span = 0.75) valores &lt;- seq(0, 25000, 100) pred &lt;- predict(fit, newdata = data.frame(income = valores)) lines(valores, pred) Este tipo de modelos los trataremos con detalle más adelante "],["22-modelos-lineales-generalizados.html", "Capítulo 22 Modelos lineales generalizados", " Capítulo 22 Modelos lineales generalizados Los modelos lineales generalizados son una extensión de los modelos lineales para el caso de que la distribución condicional de la variable respuesta no sea normal (por ejemplo discreta: Bernouilli, Binomial, Poisson, ) En los modelo lineales se supone que: \\[E( Y | \\mathbf{X} ) = \\beta_{0}+\\beta_{1}X_{1}+\\beta_{2}X_{2}+\\cdots+\\beta_{p}X_{p}\\] En los modelos lineales generalizados se introduce una función invertible g, denominada función enlace (o link): \\[g\\left(E(Y | \\mathbf{X} )\\right) = \\beta_{0}+\\beta_{1}X_{1}+\\beta_{2}X_{2}+\\cdots+\\beta_{p}X_{p}\\] "],["22-1-ajuste-función-glm.html", "22.1 Ajuste: función glm", " 22.1 Ajuste: función glm Para el ajuste (estimación de los parámetros) de un modelo lineal generalizado a un conjunto de datos (por máxima verosimilitud) se emplea la función glm: ajuste &lt;- glm(formula, family = gaussian, datos, ...) El parámetro family indica la distribución y el link. Por ejemplo: gaussian(link = \"identity\"), gaussian(link = \"log\") binomial(link = \"logit\"), binomial(link = \"probit\") poisson(link = \"log\") Gamma(link = \"inverse\") Para cada distribución se toma por defecto una función link (mostrada en primer lugar; ver help(family) para más detalles). Muchas de las herramientas y funciones genéricas disponibles para los modelos lineales son válidas también para este tipo de modelos: summary, coef, confint, predict, anova, . Veremos con más detalle el caso particular de la regresión logística. "],["22-2-regresión-logística.html", "22.2 Regresión logística", " 22.2 Regresión logística 22.2.1 Ejemplo Como ejemplo emplearemos los datos de clientes de la compañía de distribución industrial (Compañía Hair, Anderson y Tatham). load(&quot;datos/hatco.RData&quot;) as.data.frame(attr(hatco, &quot;variable.labels&quot;)) ## attr(hatco, &quot;variable.labels&quot;) ## empresa Empresa ## tamano Tamaño de la empresa ## adquisic Estructura de adquisición ## tindustr Tipo de industria ## tsitcomp Tipo de situación de compra ## velocida Velocidad de entrega ## precio Nivel de precios ## flexprec Flexibilidad de precios ## imgfabri Imagen del fabricante ## servconj Servicio conjunto ## imgfvent Imagen de fuerza de ventas ## calidadp Calidad de producto ## fidelida Porcentaje de compra a HATCO ## satisfac Satisfacción global ## nfidelid Nivel de compra a HATCO ## nsatisfa Nivel de satisfacción Consideraremos como respuesta la variable nsatisfa y como variables explicativas el resto de variables continuas menos fidelida y satisfac. Eliminamos también la última fila por tener datos faltantes (realmente no sería necesario). datos &lt;- hatco[-100, c(6:12, 16)] plot(datos, pch = as.numeric(datos$nsatisfa), col = as.numeric(datos$nsatisfa)) 22.2.2 Ajuste de un modelo de regresión logística Se emplea la función glm seleccionando family = binomial (la función de enlace por defecto será logit): modelo &lt;- glm(nsatisfa ~ velocida + imgfabri , family = binomial, data = datos) modelo ## ## Call: glm(formula = nsatisfa ~ velocida + imgfabri, family = binomial, ## data = datos) ## ## Coefficients: ## (Intercept) velocida imgfabri ## -10.127 1.203 1.058 ## ## Degrees of Freedom: 98 Total (i.e. Null); 96 Residual ## Null Deviance: 136.4 ## Residual Deviance: 88.64 AIC: 94.64 La razón de ventajas (OR) permite cuantificar el efecto de las variables explicativas en la respuesta (Incremento proporcional en la ventaja o probabilidad de éxito, al aumentar una unidad la variable manteniendo las demás fijas): exp(coef(modelo)) # Razones de ventajas (&quot;odds ratios&quot;) ## (Intercept) velocida imgfabri ## 3.997092e-05 3.329631e+00 2.881619e+00 exp(confint(modelo)) ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## (Intercept) 3.828431e-07 0.001621259 ## velocida 2.061302e+00 5.976208357 ## imgfabri 1.737500e+00 5.247303813 Para obtener un resumen más completo del ajuste también se utiliza summary() summary(modelo) ## ## Call: ## glm(formula = nsatisfa ~ velocida + imgfabri, family = binomial, ## data = datos) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.8941 -0.6697 -0.2098 0.7865 2.3378 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -10.1274 2.1062 -4.808 1.52e-06 *** ## velocida 1.2029 0.2685 4.479 7.49e-06 *** ## imgfabri 1.0584 0.2792 3.790 0.000151 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 136.42 on 98 degrees of freedom ## Residual deviance: 88.64 on 96 degrees of freedom ## AIC: 94.64 ## ## Number of Fisher Scoring iterations: 5 La desvianza (deviance) es una medida de la bondad del ajuste de un modelo lineal generalizado (sería equivalente a la suma de cuadrados residual de un modelo lineal; valores más altos indican peor ajuste). La Null deviance se correspondería con un modelo solo con la constante y la Residual deviance con el modelo ajustado. En este caso hay una reducción de 47.78 con una pérdida de 2 grados de libertad (una reducción significativa). Para contrastar globalmente el efecto de las covariables también podemos emplear: modelo.null &lt;- glm(nsatisfa ~ 1, binomial, datos) anova(modelo.null, modelo, test = &quot;Chi&quot;) ## Analysis of Deviance Table ## ## Model 1: nsatisfa ~ 1 ## Model 2: nsatisfa ~ velocida + imgfabri ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 98 136.42 ## 2 96 88.64 2 47.783 4.207e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 "],["22-3-predicción-1.html", "22.3 Predicción", " 22.3 Predicción Las predicciones se obtienen también con la función predict: p.est &lt;- predict(modelo, type = &quot;response&quot;) El parámetro type = \"response\" permite calcular las probabilidades estimadas de la segunda categoría. Podríamos obtener una tabla de clasificación: cat.est &lt;- as.numeric(p.est &gt; 0.5) tabla &lt;- table(datos$nsatisfa, cat.est) tabla ## cat.est ## 0 1 ## bajo 44 10 ## alto 7 38 print(100*prop.table(tabla), digits = 2) ## cat.est ## 0 1 ## bajo 44.4 10.1 ## alto 7.1 38.4 Por defecto predict obtiene las predicciones correspondientes a las observaciones (modelo$fitted.values). Para otros casos hay que emplear el argumento newdata. "],["22-4-selección-de-variables-explicativas-1.html", "22.4 Selección de variables explicativas", " 22.4 Selección de variables explicativas El objetivo sería conseguir un buen ajuste con el menor número de variables explicativas posible. Para actualizar un modelo (p.e. eliminando o añadiendo variables) se puede emplear la función update: modelo.completo &lt;- glm(nsatisfa ~ . , family = binomial, data = datos) summary(modelo.completo) ## ## Call: ## glm(formula = nsatisfa ~ ., family = binomial, data = datos) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.01370 -0.31260 -0.02826 0.35423 1.74741 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -32.6317 7.7121 -4.231 2.32e-05 *** ## velocida 3.9980 2.3362 1.711 0.087019 . ## precio 3.6042 2.3184 1.555 0.120044 ## flexprec 1.5769 0.4433 3.557 0.000375 *** ## imgfabri 2.1669 0.6857 3.160 0.001576 ** ## servconj -4.2655 4.3526 -0.980 0.327096 ## imgfvent -1.1496 0.8937 -1.286 0.198318 ## calidadp 0.1506 0.2495 0.604 0.546147 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 136.424 on 98 degrees of freedom ## Residual deviance: 60.807 on 91 degrees of freedom ## AIC: 76.807 ## ## Number of Fisher Scoring iterations: 7 modelo.reducido &lt;- update(modelo.completo, . ~ . - calidadp) summary(modelo.reducido) ## ## Call: ## glm(formula = nsatisfa ~ velocida + precio + flexprec + imgfabri + ## servconj + imgfvent, family = binomial, data = datos) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.0920 -0.3518 -0.0280 0.3876 1.7885 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -31.6022 7.3962 -4.273 1.93e-05 *** ## velocida 4.1831 2.2077 1.895 0.058121 . ## precio 3.8872 2.1685 1.793 0.073044 . ## flexprec 1.5452 0.4361 3.543 0.000396 *** ## imgfabri 2.1984 0.6746 3.259 0.001119 ** ## servconj -4.6985 4.0597 -1.157 0.247125 ## imgfvent -1.1387 0.8784 -1.296 0.194849 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 136.424 on 98 degrees of freedom ## Residual deviance: 61.171 on 92 degrees of freedom ## AIC: 75.171 ## ## Number of Fisher Scoring iterations: 7 Para obtener el modelo óptimo lo ideal sería evaluar todos los modelos posibles. En este caso no se puede emplear la función regsubsets del paquete leaps (sólo para modelos lineales), pero por ejemplo el paquete bestglm proporciona una herramienta equivalente (bestglm()). 22.4.1 Selección por pasos La función stepwise del paquete RcmdrMisc (interfaz de stepAIC del paquete MASS) permite seleccionar el modelo por pasos según criterio AIC o BIC: library(MASS) library(RcmdrMisc) modelo &lt;- stepwise(modelo.completo, direction=&#39;backward/forward&#39;, criterion=&#39;BIC&#39;) ## ## Direction: backward/forward ## Criterion: BIC ## ## Start: AIC=97.57 ## nsatisfa ~ velocida + precio + flexprec + imgfabri + servconj + ## imgfvent + calidadp ## ## Df Deviance AIC ## - calidadp 1 61.171 93.337 ## - servconj 1 61.565 93.730 ## - imgfvent 1 62.668 94.834 ## - precio 1 62.712 94.878 ## - velocida 1 63.105 95.271 ## &lt;none&gt; 60.807 97.568 ## - imgfabri 1 76.251 108.416 ## - flexprec 1 82.443 114.609 ## ## Step: AIC=93.34 ## nsatisfa ~ velocida + precio + flexprec + imgfabri + servconj + ## imgfvent ## ## Df Deviance AIC ## - servconj 1 62.205 89.776 ## - imgfvent 1 63.055 90.625 ## - precio 1 63.698 91.269 ## - velocida 1 63.983 91.554 ## &lt;none&gt; 61.171 93.337 ## + calidadp 1 60.807 97.568 ## - imgfabri 1 77.823 105.394 ## - flexprec 1 82.461 110.032 ## ## Step: AIC=89.78 ## nsatisfa ~ velocida + precio + flexprec + imgfabri + imgfvent ## ## Df Deviance AIC ## - imgfvent 1 64.646 87.622 ## &lt;none&gt; 62.205 89.776 ## + servconj 1 61.171 93.337 ## + calidadp 1 61.565 93.730 ## - imgfabri 1 78.425 101.401 ## - precio 1 79.699 102.675 ## - flexprec 1 82.978 105.954 ## - velocida 1 88.731 111.706 ## ## Step: AIC=87.62 ## nsatisfa ~ velocida + precio + flexprec + imgfabri ## ## Df Deviance AIC ## &lt;none&gt; 64.646 87.622 ## + imgfvent 1 62.205 89.776 ## + servconj 1 63.055 90.625 ## + calidadp 1 63.890 91.460 ## - precio 1 80.474 98.854 ## - flexprec 1 83.663 102.044 ## - imgfabri 1 85.208 103.588 ## - velocida 1 89.641 108.021 summary(modelo) ## ## Call: ## glm(formula = nsatisfa ~ velocida + precio + flexprec + imgfabri, ## family = binomial, data = datos) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.99422 -0.36209 -0.03932 0.44249 1.80432 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -28.0825 6.4767 -4.336 1.45e-05 *** ## velocida 1.6268 0.4268 3.812 0.000138 *** ## precio 1.3749 0.4231 3.250 0.001155 ** ## flexprec 1.3364 0.3785 3.530 0.000415 *** ## imgfabri 1.5168 0.4252 3.567 0.000361 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 136.424 on 98 degrees of freedom ## Residual deviance: 64.646 on 94 degrees of freedom ## AIC: 74.646 ## ## Number of Fisher Scoring iterations: 6 "],["22-5-diagnosis-del-modelo-1.html", "22.5 Diagnosis del modelo", " 22.5 Diagnosis del modelo 22.5.1 Gráficas básicas de diagnóstico Con la función plot se pueden generar gráficos de interés para la diagnosis del modelo: oldpar &lt;- par( mfrow=c(2,2)) plot(modelo) par(oldpar) Aunque su interpretación difiere un poco de la de los modelos lineales 22.5.2 Gráficos parciales de residuos Se pueden generar gráficos parciales de residuos (p.e. crPlots() del paquete car): # library(car) crPlots(modelo) 22.5.3 Estadísticos Se pueden emplear las mismas funciones vistas en los modelos lineales para obtener medidas de diagnosis de interés (ver help(influence.measures)). Por ejemplo: residuals(model, type = &quot;deviance&quot;) proporciona los residuos deviance. En general, muchas de las herramientas para modelos lineales son también válidas para estos modelos. Por ejemplo: # library(car) vif(modelo) ## velocida precio flexprec imgfabri ## 2.088609 2.653934 2.520042 1.930409 "],["22-6-alternativas-1.html", "22.6 Alternativas", " 22.6 Alternativas Además de considerar ajustes polinómicos, pueden ser de interés emplear métodos no paramétricos. Por ejemplo, puede ser recomendable la función gam del paquete mgcv. "],["23-regresión-no-paramétrica.html", "Capítulo 23 Regresión no paramétrica", " Capítulo 23 Regresión no paramétrica No se supone ninguna forma concreta en el efecto de las variables explicativas: \\[Y=f\\left( \\mathbf{X}\\right) +\\varepsilon,\\] con f función cualquiera (suave). Métodos disponibles en R: Regresión local (métodos de suavizado): loess(), KernSmooth, sm,  Modelos aditivos generalizados (GAM): gam, mgcv,   "],["23-1-modelos-aditivos.html", "23.1 Modelos aditivos", " 23.1 Modelos aditivos Se supone que: \\[Y=\\beta_{0}+f_{1}\\left( \\mathbf{X}_{1}\\right) +f_{2}\\left( \\mathbf{X}_{2}\\right) +\\cdots+f_{p}\\left( \\mathbf{X}_{p}\\right) +\\varepsilon\\text{,}\\] con \\(f_{i},\\) \\(i=1,...,p,\\) funciones cualesquiera. Los modelos lineales son un caso particular considerando \\(f_{i}(x) = \\beta_{i}·x\\). Adicionalmente se puede considerar una función link: Modelos aditivos generalizados (GAM) Hastie, T.J. y Tibshirani, R.J. (1990). Generalized Additive Models. Chapman &amp; Hall. Wood, S. N. (2006). Generalized Additive Models: An Introduction with R. Chapman &amp; Hall/CRC 23.1.1 Ajuste: función gam La función gam del paquete mgcv permite ajustar modelos aditivos (generalizados) empleando regresión por splines (ver help(\"mgcv-package\")): library(mgcv) ajuste &lt;- gam(formula, family = gaussian, datos, pesos, seleccion, na.action, ...) Algunas posibilidades de uso son las que siguen: Modelo lineal: ajuste &lt;- gam(y ~ x1 + x2 + x3) Modelo aditivo con efectos no paramétricos para x1 y x2, y un efecto lineal para x3: ajuste &lt;- gam(y ~ s(x1) + s(x2) + x3) Modelo no aditivo (con interacción): ajuste &lt;- gam(y ~ s(x1, x2)) Modelo con distintas combinaciones: ajuste &lt;- gam(y ~ s(x1, x2) + s(x3) + x4) 23.1.2 Ejemplo En esta sección utilizaremos como ejemplo el conjunto de datos Prestige de la librería car. Se tratará de explicar prestige (puntuación de ocupaciones obtenidas a partir de una encuesta ) a partir de income (media de ingresos en la ocupación) y education (media de los años de educación). library(mgcv) library(car) modelo &lt;- gam(prestige ~ s(income) + s(education), data = Prestige) summary(modelo) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## prestige ~ s(income) + s(education) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 46.8333 0.6889 67.98 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(income) 3.118 3.877 14.61 &lt;2e-16 *** ## s(education) 3.177 3.952 38.78 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.836 Deviance explained = 84.7% ## GCV = 52.143 Scale est. = 48.414 n = 102 En este caso la función plot representa los efectos (parciales) estimados de cada covariable: par.old &lt;- par(mfrow = c(1, 2)) plot(modelo, shade = TRUE) # par(par.old) 23.1.3 Superficie de predicción Las predicciones se obtienen también con la función predict: pred &lt;- predict(modelo) Por defecto predict obtiene las predicciones correspondientes a las observaciones (modelo$fitted.values). Para otros casos hay que emplear el argumento newdata. Para representar las estimaciones (la superficie de predicción) obtenidas con el modelo se puede utilizar la función persp. Esta función necesita que los valores (x,y) de entrada estén dispuestos en una rejilla bidimensional. Para generar esta rejilla se puede emplear la función expand.grid(x,y) que crea todas las combinaciones de los puntos dados en x e y. inc &lt;- with(Prestige, seq(min(income), max(income), len = 25)) ed &lt;- with(Prestige, seq(min(education), max(education), len = 25)) newdata &lt;- expand.grid(income = inc, education = ed) # Representamos la rejilla plot(income ~ education, Prestige, pch = 16) abline(h = inc, v = ed, col = &quot;grey&quot;) # Se calculan las predicciones pred &lt;- predict(modelo, newdata) # Se representan pred &lt;- matrix(pred, nrow = 25) persp(inc, ed, pred, theta = -40, phi = 30) Alternativamente se podría emplear la función contour o filled.contour: # contour(inc, ed, pred, xlab = &quot;Income&quot;, ylab = &quot;Education&quot;) filled.contour(inc, ed, pred, xlab = &quot;Income&quot;, ylab = &quot;Education&quot;, key.title = title(&quot;Prestige&quot;)) Puede ser más cómodo emplear el paquete modelr junto a los gráficos ggplot2 para trabajar con modelos y predicciones. 23.1.4 Comparación de modelos Además de las medidas de bondad de ajuste como el coeficiente de determinación ajustado, también se puede emplear la función anova para la comparación de modelos. Por ejemplo, viendo el gráfico de los efectos se podría pensar que el efecto de education podría ser lineal: # plot(modelo) modelo0 &lt;- gam(prestige ~ s(income) + education, data = Prestige) summary(modelo0) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## prestige ~ s(income) + education ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.2240 3.7323 1.132 0.261 ## education 3.9681 0.3412 11.630 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(income) 3.58 4.441 13.6 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.825 Deviance explained = 83.3% ## GCV = 54.798 Scale est. = 51.8 n = 102 anova(modelo0, modelo, test=&quot;F&quot;) ## Analysis of Deviance Table ## ## Model 1: prestige ~ s(income) + education ## Model 2: prestige ~ s(income) + s(education) ## Resid. Df Resid. Dev Df Deviance F Pr(&gt;F) ## 1 95.559 4994.6 ## 2 93.171 4585.0 2.3886 409.58 3.5418 0.0257 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 En este caso aceptaríamos que el modelo original es significativamente mejor. Alternativamente, podríamos pensar que hay interacción: modelo2 &lt;- gam(prestige ~ s(income, education), data = Prestige) summary(modelo2) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## prestige ~ s(income, education) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 46.8333 0.7138 65.61 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(income,education) 4.94 6.303 75.41 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.824 Deviance explained = 83.3% ## GCV = 55.188 Scale est. = 51.974 n = 102 # plot(modelo2, se = FALSE) En este caso el coeficiente de determinación ajustado es menor 23.1.5 Diagnosis del modelo La función gam.check realiza una diagnosis del modelo: gam.check(modelo) ## ## Method: GCV Optimizer: magic ## Smoothing parameter selection converged after 4 iterations. ## The RMS GCV score gradient at convergence was 9.783945e-05 . ## The Hessian was positive definite. ## Model rank = 19 / 19 ## ## Basis dimension (k) checking results. Low p-value (k-index&lt;1) may ## indicate that k is too low, especially if edf is close to k&#39;. ## ## k&#39; edf k-index p-value ## s(income) 9.00 3.12 0.98 0.33 ## s(education) 9.00 3.18 1.03 0.56 Lo ideal sería observar normalidad en los dos gráficos de la izquierda, falta de patrón en el superior derecho, y ajuste a una recta en el inferior derecho. En este caso parece que el modelo se comporta adecuadamente. "],["24-programacion.html", "Capítulo 24 Programación", " Capítulo 24 Programación En este capítulo se introducirán los comandos básicos de programación en R "],["24-1-funciones-1.html", "24.1 Funciones", " 24.1 Funciones El lenguaje R permite al usuario definir sus propias funciones. El esquema de una función es el que sigue: nombre &lt;- function(arg1, arg2, ... ) {expresión} En la expresión anterior arg1, arg2, ... son los argumentos de entrada (también llamados parámetros). La expresión está compuesta de comandos que utilizan los argumentos de entrada para dar la salida deseada. La salida de una función puese ser un número, un vector, una grafica, un mensaje, etc. 24.1.1 Ejemplo: progresión geométrica Para introducirnos en las funciones, vamos a escribir una función que permita trabajar con las llamadas progresiones geométricas. Una progresión geométrica es una sucesión de números \\(a_1, a_2, a_3\\ldots\\) tales que cada uno de ellos (salvo el primero) es igual al anterior multiplicado por una constante llamada razón, que representaremos por \\(r\\). Ejemplos: \\(a_1=1\\), \\(r=2\\): 1, 2, 4, 8, 16, \\(a_1=-1\\), \\(r=-2\\): 1, -2, 4, -8, 16, Según la definición anterior, se verifica que: \\[a_2=a_1\\cdot r; \\quad a_3=a_2\\cdot r=a_1\\cdot r^2; \\quad ...\\] y generalizando este proceso se obtiene el llamado término general: \\[a_n=a_1\\cdot r^{n-1}\\] También se puede comprobar que la suma de los \\(n\\) términos de la progresión es: \\[S_n=a_1+\\ldots_+a_n=\\frac{a_1(r^n-1)}{r-1}\\] La siguiente función, que llamaremos an calcula el término \\(a_n\\) de una progresión geométrica pasando como entrada el primer elemento a1, la razón r y el valor n: an &lt;- function(a1, r, n) { a1 * r^(n - 1) } A continuación algún ejemplo para comprobar su funcionamiento: an(a1 = 1, r = 2, n = 5) ## [1] 16 an(a1 = 4, r = -2, n = 6) ## [1] -128 an(a1 = -50, r = 4, n = 6) ## [1] -51200 Con la función anterior se pueden obtener, con una sola llamada, varios valores de la progresión: an(a1 = 1, r = 2, n = 1:5) # a1, ..., a5 ## [1] 1 2 4 8 16 an(a1 = 1, r = 2, n = 10:15) # a10, ..., a15 ## [1] 512 1024 2048 4096 8192 16384 La función Sn calcula la suma de los primeros n elementos de la progresión: Sn &lt;- function(a1, r, n) { a1 * (r^n - 1) / (r - 1) } Sn(a1 = 1, r = 2, n = 5) ## [1] 31 an(a1 = 1, r = 2, n = 1:5) # Valores de la progresión ## [1] 1 2 4 8 16 Sn(a1 = 1, r = 2, n = 1:5) # Suma de los valores ## [1] 1 3 7 15 31 # cumsum(an(a1 = 1, r = 2, n = 1:5)) 24.1.2 Argumentos de entrada Como ya hemos comentado, los argumentos son los valores de entrada de una función. Por ejemplo, en la función anterior: an &lt;- function(a1, r, n) {a1 * r^(n - 1)} los argumentos de entrada son a1, r y n. Veamos alguna consideración sobre los argumentos: No es necesario utilizar el nombre de los argumentos. En este caso es obligatorio mantener el orden de entrada. Por ejemplo, las siguientes llamadas son equivalentes: an(1, 2, 5) ## [1] 16 an(a1 = 1, r = 2, n = 5) ## [1] 16 Si se nombran los argumentos, se pueden pasar en cualquier orden: an(r = 2, n = 5, a1 = 1) ## [1] 16 an(n = 5, r = 2, a1 = 1) ## [1] 16 24.1.2.1 Argumentos por defecto En muchas ocasiones resulta muy interesante que las funciones tengan argumentos por defecto. Por ejemplo, si se quiere que en una función: nombre &lt;- function(arg1, arg2, arg3, arg4, ...) { expresión } los argumento arg2 y arg3 tomen por defecto los valores a y b respectivamentebastaría con escribir: nombre &lt;- function(arg1, arg2 = a, arg3 = b, arg4, ...) { expresión } Para comprender mejor esto considérese el siguiente ejemplo ilustrativo: xy2 &lt;- function(x = 2, y = 3) { x * y^2 } xy2() ## [1] 18 xy2(x = 1, y = 4) ## [1] 16 xy2(y = 4) ## [1] 32 24.1.2.2 El argumento ... El argumento ... permite pasar de manera libre argumentos adicionales para ser utilizados por otra subfunción dentro de la función principal. Por ejemplo, en la función: Density.Plot &lt;- function(datos, ...) { plot(density(datos), ...) } a partir del primer argumento, los argumentos se incluirán en ... y serán utilizados por la función plot. data(cars) Density.Plot(cars[,1]) Density.Plot(cars[,1], col = &#39;red&#39;, xlab = &quot;velocidad&quot;, ylab = &quot;distancia&quot;) Los argumentos de entrada de una función se obtienen ejecutando args(funcion): args(an) ## function (a1, r, n) ## NULL args(xy2) ## function (x = 2, y = 3) ## NULL str(args(Density.Plot)) ## function (datos, ...) Por otro lado, al escribir el nombre de una función se obtiene su contenido: an ## function(a1, r, n) { ## a1 * r^(n - 1) ## } ## &lt;bytecode: 0x0000000015f5fb70&gt; 24.1.3 Salida El valor que devolverá una función será: el último objeto evaluado dentro de ella, o lo indicado dentro de la sentencia return. Como las funciones pueden devolver objetos de varios tipos es hatibual que la salida sea una lista. an &lt;- function(a1, r, n) { a1 * r^(n - 1) } Sn &lt;- function(a1, r, n) { a1 * (r^n - 1) / (r - 1) } asn &lt;- function(a1 = 1, r = 2, n = 5) { A &lt;- an(a1, r, n) S &lt;- Sn(a1, r, n) ii &lt;- 1:n AA &lt;- an(a1, r, ii) SS &lt;- Sn(a1, r, ii) return(list(an = A, Sn = S, salida = data.frame(valores = AA, suma = SS))) } La función asn utiliza las funiones an y Sn programadas antes y devuelve como salida una lista con las siguientes componentes: an: valor de \\(a_n\\) Sn: valor de \\(S_n\\) salida: data.frame con dos variables salida: vector con las \\(n\\) primeras componentes de la progresión suma: suma de las \\(n\\) primeras componentes asn() ## $an ## [1] 16 ## ## $Sn ## [1] 31 ## ## $salida ## valores suma ## 1 1 1 ## 2 2 3 ## 3 4 7 ## 4 8 15 ## 5 16 31 La salida de la función anterior es una lista y se puede acceder a los elementos de la misma: res &lt;- asn() res$an ## [1] 16 res$Sn ## [1] 31 res$salida ## valores suma ## 1 1 1 ## 2 2 3 ## 3 4 7 ## 4 8 15 ## 5 16 31 24.1.4 Otros ejemplos 24.1.4.1 Ejemplo: letra del DNI A continuación se calculará la letra del DNI a partir de su correspondiente número. El método utilizado para obtener la letra del DNI consiste en dividir el número entre 23 y según el resto obtenido adjudicar la letra que figura en la siguiente tabla: resto letra resto letra resto letra 0 T 8 P 16 Q 1 R 9 D 17 V 2 W 10 X 18 H 3 A 11 B 19 L 4 G 12 N 20 C 5 M 13 J 21 K 6 Y 14 Z 22 E 7 F 15 S La siguiente función permite obtener la letra del DNI: DNI &lt;- function(numero) { letras &lt;- c(&quot;T&quot;, &quot;R&quot;, &quot;W&quot;, &quot;A&quot;, &quot;G&quot;, &quot;M&quot;, &quot;Y&quot;, &quot;F&quot;, &quot;P&quot;, &quot;D&quot;, &quot;X&quot;, &quot;B&quot;, &quot;N&quot;, &quot;J&quot;, &quot;Z&quot;, &quot;S&quot;, &quot;Q&quot;, &quot;V&quot;, &quot;H&quot;, &quot;L&quot;, &quot;C&quot;, &quot;K&quot;, &quot;E&quot;) return(letras[numero %% 23 + 1]) } DNI(50247828) ## [1] &quot;G&quot; 24.1.4.2 Ejemplo: simulación del lanzamiento de un dado La siguiente función simula \\(n\\) (por defecto \\(n=100\\)) lanzamientos de un dado. La función devuelve la tabla de frecuencias y realiza el correspondiente gráfico: dado &lt;- function(n = 100) { lanzamientos &lt;- sample(1:6, n, rep = TRUE) frecuencias &lt;- table(lanzamientos) / n barplot(frecuencias, main = paste(&quot;Número de lanzamientos=&quot;, n)) abline(h = 1 / 6, col = &#39;red&#39;, lwd = 2) return(frecuencias) } A continuación se muestran los resultados obtendidos para varias simulaciones: dado(100) ## lanzamientos ## 1 2 3 4 5 6 ## 0.20 0.11 0.10 0.14 0.21 0.24 dado(500) ## lanzamientos ## 1 2 3 4 5 6 ## 0.186 0.172 0.152 0.164 0.178 0.148 dado(10000) ## lanzamientos ## 1 2 3 4 5 6 ## 0.1711 0.1604 0.1688 0.1667 0.1683 0.1647 Se puede comprobar que al aumentar el valor de \\(n\\) las frecuencias se aproximan al valor teórico \\(1/6=0.1667\\). 24.1.5 Variables locales y globales En R no es necesario declarar las variables usadas dentro de una función. Se utiliza la regla llamada ámbito lexicográfico para decidir si un objeto es local a una función o global. Para entender mejor esto se consideran los siguientes ejemplos: fun &lt;- function() print(x) x &lt;- 1 fun() ## [1] 1 La variable x no está definida dentro de fun, así que R busca x en el entorno en el que se llamó a la función e imprimirá su valor. Si x es utilizado como el nombre de un objeto dentro de la función, el valor de x en el ambiente global (fuera de la función) no cambia. x &lt;- 1 fun2 &lt;- function() { x &lt;- 2 print(x) } fun2() ## [1] 2 x ## [1] 1 Para que el valor global de una variable pueda ser cambidado dentro de una función se utiliza la doble asignación &lt;&lt;-. x &lt;- 1 y &lt;- 3 fun2 &lt;- function() { x &lt;- 2 y &lt;&lt;- 5 print(x) print(y) } fun2() ## [1] 2 ## [1] 5 x # No cambió su valor ## [1] 1 y # Cambió su valor ## [1] 5 "],["24-2-ejecución-condicional.html", "24.2 Ejecución condicional", " 24.2 Ejecución condicional Para hacer ejecuciones condicionales de código se usa el comando if con sintaxis: if (condicion1) {expresión1} else {expresión2} La siguiente función comprueba si un número es múltiplo de dos: multiplo2 = function(x) { if (x %% 2 == 0) { print(paste(x,&#39;es múltiplo de dos&#39;)) } else { print(paste(x,&#39;no es múltiplo de dos&#39;)) } } multiplo2(5) ## [1] &quot;5 no es múltiplo de dos&quot; multiplo2(-2.3) ## [1] &quot;-2.3 no es múltiplo de dos&quot; multiplo2(10) ## [1] &quot;10 es múltiplo de dos&quot; "],["24-3-bucles-y-vectorización.html", "24.3 Bucles y vectorización", " 24.3 Bucles y vectorización 24.3.1 Bucles R permite crear bucles repetitivos (loops) y la ejecución condicional de sentencias. R admite bucles for, repeat and while. 24.3.1.1 El bucle for La sintaxis de un bucle for es la que sigue: for (i in lista_de_valores) { expresión } Por ejemplo, dado un vector \\(x\\) se puede calcular \\(y=x^2\\) con el código: x &lt;- seq(-2, 2, 0.5) n &lt;- length(x) y &lt;- numeric(n) # Es necesario crear el objeto para acceder a los componentes... for (i in 1:n) { y[i] &lt;- x[i] ^ 2 } x ## [1] -2.0 -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 y ## [1] 4.00 2.25 1.00 0.25 0.00 0.25 1.00 2.25 4.00 x^2 ## [1] 4.00 2.25 1.00 0.25 0.00 0.25 1.00 2.25 4.00 Otro ejemplo: for(i in 1:5) print(i) ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 El siguiente código simula gráficamente el segundero de un reloj: angulo &lt;- seq(0, 360, length = 60) radianes &lt;- angulo * pi / 180 x &lt;- cos(radianes) y &lt;- sin(radianes) for (i in 1:360) { plot(y, x, axes = FALSE, xlab = &quot;&quot;, ylab = &quot;&quot;, type = &#39;l&#39;, col = &#39;grey&#39;) arrows(0, 0, y[i], x[i], col = &#39;blue&#39;) Sys.sleep(1) # espera un segundo } 24.3.1.2 El bucle while La sintaxis del bucle while es la que sigue: while (condición lógica) { expresión } Por ejemplo, si queremos calcular el primer número entero positivo cuyo cuadrado no excede de 5000, podemos hacer: cuadrado &lt;- 0 n &lt;- 0 while (cuadrado &lt;= 5000) { n &lt;- n + 1 cuadrado &lt;- n^2 } cuadrado ## [1] 5041 n ## [1] 71 n^2 ## [1] 5041 Nota: Dentro de un bucle se puede emplear el comando break para terminarlo y el comando next para saltar a la siguiente iteración. 24.3.2 Vectorización Como hemos visto en R se pueden hacer bucles. Sin embargo, es preferible evitar este tipo de estructuras y tratar de utilizar operaciones vectorizadas que son mucho más eficientes desde el punto de vista computacional. Por ejemplo para sumar dos vectores se puede hacer con un for: x &lt;- c(1, 2, 3, 4) y &lt;- c(0, 0, 5, 1) n &lt;- length(x) z &lt;- numeric(n) for (i in 1:n) { z[i] &lt;- x[i] + y[i] } z ## [1] 1 2 8 5 Sin embargo, la operación anterior se podría hacer de modo más eficiente en modo vectorial: z &lt;- x + y z ## [1] 1 2 8 5 24.3.3 Funciones apply 24.3.3.1 La función apply Una forma de evitar la utilización de bucles es utilizando la sentica apply que permite evaluar una misma función en todas las filas, columnas, . de un array de forma simultánea. La sintaxis de esta función es: apply(X, MARGIN, FUN, ...) X: matriz (o array) MARGIN: Un vector indicando las dimensiones donde se aplicará la función. 1 indica filas, 2 indica columnas, y c(1,2) indica filas y columnas. FUN: función que será aplicada. ...: argumentos opcionales que serán usados por FUN. Veamos la utilización de la función apply con un ejemplo: x &lt;- matrix(1:9, nrow = 3) x ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 apply(x, 1, sum) # Suma por filas ## [1] 12 15 18 apply(x, 2, sum) # Suma por columnas ## [1] 6 15 24 apply(x, 2, min) # Mínimo de las columnas ## [1] 1 4 7 apply(x, 2, range) # Rango (mínimo y máximo) de las columnas ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 3 6 9 24.3.3.2 La función tapply La function tapply es similar a la función apply y permite aplicar una función a los datos desagregados, utilizando como criterio los distintos niveles de una variable factor. La sintaxis de esta función es como sigue: tapply(X, INDEX, FUN, ...,) X: matriz (o array). INDEX: factor indicando los grupos (niveles). FUN: función que será aplicada. ...: argumentos opcionales . Consideremos, por ejemplo, el data.frame ChickWeight con datos de un experimento relacionado con la repercusión de varias dietas en el peso de pollos. data(ChickWeight) head(ChickWeight) ## weight Time Chick Diet ## 1 42 0 1 1 ## 2 51 2 1 1 ## 3 59 4 1 1 ## 4 64 6 1 1 ## 5 76 8 1 1 ## 6 93 10 1 1 peso &lt;- ChickWeight$weight dieta &lt;- ChickWeight$Diet levels(dieta) &lt;- c(&quot;Dieta 1&quot;, &quot;Dieta 2&quot;, &quot;Dieta 3&quot;, &quot;Dieta 4&quot;) tapply(peso, dieta, mean) # Peso medio por dieta ## Dieta 1 Dieta 2 Dieta 3 Dieta 4 ## 102.6455 122.6167 142.9500 135.2627 tapply(peso, dieta, summary) ## $`Dieta 1` ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 35.00 57.75 88.00 102.65 136.50 305.00 ## ## $`Dieta 2` ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 39.0 65.5 104.5 122.6 163.0 331.0 ## ## $`Dieta 3` ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 39.0 67.5 125.5 142.9 198.8 373.0 ## ## $`Dieta 4` ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 39.00 71.25 129.50 135.26 184.75 322.00 Otro ejemplo: provincia &lt;- as.factor(c(1, 3, 4, 2, 4, 3, 2, 1, 4, 3, 2)) levels(provincia) = c(&quot;A Coruña&quot;, &quot;Lugo&quot;, &quot;Orense&quot;, &quot;Pontevedra&quot;) hijos &lt;- c(1, 2, 0, 3, 4, 1, 0, 0, 2, 3, 1) data.frame(provincia, hijos) ## provincia hijos ## 1 A Coruña 1 ## 2 Orense 2 ## 3 Pontevedra 0 ## 4 Lugo 3 ## 5 Pontevedra 4 ## 6 Orense 1 ## 7 Lugo 0 ## 8 A Coruña 0 ## 9 Pontevedra 2 ## 10 Orense 3 ## 11 Lugo 1 tapply(hijos, provincia, mean) # Número medio de hijos por provincia ## A Coruña Lugo Orense Pontevedra ## 0.500000 1.333333 2.000000 2.000000 "],["24-4-aplicación-validación-cruzada.html", "24.4 Aplicación: validación cruzada", " 24.4 Aplicación: validación cruzada Si deseamos evaluar la calidad predictiva de un modelo, lo ideal es disponer de suficientes datos para poder hacer dos grupos con ellos: una muestra de entrenamiento y otra de validación. Cuando hacer esto no es posible, disponemos como alternativa de la validación cruzada, una herramienta que permite estimar los errores de predicción utilizando una única muestra de datos. En su versión más simple (llamada en inglés leave-one-out): se utilizan todos los datos menos uno para realizar el ajuste, y se mide su error de predicción en el único dato no utilizado; a continuación se repite el proceso utilizando, uno a uno, todos los puntos de la muestra de datos; y finalmente se combinan todos los errores en un único error de predicción. El proceso anterior se puede generalizar repartiendo los datos en distintos grupos, más o menos del mismo tamaño, y sustituyendo en la explicación anterior dato por grupo. 24.4.1 Primer ejemplo Cuando disponemos de unos datos y los queremos ajustar utilizando un modelo que depende de un parámetro, por ejemplo un modelo de regresión polinómico que depende del grado del polinomio, podemos utilizar la validación cruzada para seleccionar el grado del polinomio que debemos utilizar. Veámoslo utilizando las variables income y prestige de la base de datos Prestige, incluida en el paquete car. library(car) plot(prestige ~ income, data = Prestige, col = &#39;darkgray&#39;) Representemos, gráficamente, los ajustes lineal, cuadrático y cúbico. plot(prestige ~ income, data = Prestige, col = &#39;darkgray&#39;) # Ajuste lineal abline(lm(prestige ~ income, data = Prestige)) # Ajuste cuadrático modelo &lt;- lm(prestige ~ income + I(income^2), data = Prestige) parest &lt;- coef(modelo) curve(parest[1] + parest[2]*x + parest[3]*x^2, lty = 2, add = TRUE) # Ajuste cúbico modelo &lt;- lm(prestige ~ poly(income, 3), data = Prestige) valores &lt;- seq(0, 26000, len = 100) pred &lt;- predict(modelo, newdata = data.frame(income = valores)) lines(valores, pred, lty = 3) legend(&quot;bottomright&quot;, c(&quot;Lineal&quot;,&quot;Cuadrático&quot;,&quot;Cúbico&quot;), lty = 1:3) Vamos a escribir una función que nos devuelva, para cada dato (fila) de Prestige, la predicción en ese punto ajustando el modelo con todos los demás puntos. cv.lm &lt;- function(formula, datos) { n &lt;- nrow(datos) cv.pred &lt;- numeric(n) for (i in 1:n) { modelo &lt;- lm(formula, datos[-i, ]) cv.pred[i] &lt;- predict(modelo, newdata = datos[i, ]) } return(cv.pred) } Por último, calculamos el error de predicción (en este caso el error cuadrático medio) en los datos de validación. Repetimos el proceso para cada valor del parámetro (grado del ajuste polinómico) y minimizamos. grado &lt;- 1:5 cv.error &lt;- numeric(5) for(p in grado){ cv.pred &lt;- cv.lm(prestige ~ poly(income, p), Prestige) cv.error[p] &lt;- mean((cv.pred - Prestige$prestige)^2) } plot(grado, cv.error, pch=16) grado[which.min(cv.error)] ## [1] 2 24.4.2 Segundo ejemplo En este segundo ejemplo vamos a aplicar una técnica de modelado local al problema de regresión del ejemplo anterior. El enfoque es data-analytic en el sentido de que no nos limitamos a una familia de funciones que dependen de unos parámetros (enfoque paramétrico), que son los que tenemos que determinar, sino que las funciones de regresión están determinadas por los datos. Aun así, sigue habiendo un parámetro que controla el proceso, cuyo valor debemos fijar siguiendo algún criterio de optimalidad. Vamos a realizar, utilizando la función loess, un ajuste polinómico local robusto, que depende del parámetro span, que podemos interpretar como la proporción de datos empleada en el ajuste. Utilizando un valor span=0.25: plot(prestige ~ income, Prestige, col = &#39;darkgray&#39;) fit &lt;- loess(prestige ~ income, Prestige, span = 0.25) valores &lt;- seq(0, 25000, 100) pred &lt;- predict(fit, newdata = data.frame(income = valores)) lines(valores, pred) Si utilizamos span=0.5: plot(prestige ~ income, Prestige, col = &#39;darkgray&#39;) fit &lt;- loess(prestige ~ income, Prestige, span = 0.5) valores &lt;- seq(0, 25000, 100) pred &lt;- predict(fit, newdata = data.frame(income = valores)) lines(valores, pred) Nuestro objetivo es seleccionar un valor razonable para span, y lo vamos a hacer utilizando validación cruzada y minimizando el error cuadrático medio de la predicción en los datos de validación. Utilizando la función cv.loess &lt;- function(formula, datos, p) { n &lt;- nrow(datos) cv.pred &lt;- numeric(n) for (i in 1:n) { modelo &lt;- loess(formula, datos[-i, ], span = p, control = loess.control(surface = &quot;direct&quot;)) # control = loess.control(surface = &quot;direct&quot;) permite extrapolaciones cv.pred[i] &lt;- predict(modelo, newdata = datos[i, ]) } return(cv.pred) } y procediendo de modo similar al caso anterior: ventanas &lt;- seq(0.2, 1, len = 10) np &lt;- length(ventanas) cv.error &lt;- numeric(np) for(p in 1:np){ cv.pred &lt;- cv.loess(prestige ~ income, Prestige, ventanas[p]) cv.error[p] &lt;- mean((cv.pred - Prestige$prestige)^2) # cv.error[p] &lt;- median(abs(cv.pred - Prestige$prestige)) } plot(ventanas, cv.error) obtenemos la ventana óptima (en este caso el valor máximo): span &lt;- ventanas[which.min(cv.error)] span ## [1] 1 y la correspondiente estimación: plot(prestige ~ income, Prestige, col = &#39;darkgray&#39;) fit &lt;- loess(prestige ~ income, Prestige, span = span) valores &lt;- seq(0, 25000, 100) pred &lt;- predict(fit, newdata = data.frame(income = valores)) lines(valores, pred) "],["25-generación-de-informes.html", "Capítulo 25 Generación de informes", " Capítulo 25 Generación de informes Una versión más completa de este capítulo está disponible en el apéndice del libro Escritura de libros con bookdown "],["25-1-r-markdown.html", "25.1 R Markdown", " 25.1 R Markdown R-Markdown es recomendable para difundir análisis realizados con R en formato HTML, PDF y DOCX (Word), entre otros. 25.1.1 Introducción R-Markdown permite combinar Markdown con R. Markdown se diseñó inicialmente para la creación de páginas web a partir de documentos de texto de forma muy sencilla y rápida (tiene unas reglas sintácticas muy simples). Actualmente gracias a múltiples herramientas como pandoc permite generar múltiples tipos de documentos (incluido LaTeX; ver Pandoc Markdown) Para más detalles ver http://rmarkdown.rstudio.com. También se dispone de información en la ayuda de RStudio: Help &gt; Markdown Quick Reference Help &gt; Cheatsheets &gt; R Markdown Cheat Sheet Help &gt; Cheatsheets &gt; R Markdown Reference Guide Al renderizar un fichero rmarkdown se generará un documento que incluye el código R y los resultados incrustados en el documento. En RStudio basta con hacer clic en el botón Knit HTML. En R se puede emplear la funcion render del paquete rmarkdown (por ejemplo: render(\"8-Informes.Rmd\")). También se puede abrir directamente el informe generado: library(rmarkdown) browseURL(url = render(&quot;8-Informes.Rmd&quot;)) 25.1.2 Inclusión de código R Se puede incluir código R entre los delimitadores ```{r} y ```. Por defecto, se mostrará el código, se evaluará y se mostrarán los resultados justo a continuación: head(mtcars[1:3]) ## mpg cyl disp ## Mazda RX4 21.0 6 160 ## Mazda RX4 Wag 21.0 6 160 ## Datsun 710 22.8 4 108 ## Hornet 4 Drive 21.4 6 258 ## Hornet Sportabout 18.7 8 360 ## Valiant 18.1 6 225 summary(mtcars[1:3]) ## mpg cyl disp ## Min. :10.40 Min. :4.000 Min. : 71.1 ## 1st Qu.:15.43 1st Qu.:4.000 1st Qu.:120.8 ## Median :19.20 Median :6.000 Median :196.3 ## Mean :20.09 Mean :6.188 Mean :230.7 ## 3rd Qu.:22.80 3rd Qu.:8.000 3rd Qu.:326.0 ## Max. :33.90 Max. :8.000 Max. :472.0 En RStudio pulsando Ctrl + Alt + I o en el icono correspondiente se incluye un trozo de código. Se puede incluir código en línea empleando `r código`, por ejemplo `r 2 + 2` produce 4. 25.1.3 Inclusión de gráficos Se pueden generar gráficos: Los trozos de código pueden tener nombre y opciones, se establecen en la cabecera de la forma ```{r nombre, op1, op2} (en el caso anterior no se muestra el código, al haber empleado ```{r, echo=FALSE}). Para un listado de las opciones disponibles ver http://yihui.name/knitr/options. En RStudio se puede pulsar en los iconos a la derecha del chunk para establecer opciones, ejecutar todo el código anterior o sólo el correspondiente trozo. 25.1.4 Inclusión de tablas Las tablas en markdown son de la forma: | First Header | Second Header | | ------------- | ------------- | | Row1 Cell1 | Row1 Cell2 | | Row2 Cell1 | Row2 Cell2 | Por ejemplo: Variable Descripción mpg Millas / galón (EE.UU.) cyl Número de cilindros disp Desplazamiento (pulgadas cúbicas) hp Caballos de fuerza bruta drat Relación del eje trasero wt Peso (miles de libras) qsec Tiempo de 1/4 de milla vs Cilindros en V/Straight (0 = cilindros en V, 1 = cilindros en línea) am Tipo de transmisión (0 = automático, 1 = manual) gear Número de marchas (hacia adelante) carb Número de carburadores Para convertir resultados de R en tablas de una forma simple se puede emplear la función ktable del paquete knitr: knitr::kable( head(mtcars), caption = &quot;Una kable knitr&quot; ) Tabla 25.1: Una kable knitr mpg cyl disp hp drat wt qsec vs am gear carb Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 Otros paquetes proporcionan opciones adicionales: xtable, stargazer, pander, tables y ascii. 25.1.5 Extracción del código R Para generar un fichero con el código R se puede emplear la función purl del paquete knitr. Por ejemplo: purl(&quot;8-Informes.Rmd&quot;) Si se quiere además el texto rmarkdown como comentarios tipo spin, se puede emplear: purl(&quot;8-Informes.Rmd&quot;, documentation = 2) "],["25-2-spin.html", "25.2 Spin", " 25.2 Spin Una forma rápida de crear este tipo de informes a partir de un fichero de código R es emplear la funcion spin del paquete knitr (ver p.e. http://yihui.name/knitr/demo/stitch). Para ello se debe comentar todo lo que no sea código R de una forma especial: El texto rmarkdown se comenta con #'. Por ejemplo: #' # Este es un título de primer nivel #' ## Este es un título de segundo nivel Las opciones de un trozo de código se comentan con #+. Por ejemplo: #+ setup, include=FALSE opts_chunk$set(comment=NA, prompt=TRUE, dev='svg', fig.height=6, fig.width=6) Para generar el informe se puede emplear la funcion spin del paquete knitr. Por ejemplo: spin(\"Ridge_Lasso.R\")). También se podría abrir directamente el informe generado: browseURL(url = knitr::spin(&quot;Ridge_Lasso.R&quot;)) Pero puede ser recomendable renderizarlo con rmarkdown: library(rmarkdown) browseURL(url = render(knitr::spin(&quot;Ridge_Lasso.R&quot;, knit = FALSE))) En RStudio basta con pulsar Ctrl + Shift + K o seleccionar File &gt; Knit Document (en las últimas versiones también File &gt; Compile Notebook o hacer clic en el icono correspondiente). "],["referencias.html", "Referencias", " Referencias Ver Apéndice A. "],["bibliografía-complementaria.html", "Bibliografía complementaria", " Bibliografía complementaria Beeley (2015). Web Application Development with R Using Shiny. Packt Publishing. Bivand et al. (2008). Applied Spatial Data Analysis with R. Springer. James et al. (2008). An Introduction to Statistical Learning: with Aplications in R. Springer. Kolaczyk y Csárdi (2014). Statistical analysis of network data with R. Springer. Munzert et al. (2014). Automated Data Collection with R: A Practical Guide to Web Scraping and Text Mining. Wiley. Ramsay et al. (2009). Functional Data Analysis with R and MATLAB. Springer. Van der Loo y de Jonge (2012). Learning RStudio for R Statistical Computing. Packt Publishing. Williams (2011). Data Mining with Rattle and R. Springer. Wood (2006). Generalized Additive Models: An Introduction with R. Chapman. Yihui Xie (2015). Dynamic Documents with R and knitr. Chapman. "],["A-links.html", "A Enlaces", " A Enlaces Recursos para el aprendizaje de R ( https://rubenfcasal.github.io/post/ayuda-y-recursos-para-el-aprendizaje-de-r ): A continuación se muestran algunos recursos que pueden ser útiles para el aprendizaje de R y la obtención de ayuda Ayuda online: Ayuda en línea sobre funciones o paquetes: RDocumentation Buscador RSeek StackOverflow Cursos: algunos cursos gratuitos: Coursera: Introducción a Data Science: Programación Estadística con R Mastering Software Development in R DataCamp: Introducción a R Stanford online: Statistical Learning Curso UCA: Introducción a R, R-commander y shiny Udacity: Data Analysis with R Swirl Courses: se pueden hacer cursos desde el propio R con el paquete swirl. Para información sobre cursos en castellano se puede recurrir a la web de R-Hispano en el apartado formación. Algunos de los cursos que aparecen en entradas antiguas son gratuitos. Ver: Cursos MOOC relacionados con R. Libros Iniciación: 2011 - The Art of R Programming. A Tour of Statistical Software Design, (No Starch Press) R for Data Science (online, OReilly) Hands-On Programming with R: Write Your Own Functions and Simulations, by Garrett Grolemund (OReilly) Avanzados: 2008 - Software for Data Analysis: Programming with R - Chambers (Springer) Advanced R by Hadley Wickham (online: 1ª ed, 2ª ed, Chapman &amp; Hall) R packages by Hadley Wickham (online, OReilly) Bookdown: el paquete bookdown de R permite escribir libros empleando R Markdown y compartirlos. En https://bookdown.org está disponible una selección de libros escritos con este paquete (un listado más completo está disponible aquí). Algunos libros en este formato en castellano son: Prácticas de Simulación (disponible en el repositorio de GitHub rubenfcasal/simbook). Escritura de libros con bookdown (disponible en el repositorio de GitHub rubenfcasal/bookdown_intro). R para profesionales de los datos: una introducción. Estadística Básica Edulcorada. Material online: en la web se puede encontrar mucho material adicional, por ejemplo: CRAN: Other R documentation Blogs en inglés: https://www.r-bloggers.com/ https://www.littlemissdata.com/blog/rstudioconf2019 RStudio: https://blog.rstudio.com Microsoft Revolutions: https://blog.revolutionanalytics.com Blogs en castellano: https://www.datanalytics.com http://oscarperpinan.github.io/R http://rubenfcasal.github.io Listas de correo: Listas de distribución de r-project.org: https://stat.ethz.ch/mailman/listinfo Búsqueda en R-help: http://r.789695.n4.nabble.com/R-help-f789696.html Búsqueda en R-help-es: https://r-help-es.r-project.narkive.com https://grokbase.com/g/r/r-help-es Archivos de R-help-es: https://stat.ethz.ch/pipermail/r-help-es "],["A-1-rstudio-links.html", "A.1 RStudio", " A.1 RStudio RStudio: Online learning Webinars sparklyr shiny tidyverse: dplyr tibble tidyr stringr readr Databases using R, dplyr as a database interface CheatSheets: rmarkdown shiny dplyr tidyr stringr "],["B-instalacion.html", "B Instalación de R", " B Instalación de R En la web del proyecto R (www.r-project.org) está disponible mucha información sobre este entorno estadístico. R-project CRAN Las descargas se realizan a través de la web del CRAN (The Comprehensive R Archive Network), con múltiples mirrors: Oficina de software libre (CIXUG) ftp.cixug.es/CRAN. Spanish National Research Network (Madrid) (RedIRIS) es cran.es.r-project.org. "],["B-1-instalación-de-r-en-windows.html", "B.1 Instalación de R en Windows", " B.1 Instalación de R en Windows Seleccionando Download R for Windows y posteriormente base accedemos al enlace con el instalador de R para Windows (actualmente de la versión 3.6.1). B.1.1 Asistente de instalación Durante el proceso de instalación la recomendación (para evitar posibles problemas) es seleccionar ventanas simples SDI en lugar de múltiples ventanas MDI (hay que utilizar opciones de configuración). Una vez terminada la instalación, al abrir el programa R, aparece la ventana de la consola (simula una ventana de comandos de Unix) que permite ejecutar comandos de R al irlos introduciendo. B.1.2 Instalación de paquetes Después de la instalación de R, puede ser necesario instalar paquetes adicionales. Para ejecutar los ejemplos mostrados en el libro será necesario tener instalados los siguientes paquetes: lattice, ggplot2, foreign, car, leaps, MASS, RcmdrMisc, lmtest, glmnet, mgcv, rmarkdown, knitr, dplyr. Por ejemplo mediante el comando: pkgs &lt;- c(&quot;lattice&quot;, &quot;ggplot2&quot;, &quot;foreign&quot;, &quot;car&quot;, &quot;leaps&quot;, &quot;MASS&quot;, &quot;RcmdrMisc&quot;, &quot;lmtest&quot;, &quot;glmnet&quot;, &quot;mgcv&quot;, &quot;rmarkdown&quot;, &quot;knitr&quot;, &quot;dplyr&quot;) install.packages(setdiff(pkgs, installed.packages()[,&quot;Package&quot;]), dependencies = TRUE) # Si aparecen errores debidos a incompatibilidades entre las versiones de los paquetes, # probar a ejecutar en lugar de lo anterior: # install.packages(pkgs, dependencies=TRUE) # Instala todos... (puede que haya que seleccionar el repositorio de descarga, e.g. Spain (Madrid)). La forma tradicional es esta: Se inicia R y se selecciona Paquetes &gt; Instalar paquetes Se selecciona el repositorio. Se selecciona el paquete y automáticamente se instala. Alternativamente se podrían instalar los siguientes paquetes: Rcmdr, RcmdrPlugin.FactoMineR, dplyr y rattle, ya que sus dependencias incluyen los empleados en este libro. La instalación de los paquetes Rcmdr y rattle (que incluyen interfaces gráficas) se describe en el Apéndice C. "],["C-interfaces.html", "C Interfaces gráficas", " C Interfaces gráficas Aunque la consola de R dispone de un editor básico de códido (script), puede ser recomendable trabajar con un editor de comandos más cómodo y flexible. En los últimos años han surgido interfaces gráficas que permiten realizar las operaciones más comunes a través de periféricos como el ratón. Una lista de de estas interfaces puede ser encontrada en www.sciviews.org/SciViews-R "],["C-1-rstudio.html", "C.1 RStudio", " C.1 RStudio Un entorno de R muy recomendable es el RStudio, http://rstudio.org: Para instalarlo descargar el archivo de instalación de http://rstudio.org/download/desktop. "],["C-2-rcmdr.html", "C.2 RCommander", " C.2 RCommander RCommander es una de las interfaces más populares para R. Algunas de sus ventajas son: Se distribuye también bajo licencia GPL de GNU Fácil instalación Numerosa documentación en castellano Adecuado para la iniciación en la Estadística Introduce a la programación de R al mostrar el código asociado a las acciones de los menús. C.2.1 Instalación de R-Commander Por ejemplo, la instalación de la interfaz gráfica R-Commander se puede hacer directamente desde la ventana de consola tecleando &gt; install.packages(&quot;Rcmdr&quot;) Otra posibilidad es seleccionar el menú Paquetes e Instalar paquetes... A continuación se abrirá una nueva ventana con todos los posibles espejos, donde conviene seleccionar el espejo de Madrid. Una vez elegido el espejo (figura de la izquierda) se seleccionará el paquete Rcmdr (figura de la derecha). El programa R realizará la correspondiente instalación y, una vez finalizada, mostrará la pantalla de consola. Entonces se escribe en la consola &gt;library(Rcmdr) y se abrirá la siguiente ventana de R-Commander. La ayuda sobre este paquete se obtiene con &gt;help(package=&quot;Rcmdr&quot;) "],["D-manipulación-de-datos-con-dplyr.html", "D Manipulación de datos con dplyr ", " D Manipulación de datos con dplyr "],["D-1-el-paquete-dplyr.html", "D.1 El paquete dplyr", " D.1 El paquete dplyr library(dplyr) dplyr Permite sustituir funciones base de R (como split(), subset(), apply(), sapply(), lapply(), tapply() y aggregate()) mediante una gramática más sencilla para la manipulación de datos: select() seleccionar variables/columnas (también rename()). mutate() crear variables/columnas (también transmute()). filter() seleccionar casos/filas (también slice()). arrange() ordenar o organizar casos/filas. summarise() resumir valores. group_by() permite operaciones por grupo empleando el concepto dividir-aplicar-combinar (ungroup() elimina el agrupamiento). Puede trabajar con conjuntos de datos en distintos formatos: data.frame, data.table, tibble,  bases de datos relacionales (lenguaje SQL),  bases de datos Hadoop (paquete plyrmr). En lugar de operar sobre vectores como las funciones base, opera sobre objetos de este tipo (solo nos centraremos en data.frame). D.1.1 Datos de ejemplo El fichero empleados.RData contiene datos de empleados de un banco. Supongamos por ejemplo que estamos interesados en estudiar si hay discriminación por cuestión de sexo o raza. load(&quot;datos/empleados.RData&quot;) data.frame(Etiquetas = attr(empleados, &quot;variable.labels&quot;)) # Listamos las etiquetas ## Etiquetas ## id Código de empleado ## sexo Sexo ## fechnac Fecha de nacimiento ## educ Nivel educativo (años) ## catlab Categoría Laboral ## salario Salario actual ## salini Salario inicial ## tiempemp Meses desde el contrato ## expprev Experiencia previa (meses) ## minoria Clasificación étnica ## sexoraza Clasificación por sexo y raza attr(empleados, &quot;variable.labels&quot;) &lt;- NULL # Eliminamos las etiquetas para que no molesten... "],["D-2-operaciones-con-variables-columnas.html", "D.2 Operaciones con variables (columnas)", " D.2 Operaciones con variables (columnas) D.2.1 Seleccionar variables con select() emplea2 &lt;- select(empleados, c(&quot;id&quot;, &quot;sexo&quot;, &quot;fechnac&quot;, &quot;educ&quot;, &quot;catlab&quot;, &quot;salario&quot;, &quot;salini&quot;, &quot;tiempemp&quot;)) head(emplea2) ## id sexo fechnac educ catlab salario salini tiempemp ## 1 1 Hombre 1952-02-03 15 Directivo 57000 27000 98 ## 2 2 Hombre 1958-05-23 16 Administrativo 40200 18750 98 ## 3 3 Mujer 1929-07-26 12 Administrativo 21450 12000 98 ## 4 4 Mujer 1947-04-15 8 Administrativo 21900 13200 98 ## 5 5 Hombre 1955-02-09 15 Administrativo 45000 21000 98 ## 6 6 Hombre 1958-08-22 15 Administrativo 32100 13500 98 Se puede cambiar el nombre (ver también ?rename()) head(select(empleados, sexo, noblanca = minoria, salario)) ## sexo noblanca salario ## 1 Hombre No 57000 ## 2 Hombre No 40200 ## 3 Mujer No 21450 ## 4 Mujer No 21900 ## 5 Hombre No 45000 ## 6 Hombre No 32100 Se pueden emplear los nombres de variables como índices: head(select(empleados, sexo:salario)) ## sexo fechnac educ catlab salario ## 1 Hombre 1952-02-03 15 Directivo 57000 ## 2 Hombre 1958-05-23 16 Administrativo 40200 ## 3 Mujer 1929-07-26 12 Administrativo 21450 ## 4 Mujer 1947-04-15 8 Administrativo 21900 ## 5 Hombre 1955-02-09 15 Administrativo 45000 ## 6 Hombre 1958-08-22 15 Administrativo 32100 head(select(empleados, -(sexo:salario))) ## id salini tiempemp expprev minoria sexoraza ## 1 1 27000 98 144 No Blanca varón ## 2 2 18750 98 36 No Blanca varón ## 3 3 12000 98 381 No Blanca mujer ## 4 4 13200 98 190 No Blanca mujer ## 5 5 21000 98 138 No Blanca varón ## 6 6 13500 98 67 No Blanca varón Hay opciones para considerar distintos criterios: starts_with(), ends_with(), contains(), matches(), one_of() (ver ?select). head(select(empleados, starts_with(&quot;s&quot;))) ## sexo salario salini sexoraza ## 1 Hombre 57000 27000 Blanca varón ## 2 Hombre 40200 18750 Blanca varón ## 3 Mujer 21450 12000 Blanca mujer ## 4 Mujer 21900 13200 Blanca mujer ## 5 Hombre 45000 21000 Blanca varón ## 6 Hombre 32100 13500 Blanca varón D.2.2 Generar nuevas variables con mutate() emplea2 &lt;- mutate(emplea2, incsal = empleados[,&quot;salario&quot;] - empleados[,&quot;salini&quot;]) head(emplea2) ## id sexo fechnac educ catlab salario salini tiempemp incsal ## 1 1 Hombre 1952-02-03 15 Directivo 57000 27000 98 30000 ## 2 2 Hombre 1958-05-23 16 Administrativo 40200 18750 98 21450 ## 3 3 Mujer 1929-07-26 12 Administrativo 21450 12000 98 9450 ## 4 4 Mujer 1947-04-15 8 Administrativo 21900 13200 98 8700 ## 5 5 Hombre 1955-02-09 15 Administrativo 45000 21000 98 24000 ## 6 6 Hombre 1958-08-22 15 Administrativo 32100 13500 98 18600 head(mutate(emplea2, tsal = empleados[,&quot;salario&quot;] - empleados[,&quot;salini&quot;]/emplea2[,&quot;tiempemp&quot;])) ## id sexo fechnac educ catlab salario salini tiempemp incsal ## 1 1 Hombre 1952-02-03 15 Directivo 57000 27000 98 30000 ## 2 2 Hombre 1958-05-23 16 Administrativo 40200 18750 98 21450 ## 3 3 Mujer 1929-07-26 12 Administrativo 21450 12000 98 9450 ## 4 4 Mujer 1947-04-15 8 Administrativo 21900 13200 98 8700 ## 5 5 Hombre 1955-02-09 15 Administrativo 45000 21000 98 24000 ## 6 6 Hombre 1958-08-22 15 Administrativo 32100 13500 98 18600 ## tsal ## 1 56724.49 ## 2 40008.67 ## 3 21327.55 ## 4 21765.31 ## 5 44785.71 ## 6 31962.24 "],["D-3-operaciones-con-casos-filas.html", "D.3 Operaciones con casos (filas)", " D.3 Operaciones con casos (filas) D.3.1 Seleccionar casos con filter() head(filter(emplea2, sexo == &quot;Mujer&quot;, empleados[,&quot;minoria&quot;] == &quot;Sí&quot;)) ## id sexo fechnac educ catlab salario salini tiempemp incsal ## 1 14 Mujer 1949-02-26 15 Administrativo 35100 16800 98 18300 ## 2 23 Mujer 1965-03-15 15 Administrativo 24000 11100 97 12900 ## 3 24 Mujer 1933-03-27 12 Administrativo 16950 9000 97 7950 ## 4 25 Mujer 1942-07-01 15 Administrativo 21150 9000 97 12150 ## 5 40 Mujer 1933-08-28 15 Administrativo 19200 9000 96 10200 ## 6 41 Mujer 1961-03-18 12 Administrativo 23550 11550 96 12000 D.3.2 Organizar casos con arrange() head(arrange(emplea2, salario)) ## id sexo fechnac educ catlab salario salini tiempemp incsal ## 1 378 Mujer 1930-09-21 8 Administrativo 15750 10200 70 5550 ## 2 338 Mujer 1938-08-12 8 Administrativo 15900 10200 74 5700 ## 3 90 Mujer 1938-02-27 8 Administrativo 16200 9750 92 6450 ## 4 224 Mujer 1934-11-20 12 Administrativo 16200 10200 82 6000 ## 5 411 Mujer 1931-08-21 12 Administrativo 16200 10200 68 6000 ## 6 448 Mujer 1933-06-05 12 Administrativo 16350 10200 66 6150 head(arrange(emplea2, desc(salini), salario)) ## id sexo fechnac educ catlab salario salini tiempemp incsal ## 1 29 Hombre 1944-01-28 19 Directivo 135000 79980 96 55020 ## 2 343 Hombre 1953-06-09 16 Directivo 103500 60000 73 43500 ## 3 205 Hombre 1944-06-22 16 Directivo 66750 52500 83 14250 ## 4 160 Hombre 1951-08-27 16 Directivo 66000 47490 86 18510 ## 5 431 Hombre 1959-01-15 18 Directivo 86250 45000 66 41250 ## 6 32 Hombre 1954-01-28 19 Directivo 110625 45000 96 65625 "],["D-4-resumir-valores-con-summarise.html", "D.4 Resumir valores con summarise()", " D.4 Resumir valores con summarise() summarise(empleados, sal.med = mean(salario), n = n()) ## sal.med n ## 1 34419.57 474 "],["D-5-agrupar-casos-con-group-by.html", "D.5 Agrupar casos con group_by()", " D.5 Agrupar casos con group_by() summarise(group_by(empleados, sexo, minoria), sal.med = mean(salario), n = n()) ## # A tibble: 4 x 4 ## # Groups: sexo [2] ## sexo minoria sal.med n ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Hombre No 44475. 194 ## 2 Hombre Sí 32246. 64 ## 3 Mujer No 26707. 176 ## 4 Mujer Sí 23062. 40 "],["D-6-operador-pipe-tubería-redirección.html", "D.6 Operador pipe %&gt;% (tubería, redirección)", " D.6 Operador pipe %&gt;% (tubería, redirección) Este operador le permite canalizar la salida de una función a la entrada de otra función. segundo(primero(datos)) se traduce en datos %&gt;% primero %&gt;% segundo (lectura de funciones de izquierda a derecha). Ejemplos: empleados %&gt;% filter(catlab == &quot;Directivo&quot;) %&gt;% group_by(sexo, minoria) %&gt;% summarise(sal.med = mean(salario), n = n()) ## # A tibble: 3 x 4 ## # Groups: sexo [2] ## sexo minoria sal.med n ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Hombre No 65684. 70 ## 2 Hombre Sí 76038. 4 ## 3 Mujer No 47214. 10 empleados %&gt;% select(sexo, catlab, salario) %&gt;% filter(catlab != &quot;Seguridad&quot;) %&gt;% group_by(catlab) %&gt;% mutate(saldif = salario - mean(salario)) %&gt;% ungroup() %&gt;% boxplot(saldif ~ sexo*droplevels(catlab), data = .) abline(h = 0, lty = 2) Para mas información sobre dplyr ver por ejemplo la vignette del paquete: Introduction to dplyr. "],["E-compañías-que-usan-r.html", "E Compañías que usan R", " E Compañías que usan R Cada vez son más las empresas que utilizan R. Grupo de empresas que apoyan a la Fundación R y a la comunidad R. Otras compañías: Facebook, Twitter, Bank of America, Monsanto,  "],["E-1-microsoft.html", "E.1 Microsoft", " E.1 Microsoft Diseñado para entornos Big Data y computación de altas prestaciones. Versión de R con rendimiento mejorado. Microsoft R Application Network: MRAN: https://mran.microsoft.com Integracion de R con: SQL Server, PowerBI, Azure y Cortana Analytics. "],["E-2-rstudio-com.html", "E.2 RStudio", " E.2 RStudio Además del entorno de desarrollo (IDE) con múltiples herramientas, descrito en el Apéndice C.1: Interfaz web que permite ejecutar RStudio en el servidor. Evita el movimiento de datos a los clientes. Ediciones Open Source y Professional. Compañía muy activa en el desarrollo de R: Múltiples paquetes: Shiny, rmarkdown, knitr, ggplot2, dplyr, tidyr,  Hadley Wickham (Jefe científico de RStudio). Ver enlaces en el Apéndice A.1. "],["F-intro-AE.html", "F Introducción al Aprendizaje Estadístico", " F Introducción al Aprendizaje Estadístico (PART) Aprendizaje estadístico con R {-} Para ejecutar los ejemplos mostrados en el libro sería necesario tener instalados los siguientes paquetes: caret, rattle, car, leaps, MASS, RcmdrMisc, lmtest, glmnet, mgcv, np, NeuralNetTools, AppliedPredictiveModeling, ISLR. Por ejemplo mediante los siguientes comandos: pkgs &lt;- c(&quot;caret&quot;, &quot;rattle&quot;, &quot;car&quot;, &quot;leaps&quot;, &quot;MASS&quot;, &quot;RcmdrMisc&quot;, &quot;lmtest&quot;, &quot;glmnet&quot;, &quot;mgcv&quot;, &quot;np&quot;, &quot;NeuralNetTools&quot;, &quot;AppliedPredictiveModeling&quot;, &quot;ISLR&quot;) install.packages(setdiff(pkgs, installed.packages()[,&quot;Package&quot;]), dependencies = TRUE) # Si aparecen errores (normalmente debidos a incompatibilidades con versiones ya instaladas), # probar a ejecutar en lugar de lo anterior: # install.packages(pkgs, dependencies=TRUE) # Instala todos... La denominada Ciencia de Datos (Data Science; también denominada Science of Learning) se ha vuelto muy popular hoy en día. Se trata de un campo multidisciplicar, con importantes aportaciones estadísticas e informáticas, dentro del que se incluirían disciplinas como Minería de Datos (Data Mining), Aprendizaje Automático (Machine Learning), Aprendizaje Profundo (Deep Learning), Modelado Predictivo (Predictive Modeling), Extracción de Conocimiento (Knowlegde Discovery) y también el Aprendizaje Estadístico (Statistical Learning). Podríamos definir la Ciencia de Datos como el conjunto de conocimientos y herramientas utilizados en las distintas etapas del análisis de datos (ver Figura 14.1). Otras definiciones podrían ser: El arte y la ciencia del análisis inteligente de los datos. El conjunto de herramientas para entender y modelizar conjuntos (complejos) de datos. El proceso de descubrir patrones y obtener conocimiento a partir de grandes conjuntos de datos (Big Data). Aunque esta ciencia incluiría también la gestión (sin olvidarnos del proceso de obtención) y la manipulación de los datos. Figura 14.1: Etapas del proceso Una de estas etapas (que están interrelacionadas) es la construcción de modelos a partir de los datos para aprender y predecir. Podríamos decir que el Aprendizaje Estadístico (AE) se encarga de este problema desde el punto de vista estadístico. En Estadística se consideran modelos estocásticos (con componente aleatoria), para tratar de tener en cuenta la incertidumbre debida a que no se disponga de toda la información (sobre las variables que influyen en el fenómeno de interés). Nothing in Nature is random a thing appears random only through the incompleteness of our knowledge.  Spinoza, Baruch (Ethics, 1677) To my mind, although Spinoza lived and thought long before Darwin, Freud, Einstein, and the startling implications of quantum theory, he had a vision of truth beyond what is normally granted to human beings.  Shirley, Samuel (Complete Works, 2002). Traductor de la obra completa de Spinoza al inglés. La Inferencia Estadística proporciona herramientas para ajustar este tipo de modelos a los datos observados (seleccionar un modelo adecuado, estimar sus parámetros y contrastar su validez). Sin embargo, en la aproximación estadística clásica como primer objetivo se trata de explicar por completo lo que ocurre en la población y suponiendo que esto se puede hacer con modelos tratables analíticamente, emplear resultados teóricos (típicamente resultados asintóticos) para realizar inferencias (entre ellas la predicción). Los avances en computación han permitido el uso de modelos estadísticos más avanzados, principalmente métodos no paramétricos, muchos de los cuales no pueden ser tratados analíticamente (por lo menos no por completo o no inicialmente), este es el campo de la Estadística Computacional1. Desde este punto de vista, el AE se enmarcaría dentro del campo de la Estadística Computacional. Cuando pensamos en AE pensamos en: Flexibilidad (hay menos suposiciones sobre los datos). Procesamiento automático de datos. Big Data (en el sentido amplio, donde big puede hacer referencia a datos complejos). Predicción. Por el contrario, muchos de los métodos del AE no se preocupan (o se preocupan poco) por: Reproducibilidad. Cuantificación de la incertidumbre (en términos de probabilidad). Inferencia. La idea es dejar hablar a los datos y no encorsetarlos a priori, dándoles mayor peso que a los modelos. Sin embargo, esta aproximación puede presentar diversos inconvenientes: Algunos métodos son poco interpretables (se sacrifica la interpretabilidad por la precisión de las predicciones). Pueden aparecer problemas de sobreajuste (overfitting; en los métodos estadísticos clásicos es más habitual que aparezcan problemas de infraajuste, underfitting). Pueden presentar más problemas al extrapolar o interpolar (en comparación con los métodos clásicos). Lauro (1996) definió la Estadística Computacional como la disciplina que tiene como objetivo diseñar algoritmos para implementar métodos estadísticos en computadoras, incluidos los impensables antes de la era de las computadoras (por ejemplo, bootstrap, simulación), así como hacer frente a problemas analíticamente intratables. "],["F-1-aprendizaje-estadístico-vs-aprendizaje-automático.html", "F.1 Aprendizaje Estadístico vs. Aprendizaje Automático", " F.1 Aprendizaje Estadístico vs. Aprendizaje Automático El término Machine Learning (ML; Aprendizaje Automático) se utiliza en el campo de la Intelingencia Artificial desde 1959 para hacer referencia, fundamentalmente, a algoritmos de predicción (inicialmente para reconocimiento de patrones). Muchas de las herramientas que utilizan provienen del campo de la Estadística y, en cualquier caso, la Estadística (y por tanto las Matemáticas) es la base de todos estos enfoques para analizar datos (y no conviene perder la base formal). Por este motivo desde la Estadística Computacional se introdujo el término Statistical Learning (Aprendizaje Estadístico) para hacer referencia a este tipo de herramientas, pero desde el punto de vista estadístico (teniendo en cuenta la incertidumbre debida a no disponer de toda la información). Tradicionalmente ML no se preocupa del origen de los datos e incluso es habitual que se considere que un conjunto enorme de datos es equivalente a disponer de toda la información (i.e. a la población). The sheer volume of data would obviate the need of theory and even scientific method  Chris Anderson, físico y periodista, 2008 Por el contrario en el caso del AE se trata de comprender, si es posible, el proceso subyacente del que provienen los datos y si estos son representativos de la población de interés (i.e. si tienen algún tipo de sesgo). No obstante, en este libro se considerará en general ambos términos como sinónimos. ML/AE hacen un importante uso de la programación matemática, ya que muchos de sus problemas se plantean en términos de la optimización de funciones bajo restricciones. Recíprocamente, en optimización también se utilizan algoritmos de ML/AE. F.1.1 Machine Learning vs. Data Mining Mucha gente utiliza indistintamente los nombres ML y Data Mining (DM). Sin embargo, aunque tienen mucho solapamiento, lo cierto es que hacen referencia a conceptos ligeramente distintos. ML es un conjunto de algoritmos principalmente dedicados a hacer predicciones y que son esencialmente automáticos minimizando la intervención humana. DM intenta entender conjuntos de datos (en el sentido de encontrar sus patrones), requiere de una intervención humana activa (al igual que la Inferencia Estadística tradicional), pero utiliza entre otras las técnicas automáticas de ML. Por tanto podríamos pensar que es más parecido al AE. F.1.2 Las dos culturas (Breiman, 2001) Breiman diferencia dos objetivos en el análisis de datos, que él llama información (en el sentido de inferencia) y predicción. Cada uno de estos objetivos da lugar a una cultura: Modelización de datos: desarrollo de modelos (estocásticos) que permitan ajustar los datos y hacer inferencia. Es el trabajo habitual de los estadísticos académicos. Modelización algorítmica (en el sentido de predictiva): esta cultura no está interesada en los mecanismos que generan los datos, sólo en los algoritmos de predicción. Es el trabajo habitual de muchos estadísticos industriales y de muchos ingenieros informáticos. El ML es el núcleo de esta cultura que pone todo el énfasis en la precisión predictiva (así, un importante elemento dinamizador son las competiciones entre algoritmos predictivos, al estilo del Netflix Challenge). F.1.3 Machine Learning vs. Estadística (Dunson, 2018) Machine learning: The main publication outlets tend to be peer-reviewed conference proceedings and the style of research is very fast paced, trendy, and driven by performance metrics in prediction and related tasks. Statistical community: The main publication outlets are peer-reviewed journals, most of which have a long drawn out review process, and the style of research tends to be careful, slower paced, intellectual as opposed to primarily performance driven, emphasizing theoretical support (e.g., through asymptotic properties), under-stated, and conservative. Big data in ML typically means that the number of examples (i.e. sample size) is very large. In statistics () it has become common to collect high dimensional, complex and intricately structured data. Often the dimensionality of the data vastly exceeds the available sample size, and the fundamental challenge of the statistical analysis is obtaining new insights from these huge data, while maintaining reproducibility/replicability and reliability of the results. "],["F-2-métodos-de-aprendizaje-estadístico.html", "F.2 Métodos de Aprendizaje Estadístico", " F.2 Métodos de Aprendizaje Estadístico Dentro de los problemas que aborda el Aprendizaje Estadístico se suelen diferenciar dos grandes bloques: el aprendizaje no supervisado y el supervisado. El aprendizaje no supervisado comprende los métodos exploratorios, es decir, aquellos en los que no hay una variable respuesta (al menos no de forma explícita). El principal objetivo de estos métodos es entender las relaciones entre los datos y su estructura, y pueden clasificarse en las siguientes categorías: Análisis descriptivo. Métodos de reducción de la dimensión (análisis de componentes principales, análisis factorial). Clúster. Detección de datos atípicos. El aprendizaje supervisado engloba los métodos predictivos, en los que una de las variables está definida como variable respuesta. Su principal objetivo es la construcción de modelos que posteriormente se utilizarán, sobre todo, para hacer predicciones. Dependiendo del tipo de variable respuesta se diferencia entre: Clasificación: respuesta categórica (también se emplea la denominación de variable cualitativa, discreta o factor). Regresión: respuesta numérica (cuantitativa). En este libro nos centraremos únicamente en el campo del aprendizaje supervisado y combinaremos la terminología propia de la Estadística con la empleada en AE (por ejemplo, en Estadística es habitual considerar un problema de clasificación como un caso particular de regresión). F.2.1 Notación y terminología Denotaremos por \\(\\mathbf{X}=(X_1, X_2, \\ldots, X_p)\\) al vector formado por las variables predictoras (variables explicativas o variables independientes; también inputs o features en la terminología de ML), cada una de las cuales podría ser tanto numérica como categórica2. En general (ver comentarios más adelante), emplearemos \\(Y\\left(\\mathbf{X} \\right)\\) para referirnos a la variable objetivo (variable respuesta o variable dependiente; también output en la terminología de ML), que como ya se comentó puede ser una variable numérica (regresión) o categórica (clasificación). Supondremos que el objetivo principal es, a partir de una muestra: \\[\\left\\{ \\left( x_{1i}, \\ldots, x_{pi}, y_{i} \\right) : i = 1, \\ldots, n \\right\\},\\] obtener (futuras) predicciones \\(\\hat Y\\left(\\mathbf{x} \\right)\\) de la respuesta para \\(\\mathbf{X}=\\mathbf{x}=\\left(x_{1}, \\ldots, x_{p}\\right)\\). En regresión consideraremos como base el siguiente modelo general (podría ser después de una transformación de la respuesta): \\[\\begin{equation} Y(\\mathbf{X})=m(\\mathbf{X})+\\varepsilon, \\tag{F.1} \\end{equation}\\] donde \\(m(\\mathbf{x}) = E\\left( \\left. Y\\right\\vert_{\\mathbf{X}=\\mathbf{x}} \\right)\\) es la media condicional, denominada función de regresión (o tendencia), y \\(\\varepsilon\\) es un error aleatorio de media cero y varianza \\(\\sigma^2\\), independiente de \\(\\mathbf{X}\\). Este modelo puede generalizarse de diversas formas, por ejemplo, asumiendo que la distribución del error depende de \\(X\\) (considerando \\(\\varepsilon(\\mathbf{X})\\) en lugar de \\(\\varepsilon\\)) podríamos incluir dependencia y heterocedasticidad. En estos casos normalmente se supone que lo hace únicamente a través de la varianza (error heterocedástico independiente), denotando por \\(\\sigma^2(\\mathbf{x}) = Var\\left( \\left. Y\\right\\vert_{\\mathbf{X}=\\mathbf{x}} \\right)\\) la varianza condicional3. Como ya se comentó se podría considerar clasificación como un caso particular, por ejemplo definiendo \\(Y\\left(\\mathbf{X} \\right)\\) de forma que tome los valores \\(1, 2, \\ldots, K\\), etiquetas que identifican las \\(K\\) posibles categorías (también se habla de modalidades, niveles, clases o grupos). Sin embargo, muchos métodos de clasificación emplean variables auxiliares (variables dummy), indicadoras de las distintas categorías, y emplearemos la notación anterior para referirnos a estas variables (también denominadas variables target). En cuyo caso, denotaremos por \\(G \\left(\\mathbf{X} \\right)\\) la respuesta categórica (la clase verdadera; \\(g_i\\), \\(i =1, \\ldots, n\\), serían los valores observados) y por \\(\\hat G \\left(\\mathbf{X} \\right)\\) el predictor. Por ejemplo, en el caso de dos categorías, se suele definir \\(Y\\) de forma que toma el valor 1 en la categoría de interés (también denominada éxito o resultado positivo) y 0 en caso contrario (fracaso o resultado negativo)4. Además, en este caso, los modelos típicamente devuelven estimaciones de la probabilidad de la clase de interés en lugar de predecir directamente la clase, por lo que se empleará \\(\\hat p\\) en lugar de \\(\\hat Y\\). A partir de esa estimación se obtiene una predicción de la categoría. Normalmente se predice la clase más probable, i.e. éxito si \\(\\hat p(\\mathbf{x}) &gt; c = 0.5\\) y fracaso en caso contrario (con probabilidad estimada \\(1 - \\hat p(\\mathbf{x})\\)). Resulta claro que el modelo base general (F.1) puede no ser adecuado para modelar variables indicadoras (o probabilidades). Muchos de los métodos de AE emplean (F.1) para una variable auxiliar numérica (denominada puntuación o score) que se transforma a escala de probabilidades mediante la función logística (denominada función sigmoidal, sigmoid function, en ML)5: \\[p(s) = \\frac{1}{1 + e^{-s}},\\] cuya inversa es la función logit: \\[\\operatorname{logit}(p)=\\log\\left( \\frac{p}{1-p} \\right).\\] Lo anterior se puede generalizar para el caso de múltiples categorías, considerando variables indicadoras de cada categoría \\(Y_1, \\ldots, Y_K\\) (es lo que se conoce como la estrategia de uno contra todos). En este caso típicamente: \\[\\hat G \\left(\\mathbf{x} \\right) = \\underset{k}{\\operatorname{argmax}} \\left\\{ \\hat p_k(\\mathbf{x}) : k = 1, 2, \\ldots, K \\right\\}.\\] F.2.2 Métodos (de aprendizaje supervisado) y paquetes de R Hay una gran cantidad de métodos de aprendizaje supervisado implementados en centenares de paquetes de R (ver por ejemplo CRAN Task View: Machine Learning &amp; Statistical Learning). A continuación se muestran los principales métodos y algunos de los paquetes de R que los implementan (muchos son válidos para regresión y clasificación, como por ejemplo los basados en árboles, aunque aquí aparecen en su aplicación habitual). Métodos de Clasificación: Análisis discriminante (lineal, cuadrático), Regresión logística, multinomial: stats, MASS Árboles de decisión, bagging, random forest, boosting: rpart, party, C50, Cubist, randomForest, adabag, xgboost Support vector machines (SVM): kernlab, e1071 Métodos de regresión: Modelos lineales: Regresión lineal: lm(), lme(), biglm Regresión lineal robusta: MASS::rlm() Métodos de regularización (Ridge regression, Lasso): glmnet, elasticnet Modelos lineales generalizados: glm(), bigglm, .. Modelos paramétricos no lineales: nls(), nlme Regresión local (vecinos más próximos y métodos de suavizado): kknn, loess(), KernSmooth, sm, np Modelos aditivos generalizados (GAM): mgcv, gam Redes neuronales: nnet También existen paquetes de R que permiten utilizar plataformas de ML externas, como por ejemplo h2o o RWeka. Como todos estos paquetes emplean opciones, estructuras y convenciones sintácticas diferentes, se han desarrollado paquetes que proporcionan interfaces unificadas a muchas de estas implementaciones. Entre ellos podríamos citar caret,mlr3 y tidymodels. En la Sección F.6 se incluye una breve introducción al paquete caret que será empleado en diversas ocasiones a lo largo del presente libro. Adicionalmente hay paquetes de R que disponen de entornos gráficos que permiten emplear estos métodos evitando el uso de comandos. Entre ellos estarían R-Commander con el plugin FactoMineR (Rcmdr, RcmdrPlugin.FactoMineR) y rattle. Aunque hay que tener en cuenta que algunos métodos están diseñados para predictores numéricos, otros para categóricos y algunos para ambos tipos. Por ejemplo considerando en el modelo base \\(\\sigma(\\mathbf{X})\\varepsilon\\) como termino de error y suponiendo adicionalmente que \\(\\varepsilon\\) tiene varianza uno. Otra alternativa sería emplear 1 y -1, algo que simplifica las expresiones de algunos métodos. De especial interés en regresión logística y en redes neuronales artificiales. "],["F-3-const-eval.html", "F.3 Construcción y evaluación de los modelos", " F.3 Construcción y evaluación de los modelos En Inferencia Estadística clásica el procedimiento habitual es emplear toda la información disponible para construir un modelo válido (que refleje de la forma más fiel posible lo que ocurre en la población) y asumiendo que el modelo es el verdadero (lo que en general sería falso) utilizar métodos de inferencia para evaluar su precisión. Por ejemplo, en el caso de regresión lineal múltiple, el coeficiente de determinación ajustado sería una medida del la precisión del modelo para predecir nuevas observaciones (no se debería emplear el coeficiente de determinación sin ajustar; aunque, en cualquier caso, su validez dependería de la de las suposiciones estructurales del modelo). Alternativamente, en Estadística Computacional es habitual emplear técnicas de remuestreo para evaluar la precisión (entrenando también el modelo con todos los datos disponibles), principalmente validación cruzada (leave-one-out, k-fold), jackniffe o bootstrap. Por otra parte, como ya se comentó, algunos de los modelos empleados en AE son muy flexibles (están hiperparametrizados) y pueden aparecer problemas si se permite que se ajusten demasiado bien a las observaciones (podrían llegar a interpolar los datos). En estos casos habrá que controlar el procedimiento de aprendizaje, típicamente a traves de parámetros relacionados con la complejidad del modelo (ver sección siguiente). En AE se distingue entre parámetros estructurales, los que van a ser estimados al ajustar el modelo a los datos (en el entrenamiento), e hiperparámetros (tuning parameters o parámetros de ajuste), que imponen restricciones al aprendizaje del modelo (por ejemplo determinando el número de parámetros estructurales). Si los hiperparámetros seleccionados producen un modelo demasiado complejo aparecerán problemas de sobreajuste (overfitting) y en caso contrario de infraajuste (undefitting). Hay que tener en cuenta también que al aumentar la complejidad disminuye la interpretabilidad de los modelos. Se trataría entonces de conseguir buenas predicciones (habrá que evaluar la capacidad predictiva) con el modelo más sencillo posible. F.3.1 Equilibrio entre sesgo y varianza: infraajuste y sobreajuste La idea es que queremos aprender más allá de los datos empleados en el entrenamiento (en Estadística diríamos que queremos hacer inferencia sobre nuevas observaciones). Como ya se comentó, en AE hay que tener especial cuidado con el sobreajuste. Este problema ocurre cuando el modelo se ajusta demasiado bien a los datos de entrenamiento pero falla cuando se utiliza en un nuevo conjunto de datos (nunca antes visto). Como ejemplo ilustrativo emplearemos regresión polinómica, considerando el grado del polinomio como un hiperparámetro que determina la complejidad del modelo. En primer lugar simulamos una muestra y ajustamos modelos polinómicos con distintos grados de complejidad. # Simulación datos n &lt;- 30 x &lt;- seq(0, 1, length = n) mu &lt;- 2 + 4*(5*x - 1)*(4*x - 2)*(x - 0.8)^2 # grado 4 sd &lt;- 0.5 set.seed(1) y &lt;- mu + rnorm(n, 0, sd) plot(x, y) lines(x, mu, lwd = 2) # Ajuste de los modelos fit1 &lt;- lm(y ~ x) lines(x, fitted(fit1)) fit2 &lt;- lm(y ~ poly(x, 4)) lines(x, fitted(fit2), lty = 2) fit3 &lt;- lm(y ~ poly(x, 20)) # NOTA: poly(x, degree, raw = FALSE) tiene un problema de desbordamiento si degree &gt; 25 lines(x, fitted(fit3), lty = 3) legend(&quot;topright&quot;, legend = c(&quot;Verdadero&quot;, &quot;Ajuste con grado 1&quot;, &quot;Ajuste con grado 4&quot;, &quot;Ajuste con grado 20&quot;), lty = c(1, 1, 2, 3), lwd = c(2, 1, 1, 1)) Figura F.1: Muestra (simulada) y ajustes polinómicos con distinta complejidad. Como se observa en la Figura F.1 al aumentar la complejidad del modelo se consigue un mejor ajuste a los datos observados (empleados en el entrenamiento), a costa de un incremento en la variabilidad de las predicciones, lo que puede producir un mal comportamiento del modelo a ser empleado en un conjunto de datos distinto del observado. Si calculamos medidas de bondad de ajuste, como el error cuadrático medio (MSE) o el coeficiente de determinación, se obtienen mejores resultados al aumentar la complejidad. Como se trata de modelos lineales, podríamos obtener también el coeficiente de determinación ajustado, que sería preferible (en principio, ya que dependería de la validez de las hipótesis estructurales del modelo) para medir la precisión al emplear los modelos en un nuevo conjunto de datos. knitr::kable(t(sapply(list(fit1 = fit1, fit2 = fit2, fit3 = fit3), function(x) with(summary(x), c(MSE = mean(residuals^2), R2 = r.squared, R2adj = adj.r.squared)))), digits = 2) MSE R2 R2adj fit1 1.22 0.20 0.17 fit2 0.19 0.87 0.85 fit3 0.07 0.95 0.84 Por ejemplo, si generamos nuevas respuestas de este proceso, la precisión del modelo más complejo empeorará considerablemente: y.new &lt;- mu + rnorm(n, 0, sd) plot(x, y) points(x, y.new, pch = 2) lines(x, mu, lwd = 2) lines(x, fitted(fit1)) lines(x, fitted(fit2), lty = 2) lines(x, fitted(fit3), lty = 3) legend(&quot;topright&quot;, legend = c(&quot;Verdadero&quot;, &quot;Muestra&quot;, &quot;Ajuste con grado 1&quot;, &quot;Ajuste con grado 4&quot;, &quot;Ajuste con grado 20&quot;, &quot;Nuevas observaciones&quot;), lty = c(1, NA, 1, 2, 3, NA), lwd = c(2, NA, 1, 1, 1, NA), pch = c(NA, 1, NA, NA, NA, 2)) Figura F.2: Muestra con ajustes polinómicos con distinta complejidad y nuevas observaciones. MSEP &lt;- sapply(list(fit1 = fit1, fit2 = fit2, fit3 = fit3), function(x) mean((y.new - fitted(x))^2)) MSEP ## fit1 fit2 fit3 ## 1.4983208 0.1711238 0.2621064 Como ejemplo adicional, para evitar el efecto de la aleatoriedad de la muestra, en el siguiente código se simulan 100 muestras del proceso anterior a las que se les ajustan modelos polinómicos variando el grado de 1 a 20. Posteriormente se evalua la precisión en la muestra empleada en el ajuste y en un nuevo conjunto de datos procedente de la misma población. nsim &lt;- 100 set.seed(1) grado.max &lt;- 20 grados &lt;- seq_len(grado.max) mse &lt;- mse.new &lt;- matrix(nrow = grado.max, ncol = nsim) # Error cuadrático medio for(i in seq_len(nsim)) { y &lt;- mu + rnorm(n, 0, sd) y.new &lt;- mu + rnorm(n, 0, sd) for (grado in grados) { # grado &lt;- 1 fit &lt;- lm(y ~ poly(x, grado)) mse[grado, i] &lt;- mean(residuals(fit)^2) mse.new[grado, i] &lt;- mean((y.new - fitted(fit))^2) } } # Simulaciones matplot(grados, mse, type = &quot;l&quot;, col = &quot;lightgray&quot;, lty = 1, ylim = c(0, 2), xlab = &quot;Grado del polinomio (complejidad)&quot;, ylab = &quot;Error cuadrático medio&quot;) matlines(grados, mse.new, type = &quot;l&quot;, lty = 2, col = &quot;lightgray&quot;) # Global precision &lt;- rowMeans(mse) precision.new &lt;- rowMeans(mse.new) lines(grados, precision, lwd = 2) lines(grados, precision.new, lty = 2, lwd = 2) abline(h = sd^2, lty = 3) abline(v = 4, lty = 3) legend(&quot;topright&quot;, legend = c(&quot;Muestras&quot;, &quot;Nuevas observaciones&quot;), lty = c(1, 2)) Figura F.3: Precisiones (errores cuadráticos medios) de ajustes polinómicos variando la complejidad, en las muestras empleadas en el ajuste y en nuevas observaciones (simulados). Como se puede observar en la Figura F.3 los errores de entrenamiento disminuyen a medida que aumenta la complejidad del modelo. Sin embargo los errores de predicción en nuevas observaciones primero disminuyen hasta alcanzar un mínimo, marcado por la línea de puntos vertical que se corresponde con el modelo de grado 4, y después aumentan (la línea de puntos horizontal es la varianza del proceso; el error cuadrático medio de predicción asintótico). La línea vertical representa el equilibrio entre el sesgo y la varianza. Considerando un valor de complejidad a la izquierda de esa línea tendríamos infraajuste (mayor sesgo y menor varianza) y a la derecha sobreajuste (menor sesgo y mayor varianza). Desde un punto de vista más formal, considerando el modelo (F.1) y una función de pérdidas cuadrática, el predictor óptimo (desconocido) sería la media condicional \\(m(\\mathbf{x}) = E\\left( \\left. Y\\right\\vert_{\\mathbf{X}=\\mathbf{x}} \\right)\\)6. Por tanto los predictores serían realmente estimaciones de la función de regresión, \\(\\hat Y(\\mathbf{x}) = \\hat m(\\mathbf{x})\\) y podemos expresar la media del error cuadrático de predicción en términos del sesgo y la varianza: \\[ \\begin{aligned} E \\left( Y(\\mathbf{x}_0) - \\hat Y(\\mathbf{x}_0) \\right)^2 &amp; = E \\left( m(\\mathbf{x}_0) + \\varepsilon - \\hat m(\\mathbf{x}_0) \\right)^2 = E \\left( m(\\mathbf{x}_0) - \\hat m(\\mathbf{x}_0) \\right)^2 + \\sigma^2 \\\\ &amp; = E^2 \\left( m(\\mathbf{x}_0) - \\hat m(\\mathbf{x}_0) \\right) + Var\\left( \\hat m(\\mathbf{x}_0) \\right) + \\sigma^2 \\\\ &amp; = \\text{sesgo}^2 + \\text{varianza} + \\text{error irreducible} \\end{aligned} \\] donde \\(\\mathbf{x}_0\\) hace referencia al vector de valores de las variables explicativas de una nueva observación (no empleada en la construcción del predictor). En general, al aumentar la complejidad disminuye el sesgo y aumenta la varianza (y viceversa). Esto es lo que se conoce como el dilema o compromiso entre el sesgo y la varianza (bias-variance tradeoff). La recomendación sería por tanto seleccionar los hiperparámetros (el modelo final) tratando de que haya un equilibrio entre el sesgo y la varianza. F.3.2 Datos de entrenamiento y datos de test Como se mostró en la sección anterior hay que tener mucho cuidado si se pretende evaluar la precisión de las predicciones empleando la muestra de entrenamiento. Si el número de observaciones no es muy grande, se puede entrenar el modelo con todos los datos y emplear técnicas de remuestreo para evaluar la precisión (típicamente validación cruzada o bootstrap). Habría que asegurase de que el procedimiento de remuestreo empleado es adecuado (por ejemplo, la presencia de dependencia requeriría de métodos más sofisticados). Sin embargo, si el número de obervaciones es grande, se suele emplear el procedimiento tradicional en ML, que consiste en particionar la base de datos en 2 (o incluso en 3) conjuntos (disjuntos): Conjunto de datos de entrenamiento (o aprendizaje) para construir los modelos. Conjunto de datos de test para evaluar el rendimiento de los modelos. Los datos de test deberían utilizarse únicamente para evaluar los modelos finales, no se deberían emplear para seleccionar hiperparámetros. Para seleccionalos se podría volver a particionar los datos de entrenamiento, es decir, dividir la muestra en tres subconjuntos: datos de entrenamiento, de validación y de test (por ejemplo considerando un 70%, 15% y 15% de las observaciones, respectivamente). Para cada combinación de hiperparámetros se ajustaría el correspondiente modelo con los datos de entrenamiento, se emplearían los de validación para evaluarlos y posteriormente seleccionar los valores óptimos. Por último, se emplean los datos de test para evaluar el rendimiento del modelo seleccionado. No obstante, lo más habitual es seleccionar los hiperparámetros empleando validación cruzada (o otro tipo de remuestreo) en la muestra de entrenamiento, en lugar de considerar una muestra adicional de validación. En la siguiente sección se describirá esta última aproximación. En R se puede realizar el particionamiento de los datos empleando la función sample() del paquete base (otra alternativa sería emplear la función createDataPartition del paquete caret como se describe en la Sección F.6). Típicamente se selecciona el 80% de los datos como muestra de entrenamiento y el 20% restante como muestra de test, aunque esto dependería del número de datos. Como ejemplo consideraremos el conjunto de datos Boston del paquete MASS que contiene, entre otros datos, la valoración de las viviendas (medv, mediana de los valores de las viviendas ocupadas, en miles de dólares) y el porcentaje de población con menor estatus (lstat) en los suburbios de Boston. Podemos contruir las muestras de entrenamiento (80%) y de test (20%) con el siguiente código: data(Boston, package = &quot;MASS&quot;) # ?Boston set.seed(1) nobs &lt;- nrow(Boston) itrain &lt;- sample(nobs, 0.8 * nobs) train &lt;- Boston[itrain, ] test &lt;- Boston[-itrain, ] F.3.3 Validación cruzada Como ya se comentó, una herramienta para evaluar la calidad predictiva de un modelo es la validación cruzada, que permite cuantificar el error de predicción utilizando una única muestra de datos. En su versión más simple, validación cruzada dejando uno fuera (Leave-one-out cross-validation, LOOCV), para cada observación de la muestra se realiza un ajuste empleando el resto de observaciones, y se mide el error de predicción en esa observación (único dato no utilizado en el ajuste del modelo). Finalmente, combinando todos los errores individuales se puede obtener medidas globales del error de predicción (o aproximar características de su distribución). El método de LOOCV requeriría, en principio (ver comentarios más adelante), el ajuste de un modelo para cada observación por lo que pueden aparecer problemas computacionales si el conjunto de datos es grande. En este caso se suele emplear grupos de observaciones en lugar de observaciones individuales. Si se particiona el conjunto de datos en k grupos, típicamente 10 o 5 grupos, se denomina k-fold cross-validation (LOOCV sería un caso particular considerando un número de grupos igual al número de observaciones). Hay muchas variaciones de este método, entre ellas particionar repetidamente de forma aleatoria los datos en un conjunto de entrenamiento y otro de validación (de esta forma algunas observaciones podrían aparecer repetidas veces y otras ninguna en las muestras de validación). Continuando con el ejemplo anterior, supongamos que queremos emplear regresión polinómica para explicar la valoración de las viviendas (medv) a partir del estatus de los residentes (lstat). Al igual que se hizo en la Sección F.3.1, consideraremos el grado del polinomio como un hiperparámetro. plot(medv ~ lstat, data = train) Podríamos emplear la siguiente función que devuelve para cada observación (fila) de una muestra de entrenamiento, el error de predicción en esa observación ajustando un modelo lineal con todas las demás observaciones: cv.lm0 &lt;- function(formula, datos) { n &lt;- nrow(datos) cv.res &lt;- numeric(n) for (i in 1:n) { modelo &lt;- lm(formula, datos[-i, ]) cv.pred &lt;- predict(modelo, newdata = datos[i, ]) cv.res[i] &lt;- cv.pred - datos[i, ] } return(cv.res) } La función anterior no es muy eficiente, pero podría modificarse fácilmente para emplear otros métodos de regresión7. En el caso de regresión lineal múltiple (y de otros modelos lineales), se pueden obtener fácilmente las predicciones eliminando una de las observaciones a partir del ajuste con todos los datos. Por ejemplo, en lugar de la anterior sería preferible emplear la siguiente función (ver ?rstandard): cv.lm &lt;- function(formula, datos) { modelo &lt;- lm(formula, datos) return(rstandard(modelo, type = &quot;predictive&quot;)) } Empleando esta función, podemos calcular una medida del error de predicción de validación cruzada (en este caso el error cuadrático medio) para cada valor del hiperparámetro (grado del ajuste polinómico) y seleccionar el que lo minimiza. grado.max &lt;- 10 grados &lt;- seq_len(grado.max) cv.mse &lt;- cv.mse.sd &lt;- numeric(grado.max) for(grado in grados){ cv.res &lt;- cv.lm(medv ~ poly(lstat, grado), train) se &lt;- cv.res^2 cv.mse[grado] &lt;- mean(se) cv.mse.sd[grado] &lt;- sd(se)/sqrt(length(se)) } plot(grados, cv.mse, ylim = c(25, 45), xlab = &quot;Grado del polinomio (complejidad)&quot;) # Valor óptimo imin.mse &lt;- which.min(cv.mse) grado.op &lt;- grados[imin.mse] points(grado.op, cv.mse[imin.mse], pch = 16) grado.op ## [1] 5 En lugar de emplear los valores óptimos de los hiperparámetros, Breiman et al. (1984) propusieron la regla de un error estándar para seleccionar la complejidad del modelo. La idea es que estamos trabajando con estimaciones de la precisión y pueden presentar variabilidad, por lo que la sugerencia es seleccionar el modelo más simple8 dentro de un error estándar de la precisión del modelo correspondiente al valor óptimo (se consideraría que no hay diferencias significativas en la precisión; además, se mitigaría el efecto de la variabilidad debida a aleatoriedad/semilla). plot(grados, cv.mse, ylim = c(25, 45)) segments(grados, cv.mse - cv.mse.sd, grados, cv.mse + cv.mse.sd) # Límite superior &quot;oneSE rule&quot; y complejidad mínima por debajo de ese valor upper.cv.mse &lt;- cv.mse[imin.mse] + cv.mse.sd[imin.mse] abline(h = upper.cv.mse, lty = 2) imin.1se &lt;- min(which(cv.mse &lt;= upper.cv.mse)) grado.1se &lt;- grados[imin.1se] points(grado.1se, cv.mse[imin.1se], pch = 16) grado.1se ## [1] 2 plot(medv ~ lstat, data = train) fit.op &lt;- lm(medv ~ poly(lstat, grado.op), train) fit.1se &lt;- lm(medv ~ poly(lstat, grado.1se), train) newdata &lt;- data.frame(lstat = seq(0, 40, len = 100)) lines(newdata$lstat, predict(fit.op, newdata = newdata)) lines(newdata$lstat, predict(fit.1se, newdata = newdata), lty = 2) legend(&quot;topright&quot;, legend = c(paste(&quot;Grado óptimo:&quot;, grado.op), paste(&quot;oneSE rule:&quot;, grado.1se)), lty = c(1, 2)) F.3.4 Evaluación de un método de regresión Para estudiar la precisión de las predicciones de un método de regresión se evalúa el modelo en el conjunto de datos de test y se comparan las predicciones frente a los valores reales. Si generamos un gráfico de dispersión de observaciones frente a predicciones, los puntos deberían estar en torno a la recta \\(y=x\\) (línea continua). obs &lt;- test$medv pred &lt;- predict(fit.op, newdata = test) plot(pred, obs, main = &quot;Observado frente a predicciones&quot;, xlab = &quot;Predicción&quot;, ylab = &quot;Observado&quot;) abline(a = 0, b = 1) res &lt;- lm(obs ~ pred) # summary(res) abline(res, lty = 2) También es habitual calcular distintas medidas de error. Por ejemplo, podríamos emplear la función postResample() del paquete caret: caret::postResample(pred, obs) ## RMSE Rsquared MAE ## 4.8526718 0.6259583 3.6671847 La función anterior, además de las medidas de error habituales (que dependen en su mayoría de la escala de la variable respuesta) calcula un pseudo R-cuadrado. En este paquete (también en rattle) se emplea uno de los más utilizados, el cuadrado del coeficiente de correlación entre las predicciones y los valores observados (que se corresponde con la línea discontinua en la figura anterior). Estos valores se interpretarían como el coeficiente de determinación en regresión lineal, debería ser próximo a 1. Hay otras alternativas (ver Kvålseth, 1985), pero la idea es que deberían medir la proporción de variabilidad de la respuesta explicada por el modelo, algo que en general no es cierto con el anterior9. La recomendación sería emplear: \\[\\tilde R^2 = 1 - \\frac{\\sum_{i=1}^n(y_i - \\hat y_i)^2}{\\sum_{i=1}^n(y_i - \\bar y_i)^2}\\] implementado junto con otras medidas en la siguiente función: accuracy &lt;- function(pred, obs, na.rm = FALSE, tol = sqrt(.Machine$double.eps)) { err &lt;- obs - pred # Errores if(na.rm) { is.a &lt;- !is.na(err) err &lt;- err[is.a] obs &lt;- obs[is.a] } perr &lt;- 100*err/pmax(obs, tol) # Errores porcentuales return(c( me = mean(err), # Error medio rmse = sqrt(mean(err^2)), # Raíz del error cuadrático medio mae = mean(abs(err)), # Error absoluto medio mpe = mean(perr), # Error porcentual medio mape = mean(abs(perr)), # Error porcentual absoluto medio r.squared = 1 - sum(err^2)/sum((obs - mean(obs))^2) # Pseudo R-cuadrado )) } accuracy(pred, obs) ## me rmse mae mpe mape r.squared ## -0.6731294 4.8526718 3.6671847 -8.2322506 19.7097373 0.6086704 accuracy(predict(fit.1se, newdata = test), obs) ## me rmse mae mpe mape r.squared ## -0.9236280 5.2797360 4.1252053 -9.0029771 21.6512406 0.5367608 Ejercicio F.1 Considerando de nuevo el ejemplo anterior, particionar la muestra en datos de entrenamiento (70%), de validación (15%) y de test (15%), para entrenar los modelos polinómicos, seleccionar el grado óptimo (el hiperparámetro) y evaluar las predicciones del modelo final, respectivamente. Podría ser de utilidad el siguiente código (basado en la aproximación de rattle), que particiona los datos suponiendo que están almacenados en el data.frame df: df &lt;- Boston set.seed(1) nobs &lt;- nrow(df) itrain &lt;- sample(nobs, 0.7 * nobs) inotrain &lt;- setdiff(seq_len(nobs), itrain) ivalidate &lt;- sample(inotrain, 0.15 * nobs) itest &lt;- setdiff(inotrain, ivalidate) train &lt;- df[itrain, ] validate &lt;- df[ivalidate, ] test &lt;- df[itest, ] Alternativamente podríamos emplear la función split() creando un factor que divida aleatoriamente los datos en tres grupos (versión simplificada de una propuesta en este post): set.seed(1) p &lt;- c(train = 0.7, validate = 0.15, test = 0.15) f &lt;- sample( rep(factor(seq_along(p), labels = names(p)), times = nrow(df)*p/sum(p)) ) samples &lt;- suppressWarnings(split(df, f)) str(samples) ## List of 3 ## $ train :&#39;data.frame&#39;: 356 obs. of 14 variables: ## ..$ crim : num [1:356] 0.00632 0.02731 0.02729 0.02985 0.08829 ... ## ..$ zn : num [1:356] 18 0 0 0 12.5 12.5 12.5 12.5 12.5 0 ... ## ..$ indus : num [1:356] 2.31 7.07 7.07 2.18 7.87 7.87 7.87 7.87 7.87 8.14 ... ## ..$ chas : int [1:356] 0 0 0 0 0 0 0 0 0 0 ... ## ..$ nox : num [1:356] 0.538 0.469 0.469 0.458 0.524 0.524 0.524 0.524 0.524 0.538 ... ## ..$ rm : num [1:356] 6.58 6.42 7.18 6.43 6.01 ... ## ..$ age : num [1:356] 65.2 78.9 61.1 58.7 66.6 100 85.9 82.9 39 56.5 ... ## ..$ dis : num [1:356] 4.09 4.97 4.97 6.06 5.56 ... ## ..$ rad : int [1:356] 1 2 2 3 5 5 5 5 5 4 ... ## ..$ tax : num [1:356] 296 242 242 222 311 311 311 311 311 307 ... ## ..$ ptratio: num [1:356] 15.3 17.8 17.8 18.7 15.2 15.2 15.2 15.2 15.2 21 ... ## ..$ black : num [1:356] 397 397 393 394 396 ... ## ..$ lstat : num [1:356] 4.98 9.14 4.03 5.21 12.43 ... ## ..$ medv : num [1:356] 24 21.6 34.7 28.7 22.9 16.5 18.9 18.9 21.7 19.9 ... ## $ validate:&#39;data.frame&#39;: 75 obs. of 14 variables: ## ..$ crim : num [1:75] 0.0324 0.6298 0.9884 0.9558 1.0025 ... ## ..$ zn : num [1:75] 0 0 0 0 0 0 0 75 75 0 ... ## ..$ indus : num [1:75] 2.18 8.14 8.14 8.14 8.14 8.14 5.96 2.95 2.95 6.91 ... ## ..$ chas : int [1:75] 0 0 0 0 0 0 0 0 0 0 ... ## ..$ nox : num [1:75] 0.458 0.538 0.538 0.538 0.538 0.538 0.499 0.428 0.428 0.448 ... ## ..$ rm : num [1:75] 7 5.95 5.81 6.05 6.67 ... ## ..$ age : num [1:75] 45.8 61.8 100 88.8 87.3 95 41.5 21.8 15.8 6.5 ... ## ..$ dis : num [1:75] 6.06 4.71 4.1 4.45 4.24 ... ## ..$ rad : int [1:75] 3 4 4 4 4 4 5 3 3 3 ... ## ..$ tax : num [1:75] 222 307 307 307 307 307 279 252 252 233 ... ## ..$ ptratio: num [1:75] 18.7 21 21 21 21 21 19.2 18.3 18.3 17.9 ... ## ..$ black : num [1:75] 395 397 395 306 380 ... ## ..$ lstat : num [1:75] 2.94 8.26 19.88 17.28 11.98 ... ## ..$ medv : num [1:75] 33.4 20.4 14.5 14.8 21 13.1 21 30.8 34.9 24.7 ... ## $ test :&#39;data.frame&#39;: 75 obs. of 14 variables: ## ..$ crim : num [1:75] 0.069 0.1446 0.2249 0.638 0.6719 ... ## ..$ zn : num [1:75] 0 12.5 12.5 0 0 0 0 90 0 0 ... ## ..$ indus : num [1:75] 2.18 7.87 7.87 8.14 8.14 ... ## ..$ chas : int [1:75] 0 0 0 0 0 0 0 0 0 0 ... ## ..$ nox : num [1:75] 0.458 0.524 0.524 0.538 0.538 0.538 0.499 0.403 0.413 0.413 ... ## ..$ rm : num [1:75] 7.15 6.17 6.38 6.1 5.81 ... ## ..$ age : num [1:75] 54.2 96.1 94.3 84.5 90.3 94.1 68.2 21.9 6.6 7.8 ... ## ..$ dis : num [1:75] 6.06 5.95 6.35 4.46 4.68 ... ## ..$ rad : int [1:75] 3 5 5 4 4 4 5 5 4 4 ... ## ..$ tax : num [1:75] 222 311 311 307 307 307 279 226 305 305 ... ## ..$ ptratio: num [1:75] 18.7 15.2 15.2 21 21 21 19.2 17.9 19.2 19.2 ... ## ..$ black : num [1:75] 397 397 393 380 377 ... ## ..$ lstat : num [1:75] 5.33 19.15 20.45 10.26 14.81 ... ## ..$ medv : num [1:75] 36.2 27.1 15 18.2 16.6 12.7 18.9 35.4 24.2 22.8 ... F.3.5 Evaluación de un método de clasificación Para estudiar la eficiencia de un método de clasificación supervisada típicamente se obtienen las predicciones para el conjunto de datos de test y se genera una tabla de contingencia, denominada matriz de confusión, con las predicciones frente a los valores reales. En primer lugar consideraremos el caso de dos categorías. La matriz de confusión será de la forma: Observado\\Predicción Positivo Negativo Verdadero Verdaderos positivos (TP) Falsos negativos (FN) Falso Falsos positivos (FP) Verdaderos negativos (TN) A partir de esta tabla se pueden obtener distintas medidas de la precisión de las predicciones. Por ejemplo, dos de las más utilizadas son la tasa de verdaderos positivos y la de verdaderos negativos (tasas de acierto en positivos y negativos), también denominadas sensibilidad y especificidad: Sensibilidad (sensitivity, recall, hit rate, true positive rate; TPR): \\[TPR = \\frac{TP}{P} = \\frac{TP}{TP+FN}\\] Especificidad (specificity, true negative rate; TNR): \\[TNR = \\frac{TN}{TN+FP}\\] La precisión global o tasa de aciertos (accuracy; ACC) sería: \\[ACC = \\frac{TP + TN}{P + N} = \\frac{TP+TN}{TP+TN+FP+FN}\\] Sin embargo hay que tener cuidado con esta medida cuando las clases no están balanceadas. Otras medidas de la precisión global que tratan de evitar este problema son la precisión balanceada (balanced accuracy, BA): \\[BA = \\frac{TPR + TNR}{2}\\] (media aritmética de TPR y TNR) o la puntuación F1 (F1 score; media armónica de TPR y el valor predictivo positivo, PPV, descrito más adelante): \\[F_1 = \\frac{2TP}{2TP+FP+FN}\\] Otra medida global es el coeficiente kappa de Cohen, que compara la tasa de aciertos con la obtenida en una clasificación al azar (un valor de 1 indicaría máxima precisión y 0 que la precisión es igual a la que obtendríamos clasificando al azar; empleando la tasa de positivos, denominada prevalencia, para predecir positivo). También hay que tener cuidado las medidas que utilizan como estimación de la probabilidad de positivo (prevalencia) la tasa de positivos en la muestra de test, como el valor (o índice) predictivo positivo (precision, positive predictive value; PPV): \\[PPV = \\frac{TP}{TP+FP}\\] (que no debe ser confundido con la precisión global ACC) y el valor predictivo negativo negativo (NPV): \\[NPV = \\frac{TN}{TN+FN},\\] si la muestra de test no refleja lo que ocurre en la población (por ejemplo si la clase de interés está sobrerrepresentada en la muestra). En estos casos habrá que recalcularlos empleando estimaciones válidas de las probabilidades de la clases (por ejemplo, en estos casos, la función caret::confusionMatrix() permite establecer estimaciones válidas mediante el argumento prevalence). Como ejemplo emplearemos los datos anteriores de valoraciones de viviendas y estatus de la población, considerando como respuesta una nueva variable fmedv que clasifica las valoraciones en Bajo o Alto dependiendo de si medv &gt; 25. # data(Boston, package = &quot;MASS&quot;) datos &lt;- Boston datos$fmedv &lt;- factor(datos$medv &gt; 25, labels = c(&quot;Bajo&quot;, &quot;Alto&quot;)) # levels = c(&#39;FALSE&#39;, &#39;TRUE&#39;) # En este caso las clases no están balanceadas table(datos$fmedv) ## ## Bajo Alto ## 382 124 caret::featurePlot(datos$lstat, datos$fmedv, plot = &quot;density&quot;, labels = c(&quot;lstat&quot;, &quot;Density&quot;), auto.key = TRUE) El siguiente código realiza la partición de los datos y posteriormente ajusta un modelo de regresión logística en la muestra de entrenamiento considerando lstat como única variable explicativa (en el Capítulo 5 se darán más detalles sobre este tipo de modelos): # Particionado de los datos set.seed(1) nobs &lt;- nrow(datos) itrain &lt;- sample(nobs, 0.8 * nobs) train &lt;- datos[itrain, ] test &lt;- datos[-itrain, ] # Ajuste modelo modelo &lt;- glm(fmedv ~ lstat, family = binomial, data = train) summary(modelo) ## ## Call: ## glm(formula = fmedv ~ lstat, family = binomial, data = train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.9749 -0.4161 -0.0890 0.3785 3.6450 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.74366 0.47901 7.815 5.48e-15 *** ## lstat -0.54231 0.06134 -8.842 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 460.84 on 403 degrees of freedom ## Residual deviance: 243.34 on 402 degrees of freedom ## AIC: 247.34 ## ## Number of Fisher Scoring iterations: 7 En este caso podemos obtener las estimaciones de la probabilidad de la segunda categoría empleando predict() con type = \"response\", a partir de las cuales podemos establecer las predicciones como la categoría más probable: obs &lt;- test$fmedv p.est &lt;- predict(modelo, type = &quot;response&quot;, newdata = test) pred &lt;- factor(p.est &gt; 0.5, labels = c(&quot;Bajo&quot;, &quot;Alto&quot;)) # levels = c(&#39;FALSE&#39;, &#39;TRUE&#39;) Finalmente podemos obtener la matriz de confusión con el siguiente código: tabla &lt;- table(obs, pred) # addmargins(tabla, FUN = list(Total = sum)) tabla ## pred ## obs Bajo Alto ## Bajo 71 11 ## Alto 8 12 # Porcentajes respecto al total print(100*prop.table(tabla), digits = 2) ## pred ## obs Bajo Alto ## Bajo 69.6 10.8 ## Alto 7.8 11.8 # Porcentajes (de aciertos y fallos) por categorías print(100*prop.table(tabla, 1), digits = 3) ## pred ## obs Bajo Alto ## Bajo 86.6 13.4 ## Alto 40.0 60.0 Alternativamente se podría emplear la función confusionMatrix() del paquete caret que permite obtener distintas medidas de la precisión: caret::confusionMatrix(pred, obs, positive = &quot;Alto&quot;, mode = &quot;everything&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction Bajo Alto ## Bajo 71 8 ## Alto 11 12 ## ## Accuracy : 0.8137 ## 95% CI : (0.7245, 0.884) ## No Information Rate : 0.8039 ## P-Value [Acc &gt; NIR] : 0.4604 ## ## Kappa : 0.4409 ## ## Mcnemar&#39;s Test P-Value : 0.6464 ## ## Sensitivity : 0.6000 ## Specificity : 0.8659 ## Pos Pred Value : 0.5217 ## Neg Pred Value : 0.8987 ## Precision : 0.5217 ## Recall : 0.6000 ## F1 : 0.5581 ## Prevalence : 0.1961 ## Detection Rate : 0.1176 ## Detection Prevalence : 0.2255 ## Balanced Accuracy : 0.7329 ## ## &#39;Positive&#39; Class : Alto ## Si el método de clasificación proporciona estimaciones de las probabilidades de las categorías, disponemos de más información en la clasificación que también podemos emplear en la evaluación del rendimiento. Por ejemplo, se puede realizar un analisis descriptivo de las probabilidades estimadas y las categorías observadas en la muestra de test: # Imitamos la función caret::plotClassProbs() library(lattice) histogram(~ p.est | obs, xlab = &quot;Probabilidad estimada de &#39;Alto&#39;&quot;) Para evaluar las estimaciones de las probabilidades se suele emplear la curva ROC (receiver operating characteristics, característica operativa del receptor; diseñada inicialmente en el campo de la detección de señales). Como ya se comentó, normalmente se emplea \\(c = 0.5\\) como punto de corte para clasificar en la categoría de interés (es lo que se conoce como regla de Bayes), aunque se podrían considerar otros valores (por ejemplo para mejorar la clasificación en una de las categorías, a costa de empeorar la precisión global). En la curva ROC se representa la sensibilidad (TPR) frente a la tasa de falsos negativos (FNR = 1 - TNR = 1 - especificidad) para distintos valores de corte. Para ello se puede emplear el paquete pROC: library(pROC) roc_glm &lt;- roc(response = obs, predictor = p.est) # View((as.data.frame(roc_glm[2:4]))) plot(roc_glm) Figura F.4: Curva ROC correspondiente al modelo de regresión logística. # plot(roc_glm, legacy.axes = TRUE, print.thres = 0.5) Lo ideal sería que la curva se aproximase a la esquina superior izquierda (máxima sensibilidad y especificidad). La recta diagonal se correspondería con un clasificador aleatorio. Una medida global del rendimiento del clasificador es el área bajo la curva ROC (AUC; equivalente al estadístico U de Mann-Whitney o al índice de Gini). Un clasificador perfecto tendría un valor de 1 y 0.5 uno aleatorio. # roc_glm$auc roc_glm ## ## Call: ## roc.default(response = obs, predictor = p.est) ## ## Data: p.est in 82 controls (obs Bajo) &lt; 20 cases (obs Alto). ## Area under the curve: 0.8427 ci.auc(roc_glm) ## 95% CI: 0.7428-0.9426 (DeLong) Como comentario adicional, aunque se puede modificar el punto de corte para mejorar la clasificación en la categoría de interés (de hecho, algunas herramientas como h2o lo modifican por defecto; en este caso concreto para maximizar \\(F_1\\) en la muestra de entrenamiento), muchos métodos de clasificación (como los basados en árboles descritos en el Capítulo 2) admiten como opción una matriz de pérdidas que se tendrá en cuenta para medir la eficiencia durante el aprendizaje y normalmente esta sería la aproximación recomendada. En el caso de más de dos categorías podríamos generar una matriz de confusión de forma análoga, aunque en este caso en principio solo podríamos calcular medidas globales de la precisión como la tasa de aciertos o el coeficiente kappa de Cohen. Podríamos obtener también medidas por clase, como la sensibilidad y la especificidad, siguiendo la estrategia uno contra todos descrita en la Sección F.2.1. Esta aproximación es la que sigue la función confusionMatrix() del paquete caret (devuelve las medidas comparando cada categoría con las restantes en el componente $byClass). Como ejemplo ilustrativo consideraremos el conocido conjunto de datos iris (Fisher, 1936) en el que el objetivo es clasificar flores de lirio en tres especies (Species) a partir del largo y ancho de sépalos y pétalos, aunque en este caso emplearemos un clasificador aleatorio. data(iris) # Partición de los datos datos &lt;- iris set.seed(1) nobs &lt;- nrow(datos) itrain &lt;- sample(nobs, 0.8 * nobs) train &lt;- datos[itrain, ] test &lt;- datos[-itrain, ] # Entrenamiento prevalences &lt;- table(train$Species)/nrow(train) prevalences ## ## setosa versicolor virginica ## 0.3250000 0.3166667 0.3583333 # Calculo de las predicciones levels &lt;- names(prevalences) # levels(train$Species) f &lt;- factor(levels, levels = levels) # factor(levels) valdría en este caso al estar por orden alfabético pred.rand &lt;- sample(f, nrow(test), replace = TRUE, prob = prevalences) # Evaluación caret::confusionMatrix(pred.rand, test$Species) ## Confusion Matrix and Statistics ## ## Reference ## Prediction setosa versicolor virginica ## setosa 3 3 1 ## versicolor 4 2 5 ## virginica 4 7 1 ## ## Overall Statistics ## ## Accuracy : 0.2 ## 95% CI : (0.0771, 0.3857) ## No Information Rate : 0.4 ## P-Value [Acc &gt; NIR] : 0.9943 ## ## Kappa : -0.1862 ## ## Mcnemar&#39;s Test P-Value : 0.5171 ## ## Statistics by Class: ## ## Class: setosa Class: versicolor Class: virginica ## Sensitivity 0.2727 0.16667 0.14286 ## Specificity 0.7895 0.50000 0.52174 ## Pos Pred Value 0.4286 0.18182 0.08333 ## Neg Pred Value 0.6522 0.47368 0.66667 ## Prevalence 0.3667 0.40000 0.23333 ## Detection Rate 0.1000 0.06667 0.03333 ## Detection Prevalence 0.2333 0.36667 0.40000 ## Balanced Accuracy 0.5311 0.33333 0.33230 Se podrían considerar otras funciones de pérdida, por ejemplo con la distancia \\(L_1\\) sería la mediana condicional, pero las consideraciones serían análogas. También puede ser de interés la función cv.glm() del paquete boot. Suponiendo que los modelos se pueden ordenar del más simple al más complejo. Por ejemplo obtendríamos el mismo valor si desplazamos las predicciones sumando una constante (i.e. no tiene en cuenta el sesgo). "],["F-4-dimen-curse.html", "F.4 La maldición de la dimensionalidad", " F.4 La maldición de la dimensionalidad Podríamos pensar que al aumentar el número de variables explicativas se mejora la capacidad predictiva de los modelos. Lo cual, en general, sería cierto si realmente los predictores fuesen de utilidad para explicar la respuesta. Sin embargo, al aumentar el número de dimensiones se pueden agravar notablemente muchos de los problemas que ya pueden aparecer en dimensiones menores, esto es lo que se conoce como la maldición de la dimensionalidad (curse of dimensionality, Bellman, 1961). Uno de estos problemas es el denominado efecto frontera que ya puede aparecer en una dimensión, especialmente al trabajar con modelos flexibles (como ajustes polinómicos con grados altos o los métodos locales que trataremos en el Capítulo 6). La idea es que en la frontera del rango de valores de una variable explicativa vamos a disponer de pocos datos y los errores de predicción van a tener gran variabilidad (se están haciendo extrapolaciones de los datos, más que interpolaciones, y van a ser menos fiables). Como ejemplo consideraremos un problema de regresión simple, con un conjunto de datos simulados (del proceso ya considerado en la Sección F.3.1) con 100 observaciones (que ya podríamos considerar que no es muy pequeño). # Simulación datos n &lt;- 100 x &lt;- seq(0, 1, length = n) mu &lt;- 2 + 4*(5*x - 1)*(4*x - 2)*(x - 0.8)^2 # grado 4 sd &lt;- 0.5 set.seed(1) y &lt;- mu + rnorm(n, 0, sd) datos &lt;- data.frame(x = x, y = y) plot(x, y) lines(x, mu, lwd = 2, col = &quot;lightgray&quot;) Figura F.5: Muestra simulada y tendencia teórica. Cuando el número de datos es más o menos grande podríamos pensar en predecir la respuesta a partir de lo que ocurre en las observaciones cercanas a la posición de predicción, esta es la idea de los métodos locales (Capítulo 6). Uno de los métodos de este tipo más conocidos es el de los k-vecinos más cercanos (k-nearest neighbors; KNN). Se trata de un método muy simple, pero que puede ser muy efectivo, que se basa en la idea de que localmente la media condicional (la predicción óptima) es constante. Concretamente, dados un entero \\(k\\) (hiperparámetro) y un conjunto de entrenamiento \\(\\mathcal{T}\\), para obtener la predicción correspondiente a un vector de valores de las variables explicativas \\(\\mathbf{x}\\), el método de regresión10 KNN promedia las observaciones en un vecindario \\(\\mathcal{N}_k(\\mathbf{x}, \\mathcal{T})\\) formado por las \\(k\\) observaciones más cercanas a \\(\\mathbf{x}\\): \\[\\hat{Y}(\\mathbf{x}) = \\hat{m}(\\mathbf{x}) = \\frac{1}{k} \\sum_{i \\in \\mathcal{N}_k(\\mathbf{x}, \\mathcal{T})} Y_i\\] (sería necesario definir una distancia, normalmente la distancia euclídea de los predictores estandarizados). Este método está implementado en numerosos paquetes, por ejemplo en la función knnreg() del paquete caret: library(caret) # Ajuste de los modelos fit1 &lt;- knnreg(y ~ x, data = datos, k = 5) # 5 observaciones más cercanas (5% de los datos) fit2 &lt;- knnreg(y ~ x, data = datos, k = 10) fit3 &lt;- knnreg(y ~ x, data = datos, k = 20) plot(x, y) lines(x, mu, lwd = 2, col = &quot;lightgray&quot;) newdata &lt;- data.frame(x = x) lines(x, predict(fit1, newdata), lwd = 2, lty = 3) lines(x, predict(fit2, newdata), lwd = 2, lty = 2) lines(x, predict(fit3, newdata), lwd = 2) legend(&quot;topright&quot;, legend = c(&quot;Verdadero&quot;, &quot;5-NN&quot;, &quot;10-NN&quot;, &quot;20-NN&quot;), lty = c(1, 3, 2, 1), lwd = 2, col = c(&quot;lightgray&quot;, 1, 1, 1)) Figura F.6: Predicciones con el método KNN y distintos vecindarios A medida que aumenta \\(k\\) disminuye la complejidad del modelo y se observa un incremento del efecto frontera. Habría que seleccionar un valor óptimo de \\(k\\) (buscando un equilibro entre sesgo y varianza, como se mostró en la Sección F.3.1 y se ilustrará en la última sección de este capítulo empleando este método con el paquete caret), que dependerá de la tendencia teórica y del número de datos. En este caso, para \\(k=5\\), podríamos pensar que el efecto frontera aparece en el 10% más externo del rango de la variable explicativa (con un número mayor de datos podría bajar al 1%). Al aumentar el número de variables explicativas, considerando que el 10% más externo del rango de cada una de ellas constituye la frontera de los datos, tendríamos que la proporción de frontera sería \\(1-0.9^d\\), siendo \\(d\\) el número de dimensiones. Lo que se traduce que con \\(d = 10\\) el 65% del espacio predictivo sería frontera y en torno al 88% para \\(d=20\\), es decir, al aumentar el número de dimensiones el problema del efecto frontera será generalizado. curve(1 - 0.9^x, 0, 200, ylab = &#39;Proporción de &quot;frontera&quot;&#39;, xlab = &#39;Número de dimensiones&#39;) curve(1 - 0.95^x, lty = 2, add = TRUE) curve(1 - 0.99^x, lty = 3, add = TRUE) abline(h = 0.5, col = &quot;lightgray&quot;) legend(&quot;bottomright&quot;, title = &quot;Rango en cada dimensión&quot;, legend = c(&quot;10%&quot; , &quot;5%&quot;, &quot;1%&quot;), lty = c(1, 2, 3)) Desde otro punto de vista, suponiendo que los predictores se distribuyen de forma uniforme, la densidad de las observaciones es proporcional a \\(n^{1/d}\\), siendo \\(n\\) el tamaño muestral. Por lo que si consideramos que una muestra de tamaño \\(n=100\\) es suficientemente densa en una dimensión, para obtener la misma densidad muestral en 10 dimensiones tendríamos que disponer de un tamaño muestral de \\(n = 100^{10} = 10^{20}\\). Por tanto, cuando el número de dimensiones es grande no va a haber muchas observaciones en el entorno de la posición de predicción y puede haber serios problemas de sobreajuste si se pretende emplear un modelo demasiado flexible (por ejemplo KNN con \\(k\\) pequeño). Hay que tener en cuenta que, en general, fijado el tamaño muestral, la flexibilidad de los modelos aumenta al aumentar el número de dimensiones del espacio predictivo. Para concluir, otro de los problemas que se agravan notablemente al aumentar el número de dimensiones es el de colinealidad (o concurvidad) que puede producir que muchos métodos (como los modelos lineales o las redes neuronales) sean muy poco eficientes o inestables (llegando incluso a que no se puedan aplicar), además de que complica notablemente la interpretación de cualquier método. Esto está relacionado también con la dificultad para determinar que variables son de interés para predecir la respuesta (i.e. no son ruido). Debido a la aleatoriedad, predictores que realmente no están relacionados con la respuesta pueden ser tenidos en cuenta por el modelo con mayor facilidad (KNN con las opciones habituales tiene en cuenta todos los predictores con el mismo peso). Lo que resulta claro es que si se agrega ruido se producirá un incremento en el error de predicción. Incluso si las variables añadidas resultan de interés, si el número de observaciones es pequeño en comparación, el incremento en la variabilidad de las predicciones puede no compensar la disminución del sesgo de predicción. Como conclusión, en el caso multidimensional habrá que tratar de emplear métodos que minimicen estos problemas. En el caso de clasificación se considerarían las variables indicadoras de las categorías y se obtendrían las frecuencias relativas en el vecindario como estimaciones de las probabilidades de las clases. "],["F-5-analisis-modelos.html", "F.5 Análisis e interpretación de los modelos", " F.5 Análisis e interpretación de los modelos El análisis e interpretación de modelos es un campo muy activo en AE/ML, para el que recientemente se ha acuñado el término de interpretable machine learning (IML). A continuación se resumen brevemente algunas de las principales ideas, para más detalles ver por ejemplo Molnar (2020). Como ya se comentó, a medida que aumenta la complejidad de los modelos generalmente disminuye su interpretabilidad, por lo que normalmente interesa encontrar el modelo más simple posible que resulte de utilidad para los objetivos propuestos. Aunque el principal objetivo sea la predicción, una vez obtenido el modelo final suele interesar medir la importancia de cada predictor en el modelo y si es posible como influyen en la predicción de la respuesta, es decir, estudiar el efecto de las variables explicativas. Esto puede presentar serias dificultades especialmente en modelos complejos en los que hay interacciones entre los predictores (el efecto de una variable explicativa depende de los valores de otras). La mayoría de los métodos de aprendizaje supervisado permiten obtener medidas de la importancia de las variables explicativas en la predicción (ver p.e. la ayuda de la función caret::varImp(); algunos, como los basados en árboles, incluso de las no incluidas en el modelo final). Muchos de los métodos de clasificación, en lugar de proporcionar medidas globales, calculan medidas para cada categoría. Alternativamente también se pueden obtener medidas de la importancia de las variables mediante procedimientos generales (en el sentido de que se pueden aplicar a cualquier modelo), pero suelen requerir de mucho más tiempo de computación (ver p.e. Molnar, 2020, Capítulo 5). En algunos de los métodos se modela explícitamente los efectos de los distintos predictores y estos se pueden analizar con (mas o menos) facilidad. Hay que tener en cuenta que, al margen de las interacciones, la colinealidad/concurvidad dificulta notablemente el estudio de los efectos de las variables explicativas. Otros métodos son más del tipo caja negra (black box) y precisan de aproximaciones más generales, como los gráficos PDP (Partial Dependence Plots; Friedman y Popescu, 2008; ver también Greenwell, 2017) o las curvas ICE (Individual Conditional Expectation; Goldstein et al. , 2015). Estos métodos11 tratan de estimar el efecto marginal de las variables explicativas, es decir, la variación en la predicción a medida que varía una variable explicativa manteniendo constantes el resto. La principal diferencia entre ambas aproximaciones es que los gráficos PDP muestran una única curva con el promedio de la respuesta mientras que las curvas ICE muestran una curva para cada observación (para más detalles ver las referencias anteriores). En problemas de clasificación también se están empleando la teoría de juegos cooperativos y las técnicas de optimización de Investigación Operativa para evaluar la importancia de las variables predictoras y determinar las más influyentes. Por citar algunos, Strumbelj y Kononenko (2010) propusieron un procedimiento general basado en el valor de Shapley de juegos cooperativos, y en Agor y Özaltn (2019) se propone el uso de algoritmos genéticos para determinar los predictores más influyentes. Paquetes y funciones de R: pdp: Partial Dependence Plots (también implementa curvas ICE y es compatible con caret) iml: Interpretable Machine Learning DALEX: moDel Agnostic Language for Exploration and eXplanation lime: Local Interpretable Model-Agnostic Explanations vip: Variable Importance Plots caret::varImp(), h2o::h2o.partialPplot() En los siguientes capítulos se mostrarán ejemplos empleando algunas de estas herramientas. Similares a los gráficos parciales de residuos de los modelos lineales o aditivos (ver p.e. las funciones termplot(), car::crPlots() o car::avPlots()). "],["F-6-caret.html", "F.6 Introducción al paquete caret", " F.6 Introducción al paquete caret Como ya se comentó en la Sección F.2.2, el paquete caret (abreviatura de Classification And REgression Training) proporciona una interfaz unificada que simplifica el proceso de modelado empleando la mayoría de los métodos de AE implementados en R (actualmente admite 238 métodos; ver el Capítulo 6 del manual de este paquete). Además de proporcionar rutinas para los principales pasos del proceso, incluye también numerosas funciones auxiliares que permitirían implementar nuevos procedimientos. En esta sección se describirán de forma esquemática las principales herramientas disponibles en este paquete, para más detalles se recomendaría consultar el manual del paquete caret. También está disponible una pequeña introducción en la vignette del paquete: A Short Introduction to the caret Package y una chuleta: Caret Cheat Sheet. La función principal es train() (descrita más adelante), que incluye un parámetro method que permite establecer el modelo mediante una cadena de texto. Podemos obtener información sobre los modelos disponibles con las funciones getModelInfo() y modelLookup() (puede haber varias implementaciones del mismo método con distintas configuraciones de hiperparámetros; también se pueden definir nuevos modelos, ver el Capítulo 13 del manual). library(caret) str(names(getModelInfo())) # Listado de todos los métodos disponibles ## chr [1:239] &quot;ada&quot; &quot;AdaBag&quot; &quot;AdaBoost.M1&quot; &quot;adaboost&quot; &quot;amdai&quot; &quot;ANFIS&quot; ... # names(getModelInfo(&quot;knn&quot;, regex = TRUE)) # Por defecto devuelve coincidencias parciales modelLookup(&quot;knn&quot;) # Información sobre hiperparámetros ## model parameter label forReg forClass probModel ## 1 knn k #Neighbors TRUE TRUE TRUE En la siguiente tabla se muestran los métodos actualmente disponibles: Figura F.7: Listado de los métodos disponiles en caret::train(). Este paquete permite, entre otras cosas: Partición de los datos createDataPartition(y, p = 0.5, list = TRUE, ...): crea particiones balanceadas de los datos. En el caso de que la respuesta y sea categórica realiza el muestreo en cada clase. Para respuestas numéricas emplea cuantiles (definidos por el argumento groups = min(5, length(y))). p: proporción de datos en la muestra de entrenamiento. list: lógico; determina si el resultado es una lista con las muestras o un vector (o matriz) de índices Funciones auxiliares: createFolds(), createMultiFolds(), groupKFold(), createResample(), createTimeSlices() Análisis descriptivo: featurePlot() Preprocesado de los datos: La función principal es preProcess(x, method = c(\"center\", \"scale\"), ...), aunque se puede integrar en el entrenamiento (función train()) para estimar los parámetros de las transformaciones a partir de la muestra de entrenamiento y posteriormente aplicarlas automáticamente al hacer nuevas predicciones (p.e. en la muestra de test). El parámetro method permite establecer una lista de procesados: Imputación: \"knnImpute\", \"bagImpute\" o \"medianImpute\" Creación y transformación de variables explicativas: \"center\", \"scale\", \"range\", \"BoxCox\", \"YeoJohnson\", \"expoTrans\", \"spatialSign\" Funciones auxiliares: `dummyVars()`... Selección de predictores y extracción de componentes: \"corr\", \"nzv\", \"zv\", \"conditionalX\", \"pca\", \"ica\" Funciones auxiliares: `rfe()`... Entrenamiento y selección de los hiperparámetros del modelo: La función principal es train(formula, data, method = \"rf\", trControl = trainControl(), tuneGrid = NULL, tuneLength = 3, ...) trControl: permite establecer el método de remuestreo para la evaluación de los hiperparámetros y el método para seleccionar el óptimo, incluyendo las medidas de precisión. Por ejemplo trControl = trainControl(method = \"cv\", number = 10, selectionFunction = \"oneSE\"). Los métodos disponibles son: \"boot\", \"boot632\", \"optimism_boot\", \"boot_all\", \"cv\", \"repeatedcv\", \"LOOCV\", \"LGOCV\", \"timeslice\", \"adaptive_cv\", \"adaptive_boot\" o \"adaptive_LGOCV\" tuneLength y tuneGrid: permite establecer cuantos hiperparámetros serán evaluados (por defecto 3) o una rejilla con las combinaciones de hiperparámetros. ... permite establecer opciones específicas de los métodos. También admite matrices x, y en lugar de fórmulas (o recetas: recipe()). Si se imputan datos en el preprocesado será necesario establecer na.action = na.pass. Predicción: Una de las ventajas es que incorpora un único método predict() para objetos de tipo train con dos únicas opciones12 type = c(\"raw\", \"prob\"), la primera para obtener predicciones de la respuesta y la segunda para obtener estimaciones de las probabilidades (en los métodos de clasificación que lo admitan). Además, si se incluyo un preprocesado en el entrenamiento, se emplearán las mismas transformaciones en un nuevo conjunto de datos newdata. Evaluación de los modelos postResample(pred, obs, ...): regresión confusionMatrix(pred, obs, ...): clasificación Funciones auxiliares: twoClassSummary(), prSummary() Analisis de la importancia de los predictores: varImp(): interfaz a las medidas específicas de los métodos de aprendizaje supervisado (Sección 15.1) o medidas genéricas (Sección 15.2). Ejemplo regresión con KNN: # caret data(Boston, package = &quot;MASS&quot;) library(caret) # Partición set.seed(1) itrain &lt;- createDataPartition(Boston$medv, p = 0.8, list = FALSE) train &lt;- Boston[itrain, ] test &lt;- Boston[-itrain, ] # Entrenamiento y selección de hiperparámetros set.seed(1) knn &lt;- train(medv ~ ., data = train, method = &quot;knn&quot;, preProc = c(&quot;center&quot;, &quot;scale&quot;), tuneGrid = data.frame(k = 1:10), trControl = trainControl(method = &quot;cv&quot;, number = 10)) plot(knn) ggplot(knn, highlight = TRUE) knn$bestTune ## k ## 3 3 knn$finalModel ## 3-nearest neighbor regression model # Interpretación varImp(knn) ## loess r-squared variable importance ## ## Overall ## lstat 100.00 ## rm 88.26 ## indus 36.29 ## ptratio 33.27 ## tax 30.58 ## crim 28.33 ## nox 23.44 ## black 21.29 ## age 20.47 ## rad 17.16 ## zn 15.11 ## dis 14.35 ## chas 0.00 # Evaluación postResample(predict(knn, newdata = test), test$medv) ## RMSE Rsquared MAE ## 4.960971 0.733945 2.724242 Un comentario final: While Im still supporting caret, the majority of my development effort has gone into the tidyverse modeling packages (called tidymodels).  Max Kuhn, autor del paquete caret (actualmente ingeniero de software en RStudio). Kuhn, M. y Wickham, H. (2020). Tidymodels: a collection of packages for modeling and machine learning using tidyverse principles. Version 0.1.1 (2020-07-14). https://www.tidymodels.org. En lugar de la variedad de opciones que emplean los distintos paquetes (e.g.: type = \"response\", \"class\", \"posterior\", \"probability\" ). "],["G-trees.html", "G Árboles de decisión", " G Árboles de decisión Los árboles de decisión son uno de los métodos más simples y fáciles de interpretar para realizar predicciones en problemas de clasificación y de regresión. Se desarrollan a partir de los años 70 del siglo pasado como una alternativa versátil a los métodos clásicos de la estadística, fuertemente basados en las hipótesis de linealidad y de normalidad, y enseguida se convierten en una técnica básica del aprendizaje automático. Aunque su calidad predictiva es mediocre (especialmente en el caso de regresión), constituyen la base de otros métodos altamente competitivos (bagging, bosques aleatorios, boosting) en los que se combinan múltiples árboles para mejorar la predicción, pagando el precio, eso sí, de hacer más difícil la interpretación del modelo resultante. La idea de este método consiste en la segmentación (partición) del espacio predictor (es decir, del conjunto de posibles valores de las variables predictoras) en regiones tan simples que el proceso se pueda representar mediante un árbol binario. Se parte de un nodo inicial que representa a toda la muestra (se utiliza la muestra de entrenamiento), del que salen dos ramas que dividen la muestra en dos subconjuntos, cada uno representado por un nuevo nodo. Este proceso se repite un número finito de veces hasta obtener las hojas del árbol, es decir, los nodos terminales, que son los que se utilizan para realizar la predicción. Una vez construido el árbol, la predicción se realizará en cada nodo terminal utilizando, típicamente, la media en un problema de regresión y la moda en un problema de clasificación. Al final de este proceso iterativo el espacio predictor se ha particionado en regiones de forma rectangular en la que la predicción de la respuesta es constante. Si la relación entre las variables predictoras y la variable respuesta no se puede describir adecuadamente mediante rectángulos, la calidad predictiva del árbol será limitada. Como vemos, la simplicidad del modelo es su principal argumento, pero también su talón de Aquiles. Como se ha dicho antes, cada nodo padre se divide, a través de dos ramas, en dos nodos hijos. Esto se hace seleccionando una variable predictora y dando respuesta a una pregunta dicotómica sobre ella. Por ejemplo, ¿es el sueldo anual menor que 30000 euros?, o ¿es el género igual a mujer? Lo que se persigue con esta partición recursiva es que los nodos terminales sean homogéneos respecto a la variable respuesta \\(Y\\). Por ejemplo, en un problema de clasificación, la homogeneidad de los nodos terminales significaría que en cada uno de ellos sólo hay elementos de una clase (categoría), y diríamos que los nodos son puros. En la práctica, esto siempre se puede conseguir construyendo árboles suficientemente profundos, con muchas hojas. Pero esta solución no es interesante, ya que va a dar lugar a un modelo excesivamente complejo y por tanto sobreajustado y de difícil interpretación. Será necesario encontrar un equilibrio entre la complejidad del árbol y la pureza de los nodos terminales. En resumen: Métodos simples y fácilmente interpretables. Se representan mediante árboles binarios. Técnica clásica de apendizaje automático (computación). Válidos para regresión y para clasificación. Válidos para predictores numéricos y categóricos. La metodología CART (Classification and Regresion Trees, Breiman et al., 1984) es la más popular para la construcción de árboles de decisión y es la que se va a explicar con algo de detalle en las siguientes secciones. En primer lugar se tratarán los árboles de regresión (árboles de decisión en un problema de regresión, en el que la variable respuesta \\(Y\\) es numérica) y después veremos los arboles de clasificación (respuesta categórica) que son los más utilizados en la práctica (los primeros se suelen emplear únicamente como métodos descriptivos o como base de métodos más complejos). Las variables predictoras \\(\\mathbf{X}=(X_1, X_2, \\ldots, X_p)\\) pueden ser tanto numéricas como categóricas. Además, con la metodología CART, las variables explicativas podrían contener datos faltantes. Se pueden establecer particiones sustitutas (surrogate splits), de forma que cuando falta un valor en una variable que determina una división, se usa una variable alternativa que produce una partición similar. "],["G-1-árboles-de-regresión-cart.html", "G.1 Árboles de regresión CART", " G.1 Árboles de regresión CART Como ya se comentó, la construcción del modelo se hace a partir de la muestra de entrenamiento, y consiste en la partición del espacio predictor en \\(J\\) regiones \\(R_1, R_2, \\ldots, R_J\\), para cada una de las cuales se va a calcular una constante: la media de la variable respuesta \\(Y\\) para las observaciones de entranamiento que caen en la región. Estas constantes son las que se van a utilizar para la predicción de nuevas observaciones; para ello solo hay que comprobar cuál es la región que le corresponde. La cuestión clave es cómo se elige la partición del espacio predictor, para lo que vamos a utilizar como criterio de error el RSS (suma de los residuos al cuadrado). Como hemos dicho, vamos a modelizar la respuesta en cada región como una constante, por tanto en la región \\(R_j\\) nos interesa el \\(min_{c_j} \\sum_{i\\in R_j} (y_i - c_j)^2\\), que se alcanza en la media de las respuestas \\(y_i\\) (de la muestra de entrenamiento) en la región \\(R_j\\), a la que llamaremos \\(\\widehat y_{R_j}\\). Por tanto, se deben seleccionar las regiones \\(R_1, R_2, \\ldots, R_J\\) que minimicen \\[RSS = \\sum_{j=1}^{J} \\sum_{i\\in R_j} (y_i - \\widehat y_{R_j})^2\\] (Obsérvese el abuso de notación \\(i\\in R_j\\), que significa las observaciones \\(i\\in N\\) que verifican \\(x_i \\in R_j\\)). Pero este problema es, en la práctica, intratable y vamos a tener que simplificarlo. El método CART busca un compromiso entre rendimiento, por una parte, y sencillez e interpretabilidad, por otra, y por ello en lugar de hacer una búsqueda por todas las particiones posibles sigue un proceso iterativo (recursivo) en el que va realizando cortes binarios. En la primera iteración se trabaja con todos los datos: Una variable explicativa \\(X_j\\) y un punto de corte \\(s\\) definen dos hiperplanos \\(R_1 = \\{ X \\mid X_j \\le s \\}\\) y \\(R_2 = \\{ X \\mid X_j &gt; s \\}\\). Se seleccionan los valores de \\(j\\) y \\(s\\) que minimizen \\[ \\sum_{i\\in R_1} (y_i - \\widehat y_{R_1})^2 + \\sum_{i\\in R_2} (y_i - \\widehat y_{R_2})^2\\] A diferencia del problema original, este se soluciona de forma muy rápida. A continuación se repite el proceso en cada una de las dos regiones \\(R_1\\) y \\(R_2\\), y así sucesivamente hasta alcanzar un criterio de parada. Fijémonos en que este método hace dos concesiones importantes: no solo restringe la forma que pueden adoptar las particiones, sino que además sigue un criterio de error greedy: en cada iteración busca minimizar el RSS de las dos regiones resultantes, sin preocuparse del error que se va a cometer en iteraciones sucesivas. Y fijémonos también en que este proceso se puede representar en forma de árbol binario (en el sentido de que de cada nodo salen dos ramas, o ninguna cuando se llega al final), de ahí la terminología de hacer crecer el árbol. ¿Y cuándo paramos? Se puede parar cuando se alcance una profundidad máxima, aunque lo más habitual es, para dividir un nodo (es decir, una región), exigirle un número mínimo de observaciones. Si el árbol resultante es demasiado grande, va a ser un modelo demasiado complejo, por tanto va a ser difícil de interpretar y, sobre todo, va a provocar un sobreajuste de los datos. Cuando se evalúe el rendimiento utilizando la muestra de validación, los resultados van a ser malos. Dicho de otra manera, tendremos un modelo con poco sesgo pero con mucha varianza y en consecuencia inestable (pequeños cambios en los datos darán lugar a modelos muy distintos). Más adelante veremos que esto justifica la utilización del bagging como técnica para reducir la varianza. Si el árbol es demasiado pequeño, va a tener menos varianza (menos inestable) a costa de más sesgo. Más adelante veremos que esto justifica la utilización del boosting. Los árboles pequeños son más fáciles de interpretar ya que permiten identificar las variables explicativas que más influyen en la predicción. Sin entrar por ahora en métodos combinados (métodos ensemble, tipo bagging o boosting), vamos a explicar cómo encontrar un equilibrio entre sesgo y varianza. Lo que se hace es construir un árbol grande para a continuación empezar a podarlo. Podar un árbol significa colapsar cualquier cantidad de sus nodos internos (no terminales), dando lugar a otro árbol más pequeño al que llamaremos subárbol del árbol original. Sabemos que el árbol completo es el que va a tener menor error si utilizamos la muestra de entrenamiento, pero lo que realmente nos interesa es encontrar el subárbol con un menor error al utilizar la muestra de validación. Lamentablemente, no es una buena estrategia el evaluar todos los subárboles: simplemente, hay demasiados. Lo que se hace es, mediante un hiperparámetro (tuning parameter o parámetro de ajuste) controlar el tamaño del árbol, es decir, la complejidad del modelo, seleccionando el subárbol optimo (para los datos de los que disponemos, claro). Veamos la idea. Dado un subárbol \\(T\\) con \\(R_1, R_2, \\ldots, R_t\\) nodos terminales, consideramos como medida del error el RSS más una penalización que depende de un hiperparámetro no negativo \\(\\alpha \\ge 0\\) \\[\\begin{equation} RSS_{\\alpha} = \\sum_{j=1}^t \\sum_{i\\in R_j} (y_i - \\widehat y_{R_j})^2 + \\alpha t \\tag{G.1} \\end{equation}\\] Para cada valor del parámetro \\(\\alpha\\) existe un único subárbol más pequeño que minimiza este error (obsérvese que aunque hay un continuo de valores distinos de \\(\\alpha\\), sólo hay una cantidad finita de subárboles). Evidentemente, cuando \\(\\alpha = 0\\), ese subárbol será el árbol completo, algo que no nos interesa. Pero a medida que se incrementa \\(\\alpha\\) se penalizan los subárboles con muchos nodos terminales, dando lugar a una solución más pequeña. Encontrarla puede parecer muy costoso computacionalmente, pero lo cierto es que no lo es. El algoritmo consistente en ir colapsando nodos de forma sucesiva, de cada vez el nodo que produzca el menor incremento en el RSS (corregido por un factor que depende del tamaño), da lugar a una sucesión finita de subárboles que contiene, para todo \\(\\alpha\\), la solución. Para finalizar, sólo resta seleccionar un valor de \\(\\alpha\\). Para ello, como se comentó en la Sección F.3.2, se podría dividir la muestra en tres subconjuntos: datos de entrenamiento, de validación y de test. Para cada valor del parámetro de complejidad \\(\\alpha\\) hemos utilizado la muestra de entrenamiento para obtener un árbol (en la jerga, para cada valor del hiperparámetro \\(\\alpha\\) se entrena un modelo). Se emplea la muestra independiente de validación para seleccionar el valor de \\(\\alpha\\) (y por tanto el árbol) con el que nos quedamos. Y por último emplearemos la muestra de test (independiente de las otras dos) para evaluar el rendimiento del árbol seleccionado. No obstante, lo más habitual para seleccionar el valor del hiperparámetro \\(\\alpha\\) es emplear validación cruzada (o otro tipo de remuestreo) en la muestra de entrenamiento en lugar de considerar una muestra adicional de validación. Hay dos opciones muy utilizadas en la práctica para seleccionar el valor de \\(\\alpha\\): se puede utilizar directamente el valor que minimice el error; o se puede forzar que el modelo sea un poco más sencillo con la regla one-standard-error, que selecciona el árbol más pequeño que esté a una distancia de un error estándar del árbol obtenido mediante la opción anterior. También es habitual escribir la Ecuación (G.1) reescalando el parámetro de complejidad como \\(\\tilde \\alpha = \\alpha / RSS_0\\), siendo \\(RSS_0 = \\sum_{i=1}^{n} (y_i - \\bar y)^2\\) la variabilidad total (la suma de cuadrados residual del árbol sin divisiones): \\[RSS_{\\tilde \\alpha}=RSS + \\tilde \\alpha RSS_0 t\\] De esta forma se podría interpretar el hiperparámetro \\(\\tilde \\alpha\\) como una penalización en la proporción de variabilidad explicada, ya que dividiendo la expresión anterior por \\(RSS_0\\) obtendríamos: \\[R^2_{\\tilde \\alpha}=R^2+ \\tilde \\alpha t\\] "],["G-2-árboles-de-clasificación-cart.html", "G.2 Árboles de clasificación CART", " G.2 Árboles de clasificación CART En un problema de clasificación la variable respuesta puede tomar los valores \\(1, 2, \\ldots, K\\), etiquetas que identifican las \\(K\\) categorías del problema. Una vez construido el árbol, se comprueba cuál es la categoría modal de cada región: considerando la muestra de entrenamiento, la categoría más frecuente. Dada una observación, se predice que pertenece a la categoría modal de la región a la que pertenece. El resto del proceso es idéntico al de los árboles de regresión ya explicado, con una única salvedad: no podemos utilizar RSS como medida del error. Es necesario buscar una medida del error adaptada a este contexto. Fijada una región, vamos a denotar por \\(\\widehat p_{k}\\), con \\(k = 1, 2, \\ldots, K\\), a la proporción de observaciones (de la muestra de entrenamiento) en la región que pertenecen a la categoría \\(k\\). Se utilizan tres medidas distintas del error en la región: Proporción de errores de clasificación: \\[1 - max_{k} (\\widehat p_{k})\\] Índice de Gini: \\[\\sum_{k=1}^K \\widehat p_{k} (1 - \\widehat p_{k})\\] Entropía13 (cross-entropy): \\[- \\sum_{k=1}^K \\widehat p_{k} \\text{log}(\\widehat p_{k})\\] Aunque la proporción de errores de clasificación es la medida del error más intuitiva, en la práctica sólo se utiliza para la fase de poda. Fijémonos que en el cálculo de esta medida sólo interviene \\(max_{k} (\\widehat p_{k})\\), mientras que en las medidas alternativas intervienen las proporciones \\(\\widehat p_{k}\\) de todas las categorías. Para la fase de crecimiento se utilizan indistintamente el índice de Gini o la entropía. Cuando nos interesa el error no en una única región sino en varias (al romper un nodo en dos, o al considerar todos los nodos terminales), se suman los errores de cada región previa ponderación por el número de observaciones que hay en cada una de ellas. En la introducción de este tema se comentó que los árboles de decisión admiten tanto variables predictoras numéricas como categóricas, y esto es cierto tanto para árboles de regresión como para árboles de clasificación. Veamos brevemente como se tratarían los predictores categóricos a la hora de incorporarlos al árbol. El problema radica en qué se entiende por hacer un corte si las categorías del predictor no están ordenadas. Hay dos soluciones básicas: Definir variables predictoras dummy. Se trata de variables indicadoras, una por cada una de las categorías que tiene el predictor. Este criterio de uno contra todos tiene la ventaja de que estas variables son fácilmente interpretables, pero tiene el inconveniente de que puede aumentar mucho el número de variables predictoras. Ordenar las categorías de la variable predictora. Lo ideal sería considerar todas las ordenaciones posibles, pero eso es desde luego poco práctico: el incremento es factorial. El truco consiste en utilizar un único órden basado en algún criterio greedy. Por ejemplo, si la variable respuesta \\(Y\\) también es categórica, se puede seleccionar una de sus categorías que resulte especialmente interesante y ordenar las categorías del predictor según su proporción en la categoría de \\(Y\\). Este enfoque no añade complejidad al modelo, pero puede dar lugar a resultados de difícil interpretación. La entropía es un concepto básico de la teoría de la información (Shannon, 1948) y se mide en bits (cuando en la definición se utilizan \\(log_2\\)). "],["G-3-cart-con-el-paquete-rpart.html", "G.3 CART con el paquete rpart", " G.3 CART con el paquete rpart La metodología CART está implementada en el paquete rpart (Recursive PARTitioning)14. La función principal es rpart() y habitualmente se emplea de la forma: rpart(formula, data, method, parms, control, ...) formula: permite especificar la respuesta y las variables predictoras de la forma habitual, se suele establecer de la forma respuesta ~ . para incluir todas las posibles variables explicativas. data: data.frame (opcional; donde se evaluará la fórmula) con la muestra de entrenamiento. method: método empleado para realizar las particiones, puede ser \"anova\" (regresión), \"class\" (clasificación), \"poisson\" (regresión de Poisson) o \"exp\" (supervivencia), o alternativamente una lista de funciones (con componentes init, split, eval; ver la vignette User Written Split Functions). Por defecto se selecciona a partir de la variable respuesta en formula, por ejemplo si es un factor (lo recomendado en clasificación) emplea method = \"class\". parms: lista de parámetros opcionales para la partición en el caso de clasificación (o regresión de Poisson). Puede contener los componentes prior (vector de probabilidades previas; por defecto las frecuencias observadas), loss (matriz de pérdidas; con ceros en la diagonal y por defecto 1 en el resto) y split (criterio de error; por defecto \"gini\" o alternativamente \"information\"). control: lista de opciones que controlan el algoritmo de partición, por defecto se seleccionan mediante la función rpart.control, aunque también se pueden establecer en la llamada a la función principal, y los principales parámetros son: rpart.control(minsplit = 20, minbucket = round(minsplit/3), cp = 0.01, xval = 10, maxdepth = 30, ...) cp es el parámetro de complejidad \\(\\tilde \\alpha\\) para la poda del árbol, de forma que un valor de 1 se corresponde con un árbol sin divisiones y un valor de 0 con un árbol de profundidad máxima. Adicionalmente, para reducir el tiempo de computación, el algoritmo empleado no realiza una partición si la proporción de reducción del error es inferior a este valor (valores más grandes simplifican el modelo y reducen el tiempo de computación). maxdepth es la profundidad máxima del árbol (la profundidad de la raíz sería 0). minsplit y minbucket son, respectivamente, los números mínimos de observaciones en un nodo intermedio para particionarlo y en un nodo terminal. xval es el número de grupos (folds) para validación cruzada. Para más detalles consultar la documentación de esta función o la vignette Introduction to Rpart. G.3.1 Ejemplo: regresión Emplearemos el conjunto de datos winequality.RData (ver Cortez et al., 2009), que contiene información fisico-química (fixed.acidity, volatile.acidity, citric.acid, residual.sugar, chlorides, free.sulfur.dioxide, total.sulfur.dioxide, density, pH, sulphates y alcohol) y sensorial (quality) de una muestra de 1250 vinos portugueses de la variedad Vinho Verde. Como respuesta consideraremos la variable quality, mediana de al menos 3 evaluaciones de la calidad del vino realizadas por expertos, que los evaluaron entre 0 (muy malo) y 10 (muy excelente). load(&quot;datos/winequality.RData&quot;) str(winequality) ## &#39;data.frame&#39;: 1250 obs. of 12 variables: ## $ fixed.acidity : num 6.8 7.1 6.9 7.5 8.6 7.7 5.4 6.8 6.1 5.5 ... ## $ volatile.acidity : num 0.37 0.24 0.32 0.23 0.36 0.28 0.59 0.16 0.28 0.28 ... ## $ citric.acid : num 0.47 0.34 0.13 0.49 0.26 0.63 0.07 0.36 0.27 0.21 ... ## $ residual.sugar : num 11.2 1.2 7.8 7.7 11.1 11.1 7 1.3 4.7 1.6 ... ## $ chlorides : num 0.071 0.045 0.042 0.049 0.03 0.039 0.045 0.034 0.03 0.032 ... ## $ free.sulfur.dioxide : num 44 6 11 61 43.5 58 36 32 56 23 ... ## $ total.sulfur.dioxide: num 136 132 117 209 171 179 147 98 140 85 ... ## $ density : num 0.997 0.991 0.996 0.994 0.995 ... ## $ pH : num 2.98 3.16 3.23 3.14 3.03 3.08 3.34 3.02 3.16 3.42 ... ## $ sulphates : num 0.88 0.46 0.37 0.3 0.49 0.44 0.57 0.58 0.42 0.42 ... ## $ alcohol : num 9.2 11.2 9.2 11.1 12 8.8 9.7 11.3 12.5 12.5 ... ## $ quality : int 5 4 5 7 5 4 6 6 8 5 ... barplot(table(winequality$quality)) En primer lugar se selecciona el 80% de los datos como muestra de entrenamiento y el 20% restante como muestra de test: set.seed(1) nobs &lt;- nrow(winequality) itrain &lt;- sample(nobs, 0.8 * nobs) train &lt;- winequality[itrain, ] test &lt;- winequality[-itrain, ] Podemos obtener el arbol con las opciones por defecto con el comando: tree &lt;- rpart(quality ~ ., data = train) Al imprimirlo se muestra el número de observaciones e información sobre los distintos nodos (número de nodo, condición que define la partición, número de observaciones en el nodo, función de pérdida y predicción), marcando con un * los nodos terminales. tree ## n= 1000 ## ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 1000 768.95600 5.862000 ## 2) alcohol&lt; 10.75 622 340.81190 5.586817 ## 4) volatile.acidity&gt;=0.2575 329 154.75990 5.370821 ## 8) total.sulfur.dioxide&lt; 98.5 24 12.50000 4.750000 * ## 9) total.sulfur.dioxide&gt;=98.5 305 132.28200 5.419672 ## 18) pH&lt; 3.315 269 101.44980 5.353160 * ## 19) pH&gt;=3.315 36 20.75000 5.916667 * ## 5) volatile.acidity&lt; 0.2575 293 153.46760 5.829352 ## 10) sulphates&lt; 0.475 144 80.32639 5.659722 * ## 11) sulphates&gt;=0.475 149 64.99329 5.993289 * ## 3) alcohol&gt;=10.75 378 303.53700 6.314815 ## 6) alcohol&lt; 11.775 200 173.87500 6.075000 ## 12) free.sulfur.dioxide&lt; 11.5 15 10.93333 4.933333 * ## 13) free.sulfur.dioxide&gt;=11.5 185 141.80540 6.167568 ## 26) volatile.acidity&gt;=0.395 7 12.85714 5.142857 * ## 27) volatile.acidity&lt; 0.395 178 121.30900 6.207865 ## 54) citric.acid&gt;=0.385 31 21.93548 5.741935 * ## 55) citric.acid&lt; 0.385 147 91.22449 6.306122 * ## 7) alcohol&gt;=11.775 178 105.23600 6.584270 * Para representarlo se puede emplear las herramientas del paquete rpart: plot(tree) text(tree) Pero puede ser preferible emplear el paquete rpart.plot library(rpart.plot) rpart.plot(tree, main=&quot;Regresion tree winequality&quot;) Nos interesa como se clasificaría a una nueva observación en los nodos terminales (en los nodos intermedios solo nos interesarían las condiciones, y el orden de las variables consideradas, hasta llegar a las hojas) y las correspondientes predicciones (la media de la respuesta en el correspondiente nodo terminal). Para ello, puede ser de utilidad imprimir las reglas: rpart.rules(tree, style = &quot;tall&quot;) ## quality is 4.8 when ## alcohol &lt; 11 ## volatile.acidity &gt;= 0.26 ## total.sulfur.dioxide &lt; 99 ## ## quality is 4.9 when ## alcohol is 11 to 12 ## free.sulfur.dioxide &lt; 12 ## ## quality is 5.1 when ## alcohol is 11 to 12 ## volatile.acidity &gt;= 0.40 ## free.sulfur.dioxide &gt;= 12 ## ## quality is 5.4 when ## alcohol &lt; 11 ## volatile.acidity &gt;= 0.26 ## total.sulfur.dioxide &gt;= 99 ## pH &lt; 3.3 ## ## quality is 5.7 when ## alcohol &lt; 11 ## volatile.acidity &lt; 0.26 ## sulphates &lt; 0.48 ## ## quality is 5.7 when ## alcohol is 11 to 12 ## volatile.acidity &lt; 0.40 ## free.sulfur.dioxide &gt;= 12 ## citric.acid &gt;= 0.39 ## ## quality is 5.9 when ## alcohol &lt; 11 ## volatile.acidity &gt;= 0.26 ## total.sulfur.dioxide &gt;= 99 ## pH &gt;= 3.3 ## ## quality is 6.0 when ## alcohol &lt; 11 ## volatile.acidity &lt; 0.26 ## sulphates &gt;= 0.48 ## ## quality is 6.3 when ## alcohol is 11 to 12 ## volatile.acidity &lt; 0.40 ## free.sulfur.dioxide &gt;= 12 ## citric.acid &lt; 0.39 ## ## quality is 6.6 when ## alcohol &gt;= 12 Por defecto se poda el arbol considerando cp = 0.01, que puede ser adecuado en muchos casos. Sin embargo, para seleccionar el valor óptimo de este (hiper)parámetro se puede emplear validación cruzada. En primer lugar habría que establecer cp = 0 para construir el árbol completo, a la profundidad máxima (determinada por los valores de minsplit y minbucket, que se podrían seleccionar a mano dependiendo del número de observaciones o también considerándolos como hiperparámetos; esto último no está implementado en rpart, ni en principio en caret)15. tree &lt;- rpart(quality ~ ., data = train, cp = 0) Posteriormente podemos emplear las funciones printcp() (o plotcp()) para obtener (representar) los valores de CP para los árboles (óptimos) de menor tamaño junto con su error de validación cruzada xerror (reescalado de forma que el máximo de rel error es 1)16: printcp(tree) ## ## Regression tree: ## rpart(formula = quality ~ ., data = train, cp = 0) ## ## Variables actually used in tree construction: ## [1] alcohol chlorides citric.acid ## [4] density fixed.acidity free.sulfur.dioxide ## [7] pH residual.sugar sulphates ## [10] total.sulfur.dioxide volatile.acidity ## ## Root node error: 768.96/1000 = 0.76896 ## ## n= 1000 ## ## CP nsplit rel error xerror xstd ## 1 0.16204707 0 1.00000 1.00203 0.048591 ## 2 0.04237491 1 0.83795 0.85779 0.043646 ## 3 0.03176525 2 0.79558 0.82810 0.043486 ## 4 0.02748696 3 0.76381 0.81350 0.042814 ## 5 0.01304370 4 0.73633 0.77038 0.039654 ## 6 0.01059605 6 0.71024 0.78168 0.039353 ## 7 0.01026605 7 0.69964 0.78177 0.039141 ## 8 0.00840800 9 0.67911 0.78172 0.039123 ## 9 0.00813924 10 0.67070 0.80117 0.039915 ## 10 0.00780567 11 0.66256 0.80020 0.040481 ## 11 0.00684175 13 0.64695 0.79767 0.040219 ## 12 0.00673843 15 0.63327 0.81381 0.040851 ## 13 0.00643577 18 0.61305 0.82059 0.041240 ## 14 0.00641137 19 0.60662 0.82323 0.041271 ## 15 0.00549694 21 0.59379 0.84187 0.042714 ## 16 0.00489406 23 0.58280 0.84748 0.042744 ## 17 0.00483045 24 0.57791 0.85910 0.043897 ## 18 0.00473741 25 0.57308 0.86553 0.045463 ## 19 0.00468372 26 0.56834 0.86455 0.045413 ## 20 0.00450496 28 0.55897 0.87049 0.045777 ## 21 0.00448365 32 0.54095 0.87263 0.045824 ## 22 0.00437484 33 0.53647 0.87260 0.045846 ## 23 0.00435280 35 0.52772 0.87772 0.046022 ## 24 0.00428623 36 0.52337 0.87999 0.046124 ## 25 0.00412515 37 0.51908 0.88151 0.046505 ## 26 0.00390866 39 0.51083 0.89242 0.047068 ## 27 0.00375301 42 0.49910 0.90128 0.047319 ## 28 0.00370055 43 0.49535 0.90965 0.047991 ## 29 0.00351987 45 0.48795 0.91404 0.048079 ## 30 0.00308860 47 0.48091 0.92132 0.048336 ## 31 0.00305781 49 0.47473 0.93168 0.049699 ## 32 0.00299018 51 0.46862 0.93258 0.049701 ## 33 0.00295148 52 0.46563 0.93062 0.049644 ## 34 0.00286138 54 0.45972 0.93786 0.050366 ## 35 0.00283972 55 0.45686 0.93474 0.050404 ## 36 0.00274809 56 0.45402 0.93307 0.050390 ## 37 0.00273457 58 0.44853 0.93642 0.050406 ## 38 0.00260607 59 0.44579 0.93726 0.050543 ## 39 0.00252978 60 0.44318 0.93692 0.050323 ## 40 0.00252428 62 0.43813 0.93778 0.050381 ## 41 0.00250804 64 0.43308 0.93778 0.050381 ## 42 0.00232226 65 0.43057 0.93642 0.050081 ## 43 0.00227625 66 0.42825 0.93915 0.050166 ## 44 0.00225146 67 0.42597 0.94101 0.050195 ## 45 0.00224774 68 0.42372 0.94101 0.050195 ## 46 0.00216406 69 0.42147 0.94067 0.050124 ## 47 0.00204851 70 0.41931 0.94263 0.050366 ## 48 0.00194517 72 0.41521 0.94203 0.050360 ## 49 0.00188139 73 0.41326 0.93521 0.050349 ## 50 0.00154129 75 0.40950 0.93500 0.050277 ## 51 0.00143642 76 0.40796 0.93396 0.050329 ## 52 0.00118294 77 0.40652 0.93289 0.050325 ## 53 0.00117607 78 0.40534 0.93738 0.050406 ## 54 0.00108561 79 0.40417 0.93738 0.050406 ## 55 0.00097821 80 0.40308 0.93670 0.050406 ## 56 0.00093107 81 0.40210 0.93752 0.050589 ## 57 0.00090075 82 0.40117 0.93752 0.050589 ## 58 0.00082968 83 0.40027 0.93634 0.050561 ## 59 0.00048303 85 0.39861 0.93670 0.050557 ## 60 0.00000000 86 0.39813 0.93745 0.050558 plotcp(tree) La tabla con los valores de las podas (óptimas, dependiendo del parámetro de complejidad) está almacenada en la componente $cptable: head(tree$cptable, 10) ## CP nsplit rel error xerror xstd ## 1 0.162047069 0 1.0000000 1.0020304 0.04859127 ## 2 0.042374911 1 0.8379529 0.8577876 0.04364585 ## 3 0.031765253 2 0.7955780 0.8281010 0.04348571 ## 4 0.027486958 3 0.7638128 0.8134957 0.04281430 ## 5 0.013043701 4 0.7363258 0.7703804 0.03965433 ## 6 0.010596054 6 0.7102384 0.7816774 0.03935308 ## 7 0.010266055 7 0.6996424 0.7817716 0.03914071 ## 8 0.008408003 9 0.6791102 0.7817177 0.03912344 ## 9 0.008139238 10 0.6707022 0.8011719 0.03991498 ## 10 0.007805674 11 0.6625630 0.8001996 0.04048088 A partir de la que podríamos seleccionar el valor óptimo de forma automática, siguiendo el criterio de un error estándar de Breiman et al. (1984): xerror &lt;- tree$cptable[,&quot;xerror&quot;] imin.xerror &lt;- which.min(xerror) # Valor óptimo tree$cptable[imin.xerror, ] ## CP nsplit rel error xerror xstd ## 0.01304370 4.00000000 0.73632581 0.77038039 0.03965433 # Límite superior &quot;oneSE rule&quot; y complejidad mínima por debajo de ese valor upper.xerror &lt;- xerror[imin.xerror] + tree$cptable[imin.xerror, &quot;xstd&quot;] icp &lt;- min(which(xerror &lt;= upper.xerror)) cp &lt;- tree$cptable[icp, &quot;CP&quot;] Para obtener el modelo final podamos el arbol con el valor de complejidad obtenido 0.0130437 (que en este caso coincide con el valor óptimo): tree &lt;- prune(tree, cp = cp) rpart.plot(tree, main=&quot;Regresion tree winequality&quot;) Podríamos estudiar el modelo final, por ejemplo mediante el método summary(), que entre otras cosas muestra una medida (en porcentaje) de la importancia de las variables explicativas para la predicción de la respuesta (teniendo en cuenta todas las particiones, principales y secundarias, en las que se emplea cada variable explicativa). Alternativamente podríamos emplear el siguiente código: # summary(tree) importance &lt;- tree$variable.importance # Equivalente a caret::varImp(tree) importance &lt;- round(100*importance/sum(importance), 1) importance[importance &gt;= 1] ## alcohol density chlorides ## 36.1 21.7 11.3 ## volatile.acidity total.sulfur.dioxide free.sulfur.dioxide ## 8.7 8.5 5.0 ## residual.sugar sulphates citric.acid ## 4.0 1.9 1.1 ## pH ## 1.1 El último paso sería evaluarlo en la muestra de test siguiendo los pasos descritos en la Sección F.3.4: obs &lt;- test$quality pred &lt;- predict(tree, newdata = test) # plot(pred, obs, main = &quot;Observado frente a predicciones (quality)&quot;, # xlab = &quot;Predicción&quot;, ylab = &quot;Observado&quot;) plot(jitter(pred), jitter(obs), main = &quot;Observado frente a predicciones (quality)&quot;, xlab = &quot;Predicción&quot;, ylab = &quot;Observado&quot;) abline(a = 0, b = 1) # Empleando el paquete caret caret::postResample(pred, obs) ## RMSE Rsquared MAE ## 0.8145614 0.1969485 0.6574264 # Con la función accuracy() accuracy &lt;- function(pred, obs, na.rm = FALSE, tol = sqrt(.Machine$double.eps)) { err &lt;- obs - pred # Errores if(na.rm) { is.a &lt;- !is.na(err) err &lt;- err[is.a] obs &lt;- obs[is.a] } perr &lt;- 100*err/pmax(obs, tol) # Errores porcentuales return(c( me = mean(err), # Error medio rmse = sqrt(mean(err^2)), # Raíz del error cuadrático medio mae = mean(abs(err)), # Error absoluto medio mpe = mean(perr), # Error porcentual medio mape = mean(abs(perr)), # Error porcentual absoluto medio r.squared = 1 - sum(err^2)/sum((obs - mean(obs))^2) )) } accuracy(pred, test$quality) ## me rmse mae mpe mape r.squared ## -0.001269398 0.814561435 0.657426365 -1.952342173 11.576716037 0.192007721 G.3.2 Ejemplo: modelo de clasificación Para ilustrar los árboles de clasificación CART, podemos emplear los datos anteriores de calidad de vino, considerando como respuesta una nueva variable taste que clasifica los vinos en good o bad dependiendo de si winequality$quality &gt;= 5 (este conjunto de datos está almacenado en el archivo winetaste.RData). # load(&quot;datos/winetaste.RData&quot;) winetaste &lt;- winequality[, colnames(winequality)!=&quot;quality&quot;] winetaste$taste &lt;- factor(winequality$quality &lt; 6, labels = c(&#39;good&#39;, &#39;bad&#39;)) # levels = c(&#39;FALSE&#39;, &#39;TRUE&#39;) str(winetaste) ## &#39;data.frame&#39;: 1250 obs. of 12 variables: ## $ fixed.acidity : num 6.8 7.1 6.9 7.5 8.6 7.7 5.4 6.8 6.1 5.5 ... ## $ volatile.acidity : num 0.37 0.24 0.32 0.23 0.36 0.28 0.59 0.16 0.28 0.28 ... ## $ citric.acid : num 0.47 0.34 0.13 0.49 0.26 0.63 0.07 0.36 0.27 0.21 ... ## $ residual.sugar : num 11.2 1.2 7.8 7.7 11.1 11.1 7 1.3 4.7 1.6 ... ## $ chlorides : num 0.071 0.045 0.042 0.049 0.03 0.039 0.045 0.034 0.03 0.032 ... ## $ free.sulfur.dioxide : num 44 6 11 61 43.5 58 36 32 56 23 ... ## $ total.sulfur.dioxide: num 136 132 117 209 171 179 147 98 140 85 ... ## $ density : num 0.997 0.991 0.996 0.994 0.995 ... ## $ pH : num 2.98 3.16 3.23 3.14 3.03 3.08 3.34 3.02 3.16 3.42 ... ## $ sulphates : num 0.88 0.46 0.37 0.3 0.49 0.44 0.57 0.58 0.42 0.42 ... ## $ alcohol : num 9.2 11.2 9.2 11.1 12 8.8 9.7 11.3 12.5 12.5 ... ## $ taste : Factor w/ 2 levels &quot;good&quot;,&quot;bad&quot;: 2 2 2 1 2 2 1 1 1 2 ... table(winetaste$taste) ## ## good bad ## 828 422 Como en el caso anterior, se contruyen las muestras de entrenamiento (80%) y de test (20%): # set.seed(1) # nobs &lt;- nrow(winetaste) # itrain &lt;- sample(nobs, 0.8 * nobs) train &lt;- winetaste[itrain, ] test &lt;- winetaste[-itrain, ] Al igual que en el caso anterior podemos obtener el árbol de clasificación con las opciones por defecto (cp = 0.01 y split = \"gini\") con el comando: tree &lt;- rpart(taste ~ ., data = train) En este caso al imprimirlo como información de los nodos se muestra (además del número de nodo, la condición de la partición y el número de observaciones en el nodo) el número de observaciones mal clasificadas, la predicción y las proporciones estimadas (frecuencias relativas en la muestra de entrenamiento) de las clases: tree ## n= 1000 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 1000 338 good (0.6620000 0.3380000) ## 2) alcohol&gt;=10.11667 541 100 good (0.8151571 0.1848429) ## 4) free.sulfur.dioxide&gt;=8.5 522 87 good (0.8333333 0.1666667) ## 8) fixed.acidity&lt; 8.55 500 73 good (0.8540000 0.1460000) * ## 9) fixed.acidity&gt;=8.55 22 8 bad (0.3636364 0.6363636) * ## 5) free.sulfur.dioxide&lt; 8.5 19 6 bad (0.3157895 0.6842105) * ## 3) alcohol&lt; 10.11667 459 221 bad (0.4814815 0.5185185) ## 6) volatile.acidity&lt; 0.2875 264 102 good (0.6136364 0.3863636) ## 12) fixed.acidity&lt; 7.45 213 71 good (0.6666667 0.3333333) ## 24) citric.acid&gt;=0.265 160 42 good (0.7375000 0.2625000) * ## 25) citric.acid&lt; 0.265 53 24 bad (0.4528302 0.5471698) ## 50) free.sulfur.dioxide&lt; 42.5 33 13 good (0.6060606 0.3939394) * ## 51) free.sulfur.dioxide&gt;=42.5 20 4 bad (0.2000000 0.8000000) * ## 13) fixed.acidity&gt;=7.45 51 20 bad (0.3921569 0.6078431) ## 26) total.sulfur.dioxide&gt;=150 26 10 good (0.6153846 0.3846154) * ## 27) total.sulfur.dioxide&lt; 150 25 4 bad (0.1600000 0.8400000) * ## 7) volatile.acidity&gt;=0.2875 195 59 bad (0.3025641 0.6974359) ## 14) pH&gt;=3.235 49 24 bad (0.4897959 0.5102041) ## 28) chlorides&lt; 0.0465 18 4 good (0.7777778 0.2222222) * ## 29) chlorides&gt;=0.0465 31 10 bad (0.3225806 0.6774194) * ## 15) pH&lt; 3.235 146 35 bad (0.2397260 0.7602740) * También puede ser preferible emplear el paquete rpart.plot para representarlo: library(rpart.plot) rpart.plot(tree, main=&quot;Classification tree winetaste&quot;) # Alternativa: rattle::fancyRpartPlot rpart.plot(tree, main=&quot;Classification tree winetaste&quot;, extra = 104, # show fitted class, probs, percentages box.palette = &quot;GnBu&quot;, # color scheme branch.lty = 3, # dotted branch lines shadow.col = &quot;gray&quot;, # shadows under the node boxes nn = TRUE) # display the node numbers Nos interesa como se clasificaría a una nueva observación (como se llega a los nodos terminales) y su probabilidad estimada (la frecuencia relativa de la clase más frecuente en el correspondiente nodo terminal). Al igual que en el caso de regresión, puede ser de utilidad imprimir las reglas: rpart.rules(tree, style = &quot;tall&quot;) ## taste is 0.15 when ## alcohol &gt;= 10 ## fixed.acidity &lt; 8.6 ## free.sulfur.dioxide &gt;= 8.5 ## ## taste is 0.22 when ## alcohol &lt; 10 ## volatile.acidity &gt;= 0.29 ## pH &gt;= 3.2 ## chlorides &lt; 0.047 ## ## taste is 0.26 when ## alcohol &lt; 10 ## volatile.acidity &lt; 0.29 ## fixed.acidity &lt; 7.5 ## citric.acid &gt;= 0.27 ## ## taste is 0.38 when ## alcohol &lt; 10 ## volatile.acidity &lt; 0.29 ## fixed.acidity &gt;= 7.5 ## total.sulfur.dioxide &gt;= 150 ## ## taste is 0.39 when ## alcohol &lt; 10 ## volatile.acidity &lt; 0.29 ## fixed.acidity &lt; 7.5 ## free.sulfur.dioxide &lt; 42.5 ## citric.acid &lt; 0.27 ## ## taste is 0.64 when ## alcohol &gt;= 10 ## fixed.acidity &gt;= 8.6 ## free.sulfur.dioxide &gt;= 8.5 ## ## taste is 0.68 when ## alcohol &lt; 10 ## volatile.acidity &gt;= 0.29 ## pH &gt;= 3.2 ## chlorides &gt;= 0.047 ## ## taste is 0.68 when ## alcohol &gt;= 10 ## free.sulfur.dioxide &lt; 8.5 ## ## taste is 0.76 when ## alcohol &lt; 10 ## volatile.acidity &gt;= 0.29 ## pH &lt; 3.2 ## ## taste is 0.80 when ## alcohol &lt; 10 ## volatile.acidity &lt; 0.29 ## fixed.acidity &lt; 7.5 ## free.sulfur.dioxide &gt;= 42.5 ## citric.acid &lt; 0.27 ## ## taste is 0.84 when ## alcohol &lt; 10 ## volatile.acidity &lt; 0.29 ## fixed.acidity &gt;= 7.5 ## total.sulfur.dioxide &lt; 150 Al igual que en el caso anterior, para seleccionar un valor óptimo del (hiper)parámetro de complejidad, se puede construir un árbol de decisión completo y emplear validación cruzada para podarlo. Además, si el número de observaciones es grande y las clases están más o menos balanceadas, se podría aumentar los valores mínimos de observaciones en los nodos intermedios y terminales17, por ejemplo: tree &lt;- rpart(taste ~ ., data = train, cp = 0, minsplit = 30, minbucket = 10) En este caso mantenemos el resto de valores por defecto: tree &lt;- rpart(taste ~ ., data = train, cp = 0) Representamos los errores (reescalados) de validación cruzada: # printcp(tree) plotcp(tree) Para obtener el modelo final, seleccionamos el valor óptimo de complejidad siguiendo el criterio de un error estándar de Breiman et al. (1984) y podamos el arbol: xerror &lt;- tree$cptable[,&quot;xerror&quot;] imin.xerror &lt;- which.min(xerror) upper.xerror &lt;- xerror[imin.xerror] + tree$cptable[imin.xerror, &quot;xstd&quot;] icp &lt;- min(which(xerror &lt;= upper.xerror)) cp &lt;- tree$cptable[icp, &quot;CP&quot;] tree &lt;- prune(tree, cp = cp) # tree # summary(tree) # caret::varImp(tree) # importance &lt;- tree$variable.importance # importance &lt;- round(100*importance/sum(importance), 1) # importance[importance &gt;= 1] rpart.plot(tree, main=&quot;Classification tree winetaste&quot;) El último paso sería evaluarlo en la muestra de test siguiendo los pasos descritos en la Sección F.3.5. El método predict() por defecto (type = \"prob\") devuelve una matriz con las probabilidades de cada clase, habrá que establecer type = \"class\" (para más detalles consultar la ayuda de predic.rpart()). obs &lt;- test$taste head(predict(tree, newdata = test)) ## good bad ## 1 0.3025641 0.6974359 ## 4 0.8151571 0.1848429 ## 9 0.8151571 0.1848429 ## 10 0.8151571 0.1848429 ## 12 0.8151571 0.1848429 ## 16 0.8151571 0.1848429 pred &lt;- predict(tree, newdata = test, type = &quot;class&quot;) table(obs, pred) ## pred ## obs good bad ## good 153 13 ## bad 54 30 caret::confusionMatrix(pred, obs) ## Confusion Matrix and Statistics ## ## Reference ## Prediction good bad ## good 153 54 ## bad 13 30 ## ## Accuracy : 0.732 ## 95% CI : (0.6725, 0.7859) ## No Information Rate : 0.664 ## P-Value [Acc &gt; NIR] : 0.01247 ## ## Kappa : 0.3171 ## ## Mcnemar&#39;s Test P-Value : 1.025e-06 ## ## Sensitivity : 0.9217 ## Specificity : 0.3571 ## Pos Pred Value : 0.7391 ## Neg Pred Value : 0.6977 ## Prevalence : 0.6640 ## Detection Rate : 0.6120 ## Detection Prevalence : 0.8280 ## Balanced Accuracy : 0.6394 ## ## &#39;Positive&#39; Class : good ## G.3.3 Interfaz de caret En caret podemos ajustar un árbol CART seleccionando method = \"rpart\". Por defecto emplea bootstrap de las observaciones para seleccionar el valor óptimo del hiperparámetro cp (considerando únicamente tres posibles valores). Si queremos emplear validación cruzada como en el caso anterior podemos emplear la función auxiliar trainControl() y para considerar un mayor rango de posibles valores, el argumento tuneLength. library(caret) # names(getModelInfo()) # Listado de todos los métodos disponibles # modelLookup(&quot;rpart&quot;) # Información sobre hiperparámetros set.seed(1) # itrain &lt;- &lt;- createDataPartition(winetaste$taste, p = 0.8, list = FALSE) # train &lt;- winetaste[itrain, ] # test &lt;- winetaste[-itrain, ] caret.rpart &lt;- train(taste ~ ., method = &quot;rpart&quot;, data = train, tuneLength = 20, trControl = trainControl(method = &quot;cv&quot;, number = 10)) caret.rpart ## CART ## ## 1000 samples ## 11 predictor ## 2 classes: &#39;good&#39;, &#39;bad&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 901, 900, 900, 900, 900, 900, ... ## Resampling results across tuning parameters: ## ## cp Accuracy Kappa ## 0.000000000 0.7018843 0.3487338 ## 0.005995017 0.7330356 0.3870552 ## 0.011990034 0.7410655 0.3878517 ## 0.017985051 0.7230748 0.3374518 ## 0.023980069 0.7360748 0.3698691 ## 0.029975086 0.7340748 0.3506377 ## 0.035970103 0.7320748 0.3418235 ## 0.041965120 0.7350849 0.3422651 ## 0.047960137 0.7350849 0.3422651 ## 0.053955154 0.7350849 0.3422651 ## 0.059950171 0.7350849 0.3422651 ## 0.065945188 0.7350849 0.3422651 ## 0.071940206 0.7350849 0.3422651 ## 0.077935223 0.7350849 0.3422651 ## 0.083930240 0.7350849 0.3422651 ## 0.089925257 0.7350849 0.3422651 ## 0.095920274 0.7350849 0.3422651 ## 0.101915291 0.7350849 0.3422651 ## 0.107910308 0.7229637 0.2943312 ## 0.113905325 0.6809637 0.1087694 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was cp = 0.01199003. ggplot(caret.rpart) caret.rpart$finalModel ## n= 1000 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 1000 338 good (0.6620000 0.3380000) ## 2) alcohol&gt;=10.11667 541 100 good (0.8151571 0.1848429) ## 4) free.sulfur.dioxide&gt;=8.5 522 87 good (0.8333333 0.1666667) ## 8) fixed.acidity&lt; 8.55 500 73 good (0.8540000 0.1460000) * ## 9) fixed.acidity&gt;=8.55 22 8 bad (0.3636364 0.6363636) * ## 5) free.sulfur.dioxide&lt; 8.5 19 6 bad (0.3157895 0.6842105) * ## 3) alcohol&lt; 10.11667 459 221 bad (0.4814815 0.5185185) ## 6) volatile.acidity&lt; 0.2875 264 102 good (0.6136364 0.3863636) ## 12) fixed.acidity&lt; 7.45 213 71 good (0.6666667 0.3333333) ## 24) citric.acid&gt;=0.265 160 42 good (0.7375000 0.2625000) * ## 25) citric.acid&lt; 0.265 53 24 bad (0.4528302 0.5471698) ## 50) free.sulfur.dioxide&lt; 42.5 33 13 good (0.6060606 0.3939394) * ## 51) free.sulfur.dioxide&gt;=42.5 20 4 bad (0.2000000 0.8000000) * ## 13) fixed.acidity&gt;=7.45 51 20 bad (0.3921569 0.6078431) ## 26) total.sulfur.dioxide&gt;=150 26 10 good (0.6153846 0.3846154) * ## 27) total.sulfur.dioxide&lt; 150 25 4 bad (0.1600000 0.8400000) * ## 7) volatile.acidity&gt;=0.2875 195 59 bad (0.3025641 0.6974359) ## 14) pH&gt;=3.235 49 24 bad (0.4897959 0.5102041) ## 28) chlorides&lt; 0.0465 18 4 good (0.7777778 0.2222222) * ## 29) chlorides&gt;=0.0465 31 10 bad (0.3225806 0.6774194) * ## 15) pH&lt; 3.235 146 35 bad (0.2397260 0.7602740) * rpart.plot(caret.rpart$finalModel, main=&quot;Classification tree winetaste&quot;) Para utilizar la regla de un error estándar se puede añadir selectionFunction = \"oneSE\" set.seed(1) caret.rpart &lt;- train(taste ~ ., method = &quot;rpart&quot;, data = train, tuneLength = 20, trControl = trainControl(method = &quot;cv&quot;, number = 10, selectionFunction = &quot;oneSE&quot;)) caret.rpart ## CART ## ## 1000 samples ## 11 predictor ## 2 classes: &#39;good&#39;, &#39;bad&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 901, 900, 900, 900, 900, 900, ... ## Resampling results across tuning parameters: ## ## cp Accuracy Kappa ## 0.000000000 0.7018843 0.3487338 ## 0.005995017 0.7330356 0.3870552 ## 0.011990034 0.7410655 0.3878517 ## 0.017985051 0.7230748 0.3374518 ## 0.023980069 0.7360748 0.3698691 ## 0.029975086 0.7340748 0.3506377 ## 0.035970103 0.7320748 0.3418235 ## 0.041965120 0.7350849 0.3422651 ## 0.047960137 0.7350849 0.3422651 ## 0.053955154 0.7350849 0.3422651 ## 0.059950171 0.7350849 0.3422651 ## 0.065945188 0.7350849 0.3422651 ## 0.071940206 0.7350849 0.3422651 ## 0.077935223 0.7350849 0.3422651 ## 0.083930240 0.7350849 0.3422651 ## 0.089925257 0.7350849 0.3422651 ## 0.095920274 0.7350849 0.3422651 ## 0.101915291 0.7350849 0.3422651 ## 0.107910308 0.7229637 0.2943312 ## 0.113905325 0.6809637 0.1087694 ## ## Accuracy was used to select the optimal model using the one SE rule. ## The final value used for the model was cp = 0.1019153. # ggplot(caret.rpart) caret.rpart$finalModel ## n= 1000 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 1000 338 good (0.6620000 0.3380000) ## 2) alcohol&gt;=10.11667 541 100 good (0.8151571 0.1848429) * ## 3) alcohol&lt; 10.11667 459 221 bad (0.4814815 0.5185185) ## 6) volatile.acidity&lt; 0.2875 264 102 good (0.6136364 0.3863636) * ## 7) volatile.acidity&gt;=0.2875 195 59 bad (0.3025641 0.6974359) * rpart.plot(caret.rpart$finalModel, main = &quot;Classification tree winetaste&quot;) var.imp &lt;- varImp(caret.rpart) plot(var.imp) Para calcular las predicciones (o las estimaciones de las probabilidades) podemos emplear el método predict.train() y posteriormente confusionMatrix() para evaluar su precisión: pred &lt;- predict(caret.rpart, newdata = test) # p.est &lt;- predict(caret.rpart, newdata = test, type = &quot;prob&quot;) confusionMatrix(pred, test$taste) ## Confusion Matrix and Statistics ## ## Reference ## Prediction good bad ## good 153 54 ## bad 13 30 ## ## Accuracy : 0.732 ## 95% CI : (0.6725, 0.7859) ## No Information Rate : 0.664 ## P-Value [Acc &gt; NIR] : 0.01247 ## ## Kappa : 0.3171 ## ## Mcnemar&#39;s Test P-Value : 1.025e-06 ## ## Sensitivity : 0.9217 ## Specificity : 0.3571 ## Pos Pred Value : 0.7391 ## Neg Pred Value : 0.6977 ## Prevalence : 0.6640 ## Detection Rate : 0.6120 ## Detection Prevalence : 0.8280 ## Balanced Accuracy : 0.6394 ## ## &#39;Positive&#39; Class : good ## NOTA: En principio también se podría utilizar la regla de un error estándar seleccionando method = \"rpart1SE\" (pero caret implementa internamente este método y en ocasiones no se obtienen los resultados esperados). set.seed(1) caret.rpart &lt;- train(taste ~ ., method = &quot;rpart1SE&quot;, data = train) caret.rpart printcp(caret.rpart$finalModel) caret.rpart$finalModel rpart.plot(caret.rpart$finalModel, main = &quot;Classification tree winetaste&quot;) varImp(caret.rpart) El paquete tree es una traducción del original en S. Los parámetros maxsurrogate, usesurrogate y surrogatestyle serían de utilidad si hay datos faltantes. Realmente en la tabla de texto se muestra el valor mínimo de CP, ya que se obtendría la misma solución para un rango de valores de CP (desde ese valor hasta el anterior, sin incluirlo), mientras que en el gráfico generado por plotcp() se representa la media geométrica de los extremos de ese intervalo. Otra opción, más interesante para regresión, sería considerar estos valores como hiperparámetros. "],["G-4-alternativas-a-los-árboles-cart.html", "G.4 Alternativas a los árboles CART", " G.4 Alternativas a los árboles CART Una de las alternativas más populares es la metodología C4.5 (Quinlan, 1993), evolución de ID3 (1986), que en estos momentos se encuentra en la versión C5.0 (y es ya muy similar a CART). C5.0 se utiliza sólo para clasificación e incorpora boosting (que veremos en el tema siguiente). Esta metodología está implementada en el paquete C50. Ross Quinlan desarrolló también la metodologia M5 (Quinlan, 1992) para regresión. Su principal característica es que los nodos terminales, en lugar de contener un número, contienen un modelo (de regresión) lineal. El paquete Cubist es una evolución de M5 que incorpora un método ensemble similar a boosting. La motivación detrás de M5 es que, si la predicción que aporta un nodo terminal se limita a un único número (como hace la metodología CART), entonces el modelo va a predecir muy mal los valores que realmente son muy extremos, ya que el número de posibles valores predichos está limitado por el número de nodos terminales, y en cada uno de ellos se utiliza una media. Por ello M5 le asocia a cada nodo un modelo de regresión lineal, para cuyo ajuste se utilizan los datos del nodo y todas las variables que están en la ruta del nodo. Para evaluar los posibles cortes que conducen al siguiente nodo, se utilizan los propios modelos lineales para calcular la medida del error. Una vez se ha construido todo el árbol, para realizar la predicción se puede utilizar el modelo lineal que está en el nodo terminal correspondiente, pero funciona mejor si se utiliza una combinación lineal del modelo del nodo terminal y de todos sus nodos ascendientes (es decir, los que están en su camino). Otra opción es CHAID (CHi-squared Automated Interaction Detection, Kass, 1980), que se basa en una idea diferente. Es un método de construcción de árboles de clasificación que se utiliza cuando las variables predictoras son cualitativas o discretas; en caso contrario deben ser categorizadas previamente. Y se basa en el contraste chi-cuadrado de independencia para tablas de contingencia. Para cada par \\((X_i, Y)\\), se considera su tabla de contingencia y se calcula el p-valor del contraste chi-cuadrado, seleccionándose la variable predictora que tenga un p-valor más pequeño, ya que se asume que las variables predictoras más relacionadas con la respuesta \\(Y\\) son las que van a tener p-valores más pequeños y darán lugar a mejores predicciones. Se divide el nodo de acuerdo con los distintos valores de la variable predictora seleccionada, y se repite el proceso mientras haya variables significativas. Como el método exige que el p-valor sea menor que 0.05 (o el nivel de significación que se elija), y hay que hacer muchas comparaciones es necesario aplicar una corrección para comparaciones múltiples, por ejemplo la de Bonferroni. Lo que acabamos de explicar daría lugar a árboles no necesariamente binarios. Como se desea trabajar con árboles binarios (si se admite que de un nodo salga cualquier número de ramas, con muy pocos niveles de profundidad del árbol ya nos quedaríamos sin datos), es necesario hacer algo más: forzar a que las variables predictoras tengan sólo dos categorías mediante un proceso de fusión. Se van haciendo pruebas chi-cuadrado entre pares de categorías y la variable respuesta, y se fusiona el par con el p-valor más alto, ya que se trata de fusionar las categorías que sean más similares. Para árboles de regresión hay metodologías que, al igual que CHAID, se basan en el cálculo de p-valores, en este caso de contrastes de igualdes de medias. Una de las más utilizadas son los conditional inference trees (Hothorn et al., 2006)18, implementada en la función ctree() del paquete party. Un problema conocido de los árboles CART es que sufren un sesgo de selección de variables: los predictores con más valores distintos son favorecidos. Esta es una de las motivaciones de utilizar estos métodos basados en contrastes de hipótesis. Por otra parte hay que ser conscientes de que los contrastes de hipótesis y la calidad predictiva son cosas distintas. G.4.1 Ejemplo library(party) tree2 &lt;- ctree(taste ~ ., data = train) plot(tree2) Otra alternativa es GUIDE (Generalized, Unbiased, Interaction Detection and Estimation; Loh, 2002). "],["H-bagging-boosting.html", "H Bagging y Boosting", " H Bagging y Boosting Tanto el bagging como el boosting son procedimientos generales para la reducción de la varianza de un método estadístico de aprendizaje. La idea básica consiste en combinar métodos de predicción sencillos (débiles), es decir, con poca capacidad predictiva, para obtener un método de predicción muy potente (y robusto). Estas ideas se pueden aplicar tanto a problemas de regresión como de clasificación. Son muy empleados con árboles de decisión: son predictores débiles y se generan de forma rápida. Lo que se hace es construir muchos modelos (crecer muchos árboles) que luego se combinan para producir predicciones (promediando o por consenso). "],["H-1-bagging.html", "H.1 Bagging", " H.1 Bagging En la década de 1990 empiezan a utilizarse los métodos ensemble (métodos combinados), esto es, métodos predictivos que se basan en combinar las predicciones de cientos de modelos. Uno de los primeros métodos combinados que se utilizó fue el bagging (nombre que viene de bootstrap aggregation), propuesto en Breiman (1996). Es un método general de reducción de la varianza que se basa en la utilización del bootstrap junto con un modelo de regresión o de clasificación, como puede ser un árbol de decisión. La idea es muy sencilla. Si disponemos de muchas muestras de entrenamiento, podemos utilizar cada una de ellas para entrenar un modelo que después nos servirá para hacer una predicción. De este modo tendremos tantas predicciones como modelos y por tanto tantas predicciones como muestras de entrenamiento. El procedimiento consistente en promediar todas las predicciones anteriores tiene dos ventajas importantes: simplifica la solución y reduce mucho la varianza. El problema es que en la práctica no suele disponerse más que de una única muestra de entrenamiento. Aquí es donde entra en juego el bootstrap, técnica especialmente útil para estimar varianzas, pero que en esta aplicación se utiliza para reducir la varianza. Lo que se hace es generar cientos o miles de muestras bootstrap a partir de la muestra de entrenamiento, y después utilizar cada una de estas muestras bootstrap como una muestra de entrenamiento (bootstrapped training data set). Para un modelo que tenga intrínsecamente poca variabilidad, como puede ser una regresión lineal, aplicar bagging puede ser poco interesante, ya que hay poco margen para mejorar el rendimiento. Por contra, es un método muy importante para los árboles de decisión, porque un árbol con mucha profundidad (sin podar) tiene mucha variabilidad: si modificamos ligeramente los datos de entrenamiento es muy posible que se obtenga un nuevo árbol completamente distinto al anterior; y esto se ve como un inconveniente. Por esa razón, en este contexto encaja perfectamente la metodología bagging. Así, para árboles de regresión se hacen crecer muchos árboles (sin poda) y se calcula la media de las predicciones. En el caso de los árboles de clasificación lo más sencillo es sustituir la media por la moda y utilizar el criterio del voto mayoritario: cada modelo tiene el mismo peso y por tanto cada modelo aporta un voto. Además, la proporción de votos de cada categoría es una estimación de su probabilidad. Una ventaja adicional del bagging es que permite estimar el error de la predicción de forma directa, sin necesidad de utilizar una muestra de test o de aplicar validación cruzada u, otra vez, remuestreo, y se obtiene un resultado similar al que obtendríamos con estos métodos. Es bien sabido que una muestra bootstrap va a contener muchas observaciones repetidas y que, en promedio, sólo utiliza aproximadamente dos tercios de los datos (para ser más precisos, \\(1 - (1 - 1/n)^n \\approx 1 - e^{-1} = 0.6321\\) al aumentar el tamaño del conjunto de datos de entrenamiento). Un dato que no es utilizado para construir un árbol se denomina un dato out-of-bag (OOB). De este modo, para cada observación se pueden utilizar los árboles para los que esa observación es out-of-bag (aproximadamente una tercera parte de los árboles construidos) para generar una única predicción para ella. Repitiendo el proceso para todas las observaciones se obtiene una medida del error. Una decisión que hay que tomar es cuántas muestras bootstrap se toman (o lo que es lo mismo, cuántos árboles se construyen). Realmente se trata de una aproximación Monte Carlo, por lo que típicamente se estudia gráficamente la convergencia del error OOB al aumentar el número de árboles (para más detalles ver p.e. Fernández-Casal y Cao, 2020, Sección 4.1). Si aparentemente hay convergencia con unos pocos cientos de árboles, no va a variar mucho el nivel de error al aumentar el número. Por tanto aumentar mucho el número de árboles no mejora las predicciones, aunque tampoco aumenta el riesgo de sobreajuste. Los costes computacionales aumentan con el número de árboles, pero la construcción y evaluación del modelo son fácilmente paralelizables (aunque pueden llegar a requerir mucha memoria si el conjunto de datos es muy grande). Por otra parte si el número de árboles es demasiado pequeño puede que se obtengan pocas (o incluso ninguna) predicciones OOB para alguna de las observaciones de la muestra de entrenamiento. Una ventaja que ya sabemos que tienen los árboles de decisión es su fácil interpretabilidad. En un árbol resulta evidente cuales son los predictores más influyentes. Al utilizar bagging se mejora (mucho) la predicción, pero se pierde la interpretabilidad. Aún así, hay formas de calcular la importancia de los predictores. Por ejemplo, si fijamos un predictor y una medida del error podemos, para cada uno de los árboles, medir la reducción del error que se consigue cada vez que hay un corte que utilice ese predictor particular. Promediando sobre todos los árboles bagging se obtiene una medida global de la importancia: un valor alto en la reducción del error sugiere que el predictor es importante. En resumen: Se remuestrea repetidamente el conjunto de datos de entrenamiento. Con cada conjunto de datos se entrena un modelo. Las predicciones se obtienen promediando las predicciones de los modelos (la decisión mayoritaria en el caso de clasificación). Se puede estimar la precisión de las predicciones con el error OOB (out-of-bag). "],["H-2-bosques-aleatorios.html", "H.2 Bosques aleatorios", " H.2 Bosques aleatorios Los bosques aleatorios (random forest) son una variante de bagging específicamente diseñados para trabajar con árboles de decisión. Las muestras bootstrap que se generan al hacer bagging introducen un elemento de aleatoriedad que en la práctica provoca que todos los árboles sean distintos, pero en ocasiones no son lo suficientemente distintos. Es decir, suele ocurrir que los árboles tengan estructuras muy similares, especialmente en la parte alta, aunque después se vayan diferenciando según se desciende por ellos. Esta característica se conoce como correlación entre árboles y se da cuando el árbol es un modelo adecuado para describir la relación ente los predictores y la respuesta, y también cuándo uno de los predictores es muy fuerte, es decir, es especialmente relevante, con lo cual casi siempre va a estar en el primer corte. Esta correlación entre árboles se va a traducir en una correlación entre sus predicciones (más formalmente, entre los predictores). Promediar variables altamente correladas produce una reducción de la varianza mucho menor que si promediamos variables incorreladas. La solución pasa por añadir aleatoriedad al proceso de construcción de los árboles, para que estos dejen de estar correlados. Hubo varios intentos, entre los que destaca Dietterich (2000) al proponer la idea de introducir aleatorieadad en la selección de las variables de cada corte. Breiman (2001) propuso un algoritmo unificado al que llamó bosques aleatorios. En la construcción de cada uno de los árboles que finalmente constituirán el bosque, se van haciendo cortes binarios, y para cada corte hay que seleccionar una variable predictora. La modificación introducida fue que antes de hacer cada uno de los cortes, de todas las \\(p\\) variables predictoras, se seleccionan al azar \\(m &lt; p\\) predictores que van a ser los candidatos para el corte. El hiperparámetro de los bosques aleatorios es \\(m\\), y se puede seleccionar mediante las técnicas habituales. Como puntos de partida razonables se pueden considerar \\(m = \\sqrt{p}\\) (para problemas de clasificación) y \\(m = p/3\\) (para problemas de regresión). El número de árboles que van a constituir el bosque también puede tratarse como un hiperparámetro, aunque es más frecuente tratarlo como un problema de convergencia. En general, van a hacer falta más árboles que en bagging. Los bosques aleatorios son computacionalmente más eficientes que bagging porque, aunque como acabamos de decir requieren más árboles, la construcción de cada árbol es mucho más rápida al evaluarse sólo unos pocos predictores en cada corte. Este método también puede ser empleado para aprendizaje no supervisado, por ejemplo se puede construir una matriz de proximidad entre observaciones a partir de la proporción de veces que están en un mismo nodo terminal (para más detalles ver Liaw y Wiener, 2002). En resumen: Los bosques aleatorios son una modificación del bagging para el caso de árboles de decisión. También se introduce aleatoriedad en las variables, no sólo en las observaciones. Para evitar dependencias, los posibles predictores se seleccionan al azar en cada nodo (e.g. \\(m=\\sqrt{p}\\)). Se utilizan árboles sin podar. Estos métodos dificultan la interpretación. Se puede medir la importancia de las variables (índices de importancia). Por ejemplo, para cada árbol se suman las reducciones en el índice de Gini correspondientes a las divisiones de un predictor y posteriormente se promedian los valores de todos los árboles. Alternativamente (Breiman, 2001) se puede medir el incremento en el error de predicción OOB al permutar aleatoriamente los valores de la variable explicativa en las muestras OOB (manteniendo el resto sin cambios). "],["H-3-bagging-rf-r.html", "H.3 Bagging y bosques aleatorios en R", " H.3 Bagging y bosques aleatorios en R Estos algoritmos son de los más populares en AE y están implementados en numerosos paquetes de R, aunque la referencia es el paquete randomForest (que emplea el código Fortran desarrollado por Leo Breiman y Adele Cutler). La función principal es randomForest() y se suele emplear de la forma: randomForest(formula, data, ntree, mtry, nodesize, ...) formula y data (opcional): permiten especificar la respuesta y las variables predictoras de la forma habitual (típicamente respuesta ~ .), aunque si el conjunto de datos es muy grande puede ser preferible emplear una matriz o un data.frame para establecer los predictores y un vector para la respuesta (sustituyendo estos argumentos por x e y). Si la respuesta es un factor asumirá que se trata de un problema de clasificación y de regresión en caso contrario. ntree: número de árboles que se crecerán; por defecto 500. mtry: número de predictores seleccionados al azar en cada división; por defecto max(floor(p/3), 1) en el caso de regresión y floor(sqrt(p)) en clasificación, siendo p = ncol(x) = ncol(data) - 1 el número de predictores. nodesize: número mínimo de observaciones en un nodo terminal; por defecto 1 en clasificación y 5 en regresión (puede ser recomendable incrementarlo si el conjunto de datos es muy grande, para evitar posibles problemas de sobreajuste, disminuir el tiempo de computación y los requerimientos de memoria; también podría ser considerado como un hiperparámetro). Otros argumentos que pueden ser de interés19 son: maxnodes: número máximo de nodos terminales (como alternativa para la establecer la complejidad). importance = TRUE: permite obtener medidas adicionales de importancia. proximity = TRUE: permite obtener una matriz de proximidades (componente $proximity) entre las observaciones (frecuencia con la que los pares de observaciones están en el mismo nodo terminal). na.action = na.fail: por defecto no admite datos faltantes con la interfaz de fórmulas. Si los hubiese, se podrían imputar estableciendo na.action = na.roughfix (empleando medias o modas) o llamando previamente a rfImpute() (que emplea proximidades obtenidas con un bosque aleatorio). Más detalles en la ayuda de esta función o en Liaw y Wiener (2002). Entre las numerosas alternativas, además de las implementadas en paquetes que integran colecciones de métodos como h2o o RWeka, una de las más utilizadas son los bosques aleatorios con conditional inference trees, implementada en la función cforest() del paquete party. H.3.1 Ejemplo: Clasificación con bagging Como ejemplo consideraremos el conjunto de datos de calidad de vino empleado en la Sección G.3.2 (para hacer comparaciones con el ajuste de un único árbol). load(&quot;datos/winetaste.RData&quot;) set.seed(1) df &lt;- winetaste nobs &lt;- nrow(df) itrain &lt;- sample(nobs, 0.8 * nobs) train &lt;- df[itrain, ] test &lt;- df[-itrain, ] Al ser bagging con árboles un caso particular de bosques aleatorios, cuando \\(m = p\\), también podemos emplear randomForest: library(randomForest) set.seed(4) # NOTA: Fijamos esta semilla para ilustrar dependencia bagtrees &lt;- randomForest(taste ~ ., data = train, mtry = ncol(train) - 1) bagtrees ## ## Call: ## randomForest(formula = taste ~ ., data = train, mtry = ncol(train) - 1) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 11 ## ## OOB estimate of error rate: 23.5% ## Confusion matrix: ## good bad class.error ## good 565 97 0.1465257 ## bad 138 200 0.4082840 Con el método plot() podemos examinar la convergencia del error en las muestras OOB (simplemente emplea matplot() para representar la componente $err.rate): plot(bagtrees, main = &quot;Tasas de error&quot;) legend(&quot;topright&quot;, colnames(bagtrees$err.rate), lty = 1:5, col = 1:6) Como vemos que los errores se estabilizan podríamos pensar que aparentemente hay convergencia (aunque situaciones de alta dependencia entre los árboles dificultarían su interpretación). Con la función getTree() podemos extraer los árboles individuales. Por ejemplo el siguiente código permite extraer la variable seleccionada para la primera división: # View(getTree(bagtrees, 1, labelVar=TRUE)) split_var_1 &lt;- sapply(seq_len(bagtrees$ntree), function(i) getTree(bagtrees, i, labelVar=TRUE)[1, &quot;split var&quot;]) En este caso concreto podemos observar que siempre es la misma, lo que indicaría una alta dependencia entre los distintos árboles: table(split_var_1) ## split_var_1 ## alcohol chlorides citric.acid ## 500 0 0 ## density fixed.acidity free.sulfur.dioxide ## 0 0 0 ## pH residual.sugar sulphates ## 0 0 0 ## total.sulfur.dioxide volatile.acidity ## 0 0 Por último evaluamos la precisión en la muestra de test: pred &lt;- predict(bagtrees, newdata = test) caret::confusionMatrix(pred, test$taste) ## Confusion Matrix and Statistics ## ## Reference ## Prediction good bad ## good 145 42 ## bad 21 42 ## ## Accuracy : 0.748 ## 95% CI : (0.6894, 0.8006) ## No Information Rate : 0.664 ## P-Value [Acc &gt; NIR] : 0.002535 ## ## Kappa : 0.3981 ## ## Mcnemar&#39;s Test P-Value : 0.011743 ## ## Sensitivity : 0.8735 ## Specificity : 0.5000 ## Pos Pred Value : 0.7754 ## Neg Pred Value : 0.6667 ## Prevalence : 0.6640 ## Detection Rate : 0.5800 ## Detection Prevalence : 0.7480 ## Balanced Accuracy : 0.6867 ## ## &#39;Positive&#39; Class : good ## H.3.2 Ejemplo: Clasificación con bosques aleatorios Continuando con el ejemplo anterior, empleamos la función randomForest() con las opciones por defecto para ajustar un bosque aleatorio: # load(&quot;datos/winetaste.RData&quot;) # set.seed(1) # df &lt;- winetaste # nobs &lt;- nrow(df) # itrain &lt;- sample(nobs, 0.8 * nobs) # train &lt;- df[itrain, ] # test &lt;- df[-itrain, ] set.seed(1) rf &lt;- randomForest(taste ~ ., data = train) rf ## ## Call: ## randomForest(formula = taste ~ ., data = train) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 3 ## ## OOB estimate of error rate: 22% ## Confusion matrix: ## good bad class.error ## good 578 84 0.1268882 ## bad 136 202 0.4023669 En este caso también observamos que aparentemente hay convergencia y tampoco sería necesario incrementar el número de árboles: plot(rf, main = &quot;Tasas de error&quot;) legend(&quot;topright&quot;, colnames(rf$err.rate), lty = 1:5, col = 1:6) Podemos mostrar la importancia de las variables predictoras con la función importance() o representarlas con varImpPlot(): importance(rf) ## MeanDecreaseGini ## fixed.acidity 37.77155 ## volatile.acidity 43.99769 ## citric.acid 41.50069 ## residual.sugar 36.79932 ## chlorides 33.62100 ## free.sulfur.dioxide 42.29122 ## total.sulfur.dioxide 39.63738 ## density 45.38724 ## pH 32.31442 ## sulphates 30.32322 ## alcohol 63.89185 varImpPlot(rf) Si evaluamos la precisión en la muestra de test podemos observar un ligero incremento en la precisión en comparación con el método anterior: pred &lt;- predict(rf, newdata = test) caret::confusionMatrix(pred, test$taste) ## Confusion Matrix and Statistics ## ## Reference ## Prediction good bad ## good 153 43 ## bad 13 41 ## ## Accuracy : 0.776 ## 95% CI : (0.7192, 0.8261) ## No Information Rate : 0.664 ## P-Value [Acc &gt; NIR] : 7.227e-05 ## ## Kappa : 0.4494 ## ## Mcnemar&#39;s Test P-Value : 0.0001065 ## ## Sensitivity : 0.9217 ## Specificity : 0.4881 ## Pos Pred Value : 0.7806 ## Neg Pred Value : 0.7593 ## Prevalence : 0.6640 ## Detection Rate : 0.6120 ## Detection Prevalence : 0.7840 ## Balanced Accuracy : 0.7049 ## ## &#39;Positive&#39; Class : good ## Esta mejora sería debida a que en este caso la dependencia entre los árboles es menor: split_var_1 &lt;- sapply(seq_len(rf$ntree), function(i) getTree(rf, i, labelVar=TRUE)[1, &quot;split var&quot;]) table(split_var_1) ## split_var_1 ## alcohol chlorides citric.acid ## 150 49 38 ## density fixed.acidity free.sulfur.dioxide ## 114 23 20 ## pH residual.sugar sulphates ## 11 0 5 ## total.sulfur.dioxide volatile.acidity ## 49 41 El análisis e interpretación del modelo puede resultar más complicado en este tipo de métodos. Por ejemplo, podemos emplear alguna de las herramientas mostradas en la Sección F.5: # install.packages(&quot;pdp&quot;) library(&quot;pdp&quot;) pdp1 &lt;- partial(rf, &quot;alcohol&quot;) plotPartial(pdp1) pdp2 &lt;- partial(rf, c(&quot;alcohol&quot;, &quot;density&quot;)) plotPartial(pdp2) En este caso también puede ser de utilidad el paquete randomForestExplainer. H.3.3 Ejemplo: bosques aleatorios con caret En paquete caret hay varias implementaciones de bagging y bosques aleatorios20, incluyendo el algoritmo del paquete randomForest considerando como hiperparámetro el número de predictores seleccionados al azar en cada división mtry. Para ajustar este modelo a una muestra de entrenamiento hay que establecer method = \"rf\" en la llamada a train(). library(caret) # str(getModelInfo(&quot;rf&quot;, regex = FALSE)) modelLookup(&quot;rf&quot;) ## model parameter label forReg forClass probModel ## 1 rf mtry #Randomly Selected Predictors TRUE TRUE TRUE # load(&quot;datos/winetaste.RData&quot;) # set.seed(1) # df &lt;- winetaste # nobs &lt;- nrow(df) # itrain &lt;- sample(nobs, 0.8 * nobs) # train &lt;- df[itrain, ] # test &lt;- df[-itrain, ] Con las opciones por defecto únicamente evalúa tres valores posibles del hiperparámetro (se podría aumentar el número con tuneLength o especificarlos con tuneGrid), pero aún así el tiempo de computación puede ser alto (puede ser recomendable reducir el valor de nodesize o paralelizar los cálculos; otras implementaciones pueden ser más eficientes). set.seed(1) rf.caret &lt;- train(taste ~ ., data = train, method = &quot;rf&quot;) plot(rf.caret) Breiman (2001) sugiere emplear el valor por defecto, la mitad y el doble: mtry.class &lt;- sqrt(ncol(train) - 1) tuneGrid &lt;- data.frame(mtry = floor(c(mtry.class/2, mtry.class, 2*mtry.class))) set.seed(1) rf.caret &lt;- train(taste ~ ., data = train, method = &quot;rf&quot;, tuneGrid = tuneGrid) plot(rf.caret) Si se quiere minimizar el uso de memoria, por ejemplo mientras se seleccionan hiperparámetros, se puede establecer keep.forest=FALSE. Se puede hacer una búsqueda en la tabla del Capítulo 6: Available Models del manual. "],["H-4-boosting.html", "H.4 Boosting", " H.4 Boosting La metodología boosting es una metodología general de aprendizaje lento en la que se combinan muchos modelos obtenidos mediante un método con poca capacidad predictiva para, impulsados, dar lugar a un mejor predictor. Los árboles de decisión pequeños (construidos con poca profundidad) resultan perfectos para esta tarea, al ser realmente malos predictores (weak learners), fáciles de combinar y generarse de forma muy rápida. El boosting nació en el contexto de los problemas de clasificación y tardó varios años en poderse extender a los problemas de regresión. Por ese motivo vamos a empezar viendo el boosting en clasificación. La idea del boosting la desarrollaron Valiant (1984) y Kearns y Valiant (1989), pero encontrar una implementación efectiva fue una tarea difícil que no se resolvió satisfactoriamente hasta que Freund y Schapire (1996) presentaron el algoritmo AdaBoost, que rápidamente se convirtió en un éxito. Veamos, de forma muy esquemática, en que consiste el algoritmo AdaBoost para un problema de clasificación en el que sólo hay dos categorías y en el que se utiliza como clasificador débil un árbol de decisión con pocos nodos terminales, sólo marginalmente superior a un clasificador aleatorio. En este caso resulta más cómodo recodificar la variable indicadora \\(Y\\) como 1 si éxito y -1 si fracaso. Seleccionar \\(B\\), número de iteraciones. Se les asigna el mismo peso a todas las observaciones de la muestra de entrenamiento (\\(1/n\\)). Para \\(b = 1, 2,\\ldots, B\\), repetir: Ajustar el árbol utilizando las observaciones ponderadas. Calcular la proporción de errores en la clasificación \\(e_b\\). Calcular \\(s_b = \\text{log}((1 - e_b)/e_b)\\). Actualizar los pesos de las observaciones. Los pesos de las observaciones correctamente clasificadas no cambian; se les da más peso a las observaciones incorrectamente clasificadas, multiplicando su peso anterior por \\((1 - e_b)/e_b\\). Dada una observación \\(\\mathbf{x}\\), si denotamos por \\(\\hat y_b ( \\mathbf{x} )\\) su clasificación utilizando árbol \\(b\\)-ésimo, entonces \\(\\hat y( \\mathbf{x} ) = signo \\left( \\sum_b s_b \\hat y_b ( \\mathbf{x} ) \\right)\\) (si la suma es positiva, se clasifica la observación como perteneciente a la clase +1, en caso contrario a la clase -1). Vemos que el algoritmo AdaBoost no combina árboles independientes (como sería el caso de los bosques aleatorios, por ejemplo), sino que estos se van generando en una secuencia en la que cada árbol depende del anterior. Se utiliza siempre el mismo conjunto de datos (de entrenamiento), pero a estos datos se les van poniendo unos pesos en cada iteración que dependen de lo que ha ocurrido en la iteración anterior: se les da más peso a las observaciones mal clasificadas para que en sucesivas iteraciones se clasifiquen bien. Finalmente, la combinación de los árboles se hace mediante una suma ponderada de las \\(B\\) clasificaciones realizadas. Los pesos de esta suma son los valores \\(s_b\\). Un árbol que clasifique de forma aleatoria \\(e_b = 0.5\\) va a tener un peso \\(s_b = 0\\) y cuando mejor clasifique el árbol mayor será su peso. Al estar utilizando clasificadores débiles (árboles pequeños) es de esperar que los pesos sean en general próximos a cero. El siguiente hito fue la aparición del método gradient boosting machine (Friedman, 2001), perteneciente a la familia de los métodos iterativos de descenso de gradientes. Entre otras muchas ventajas, este método permitió resolver no sólo problemas de clasificación sino también de regresión; y permitió la conexión con lo que se estaba haciendo en otros campos próximos como pueden ser los modelos aditivos o la regresión logística. La idea es encontrar un modelo aditivo que minimice una función de perdida utilizando predictores débiles (por ejemplo árboles). Si como función de pérdida se utiliza RSS, entonces la pérdida de utilizar \\(m(x)\\) para predecir \\(y\\) en los datos de entrenamiento es \\[L(m) = \\sum_{i=1}^n L(y_i, m(x_i)) = \\sum_{i=1}^n (y_i - m(x_i))^2\\] Se desea minimizar \\(L(m)\\) con respecto a \\(m\\) mediante el método de los gradientes, pero estos son precisamente los residuos: si \\(L(m)= \\frac{1}{2} (y_i - m(x_i))^2\\), entonces \\[- \\frac{\\partial L(y_i, m(x_i))} {\\partial m(x_i)} = y_i - m(x_i) = r_i\\] Una ventaja de esta aproximación es que puede extenderse a otras funciones de pérdida, por ejemplo si hay valores atípicos se puede considerar como función de pérdida el error absoluto. Veamos el algoritmo para un problema de regresión utilizando árboles de decisión. Es un proceso iterativo en el que lo que se ataca no son los datos directamente, sino los residuos (gradientes) que van quedando con los sucesivos ajustes, siguiendo una idea greedy (la optimización se resuelve en cada iteración, no globalmente). Seleccionar el número de iteraciones \\(B\\), el parámetro de regularización \\(\\lambda\\) y el número de cortes de cada árbol \\(d\\). Establecer una predicción inicial constante y calcular los residuos de los datos \\(i\\) de la muestra de entrenamiento: \\[\\hat m (x) = 0, \\ r_i = y_i\\] Para \\(b = 1, 2,\\ldots, B\\), repetir: Ajustar un árbol de regresión \\(\\hat m^b\\) con \\(d\\) cortes utilizando los residuos como respuesta: \\((X, r)\\). Calcular la versión regularizada del árbol: \\[\\lambda \\hat m^b (x)\\] Actualizar los residuos: \\[r_i \\leftarrow r_i - \\lambda \\hat m^b (x_i)\\] Calcular el modelo boosting: \\[\\hat m (x) = \\sum_{b=1}^{B} \\lambda \\hat m^b (x)\\] Comprobamos que este método depende de 3 hiperparámetros, \\(B\\), \\(d\\) y \\(\\lambda\\), susceptibles de ser seleccionados de forma óptima: \\(B\\) es el número de árboles. Un valor muy grande podría llegar a provocar un sobreajuste (algo que no ocurre ni con bagging ni con bosques aleatorios, ya que estos son métodos en los que se construyen árboles independientes). En cada iteración, el objetivo es ajustar de forma óptima el gradiente (en nuestro caso, los residuos), pero este enfoque greedy no garantiza el óptimo global y puede dar lugar a sobreajustes. Al ser necesario que el aprendizaje sea lento se utilizan árboles muy pequeños. Esto consigue que poco a poco se vayan cubriendo las zonas en las que es más difícil predecir bien. En muchas situaciones funciona bien utilizar \\(d = 1\\), es decir, con un único corte. En este caso en cada \\(\\hat m^b\\) interviene una única variable, y por tanto \\(\\hat m\\) es un ajuste de un modelo aditivo. Si \\(d&gt;1\\) se puede interpretar como un parámetro que mide el órden de interacción entre las variables. \\(0 &lt; \\lambda &lt; 1\\), parámetro de regularización. Las primeras versiones del algorimo utilizaban un \\(\\lambda = 1\\), pero no funcionaba bien del todo. Se mejoró mucho el rendimiento ralentizando aún más el aprendizaje al incorporar al modelo el parámetro \\(\\lambda\\), que se puede interpretar como una proporción de aprendizaje (la velocidad a la que aprende, learning rate). Valores pequeños de \\(\\lambda\\) evitan el problema del sobreajuste, siendo habitual utilizar \\(\\lambda = 0.01\\) o \\(\\lambda = 0.001\\). Como ya se ha dicho, lo ideal es seleccionar su valor utilizando, por ejemplo, validación cruzada. Por supuesto, cuanto más pequeño sea el valor de \\(\\lambda\\), más lento va a ser el proceso de aprendizaje y serán necesarias más iteraciones, lo cual incrementa los tiempos de cómputo. El propio Friedman propuso una mejora de su algoritmo (Friedman, 2002), inspirado por la técnica bagging de Breiman. Esta variante, conocida como stochastic gradient boosting (SGB), es a día de hoy una de las más utilizadas. La única diferencia respecto al algoritmo anterior es en la primera línea dentro del bucle: al hacer el ajuste de \\((X, r)\\), no se considera toda la muestra de entrenamiento, sino que se selecciona al azar un subconjunto. Esto incorpora un nuevo hiperparámetro a la metodología, la fracción que se utiliza de los datos. Lo ideal es seleccionar un valor por algún método automático (tunearlo) tipo validación cruzada; una selección manual típica es 0.5. Hay otras variantes, como por ejemplo la selección aleatoria de predictores antes de crecer cada árbol o antes de cada corte (ver por ejemplo la documentación de h2o::gbm). Este sería un ejemplo de un método con muchos hiperparámetros y diseñar una buena estrategia para ajustarlos (tunearlos) puede resultar mucho más complicado (puede haber problemas de mínimos locales, problemas computacionales, etc.). SGB incorpora dos ventajas importantes: reduce la varianza y reduce los tiempos de cómputo. En terminos de rendimiento tanto el método SGB como random forest son muy competitivos, y por tanto son muy utilizando en la práctica. Los bosques aleatorios tienen la ventaja de que, al construir árboles de forma independiente, es paralelizable y eso puede reducir los tiempos de cómputo. Otro método reciente que está ganando popularidad es extreme gradient boosting, también conocido como XGBoost (Chen y Guestrin, 2016). Es un metodo más complejo que el anterior que, entre otras modificaciones, utiliza una función de pérdida con una penalización por complejidad y, para evitar el sobreajuste, regulariza utilizando la hessiana de la función de pérdida (necesita calcular las derivadas parciales de primer y de segundo orden), e incorpora parámetros de regularización adicionales para evitar el sobreajuste. Por último, la importancia de las variables se puede medir de forma similar a lo que ya hemos visto en otros métodos: dentro de cada árbol se sumas las reducciones del error que consigue cada predictor, y se promedia entre todos los árboles utilizados. En resumen: La idea es hacer un aprendizaje lento. Los arboles se crecen de forma secuencial, se trata de mejorar la clasificación anterior. Se utilizan arboles pequeños. A diferencia de bagging y bosques aleatorios puede haber problemas de sobreajuste (si el número de árboles es grande y la tasa de aprendizaje es alta). Se puede pensar que se ponderan las observaciones iterativamente, se asigna más peso a las que resultaron más difíciles de clasificar. El modelo final es un modelo aditivo (media ponderada de los árboles). "],["H-5-boosting-en-r.html", "H.5 Boosting en R", " H.5 Boosting en R Estos métodos son también de los más populares en AE y están implementados en numerosos paquetes de R: ada, adabag, mboost, gbm, xgboost H.5.1 Ejemplo: clasificación con el paquete ada La función ada() del paquete ada (Culp et al., 2006) implementa diversos métodos boosting (incluyendo el algoritmo original AdaBoost). Emplea rpart para la construcción de los árboles, aunque solo admite respuestas dicotómicas y dos funciones de pérdida (exponencial y logística). Además, un posible problema al emplear esta función es que ordena alfabéticamente los niveles del factor, lo que puede llevar a una mala interpretación de los resultados. Los principales parámetros son los siguientes: ada(formula, data, loss = c(&quot;exponential&quot;, &quot;logistic&quot;), type = c(&quot;discrete&quot;, &quot;real&quot;, &quot;gentle&quot;), iter = 50, nu = 0.1, bag.frac = 0.5, ...) formula y data (opcional): permiten especificar la respuesta y las variables predictoras de la forma habitual (típicamente respuesta ~ .; también admite matrices x e y en lugar de fórmulas). loss: función de pérdida; por defecto \"exponential\" (algoritmo AdaBoost). type: algoritmo boosting; por defecto \"discrete\" que implementa el algoritmo AdaBoost original que predice la variable respuesta. Otras alternativas son \"real\", que implementa el algoritmo Real AdaBoost (Friedman et al., 2000) que permite estimar las probabilidades, y \"gentle\" , versión modificada del anterior que emplea un método Newton de optimización por pasos (en lugar de optimización exacta). iter: número de iteraciones boosting; por defecto 50. nu: parámetro de regularización \\(\\lambda\\); por defecto 0.1 (disminuyendo este parámetro es de esperar que se obtenga una mejora en la precisión de las predicciones pero requería aumentar iter aumentando notablemente el tiempo de computación y los requerimientos de memoria). bag.frac: proporción de observaciones seleccionadas al azar para crecer cada árbol; por defecto 0.5. ...: argumentos adicionales para rpart.control; por defecto rpart.control(maxdepth = 1, cp = -1, minsplit = 0, xval = 0). Como ejemplo consideraremos el conjunto de datos de calidad de vino empleado en las secciones G.3.2 y H.3, pero para evitar problemas reordenamos alfabéticamente los niveles de la respuesta. load(&quot;datos/winetaste.RData&quot;) # Reordenar alfabéticamente los niveles de winetaste$taste # winetaste$taste &lt;- factor(winetaste$taste, sort(levels(winetaste$taste))) winetaste$taste &lt;- factor(as.character(winetaste$taste)) # Partición de los datos set.seed(1) df &lt;- winetaste nobs &lt;- nrow(df) itrain &lt;- sample(nobs, 0.8 * nobs) train &lt;- df[itrain, ] test &lt;- df[-itrain, ] Por ejemplo, el siguiente código llama a la función ada() con la opción para estimar probabilidades (type = \"real\", Real AdaBoost), considerando interacciones (de orden 2) entre los predictores (maxdepth = 2), disminuyendo ligeramente el valor del parámetro de aprendizaje y aumentando el número de iteraciones: library(ada) ada.boost &lt;- ada(taste ~ ., data = train, type = &quot;real&quot;, control = rpart.control(maxdepth = 2, cp = 0, minsplit = 10, xval = 0), iter = 100, nu = 0.05) ada.boost ## Call: ## ada(taste ~ ., data = train, type = &quot;real&quot;, control = rpart.control(maxdepth = 2, ## cp = 0, minsplit = 10, xval = 0), iter = 100, nu = 0.05) ## ## Loss: exponential Method: real Iteration: 100 ## ## Final Confusion Matrix for Data: ## Final Prediction ## True value bad good ## bad 162 176 ## good 46 616 ## ## Train Error: 0.222 ## ## Out-Of-Bag Error: 0.233 iteration= 99 ## ## Additional Estimates of number of iterations: ## ## train.err1 train.kap1 ## 93 93 Con el método plot() podemos representar la evolución del error de clasificación al aumentar el número de iteraciones: plot(ada.boost) Podemos evaluar la precisión en la muestra de test empleando el procedimiento habitual: pred &lt;- predict(ada.boost, newdata = test) caret::confusionMatrix(pred, test$taste, positive = &quot;good&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction bad good ## bad 34 16 ## good 50 150 ## ## Accuracy : 0.736 ## 95% CI : (0.6768, 0.7895) ## No Information Rate : 0.664 ## P-Value [Acc &gt; NIR] : 0.008615 ## ## Kappa : 0.3426 ## ## Mcnemar&#39;s Test P-Value : 4.865e-05 ## ## Sensitivity : 0.9036 ## Specificity : 0.4048 ## Pos Pred Value : 0.7500 ## Neg Pred Value : 0.6800 ## Prevalence : 0.6640 ## Detection Rate : 0.6000 ## Detection Prevalence : 0.8000 ## Balanced Accuracy : 0.6542 ## ## &#39;Positive&#39; Class : good ## Para obtener las estimaciones de las probabilidades, habría que establecer type = \"probs\" al predecir (devolverá una matriz con columnas correspondientes a los niveles): p.est &lt;- predict(ada.boost, newdata = test, type = &quot;probs&quot;) head(p.est) ## [,1] [,2] ## 1 0.49877103 0.5012290 ## 4 0.30922187 0.6907781 ## 9 0.02774336 0.9722566 ## 10 0.04596187 0.9540381 ## 12 0.44274407 0.5572559 ## 16 0.37375910 0.6262409 Este procedimiento también está implementado en el paquete caret seleccionando el método \"ada\", que considera como hiperparámetros: library(caret) modelLookup(&quot;ada&quot;) ## model parameter label forReg forClass probModel ## 1 ada iter #Trees FALSE TRUE TRUE ## 2 ada maxdepth Max Tree Depth FALSE TRUE TRUE ## 3 ada nu Learning Rate FALSE TRUE TRUE Aunque por defecto la función train() solo considera nueve combinaciones de hiperparámetros: set.seed(1) caret.ada0 &lt;- train(taste ~ ., method = &quot;ada&quot;, data = train, trControl = trainControl(method = &quot;cv&quot;, number = 5)) caret.ada0 ## Boosted Classification Trees ## ## 1000 samples ## 11 predictor ## 2 classes: &#39;bad&#39;, &#39;good&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 800, 801, 800, 800, 799 ## Resampling results across tuning parameters: ## ## maxdepth iter Accuracy Kappa ## 1 50 0.7100121 0.2403486 ## 1 100 0.7220322 0.2824931 ## 1 150 0.7360322 0.3346624 ## 2 50 0.7529774 0.3872880 ## 2 100 0.7539673 0.4019619 ## 2 150 0.7559673 0.4142035 ## 3 50 0.7570024 0.4112842 ## 3 100 0.7550323 0.4150030 ## 3 150 0.7650024 0.4408835 ## ## Tuning parameter &#39;nu&#39; was held constant at a value of 0.1 ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were iter = 150, maxdepth = 3 and nu = 0.1. confusionMatrix(predict(caret.ada0, newdata = test), test$taste, positive = &quot;good&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction bad good ## bad 37 22 ## good 47 144 ## ## Accuracy : 0.724 ## 95% CI : (0.6641, 0.7785) ## No Information Rate : 0.664 ## P-Value [Acc &gt; NIR] : 0.024724 ## ## Kappa : 0.3324 ## ## Mcnemar&#39;s Test P-Value : 0.003861 ## ## Sensitivity : 0.8675 ## Specificity : 0.4405 ## Pos Pred Value : 0.7539 ## Neg Pred Value : 0.6271 ## Prevalence : 0.6640 ## Detection Rate : 0.5760 ## Detection Prevalence : 0.7640 ## Balanced Accuracy : 0.6540 ## ## &#39;Positive&#39; Class : good ## Se puede aumentar el número de combinaciones empleando tuneLength o tuneGrid pero la búsqueda en una rejilla completa puede incrementar considerablemente el tiempo de computación. Por este motivo se suelen seguir distintos procedimientos de búsqueda. Por ejemplo, fijar la tasa de aprendizaje (inicialmente a un valor alto) para seleccionar primero un número de interaciones y la complejidad del árbol, y posteriormente fijar estos valores para seleccionar una nueva tasa de aprendizaje (repitiendo el proceso, si es necesario, hasta convergencia). set.seed(1) caret.ada1 &lt;- train(taste ~ ., method = &quot;ada&quot;, data = train, tuneGrid = data.frame(iter = 150, maxdepth = 3, nu = c(0.3, 0.1, 0.05, 0.01, 0.005)), trControl = trainControl(method = &quot;cv&quot;, number = 5)) caret.ada1 ## Boosted Classification Trees ## ## 1000 samples ## 11 predictor ## 2 classes: &#39;bad&#39;, &#39;good&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 800, 801, 800, 800, 799 ## Resampling results across tuning parameters: ## ## nu Accuracy Kappa ## 0.005 0.7439722 0.3723405 ## 0.010 0.7439822 0.3725968 ## 0.050 0.7559773 0.4116753 ## 0.100 0.7619774 0.4365242 ## 0.300 0.7580124 0.4405127 ## ## Tuning parameter &#39;iter&#39; was held constant at a value of 150 ## Tuning ## parameter &#39;maxdepth&#39; was held constant at a value of 3 ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were iter = 150, maxdepth = 3 and nu = 0.1. confusionMatrix(predict(caret.ada1, newdata = test), test$taste, positive = &quot;good&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction bad good ## bad 40 21 ## good 44 145 ## ## Accuracy : 0.74 ## 95% CI : (0.681, 0.7932) ## No Information Rate : 0.664 ## P-Value [Acc &gt; NIR] : 0.005841 ## ## Kappa : 0.375 ## ## Mcnemar&#39;s Test P-Value : 0.006357 ## ## Sensitivity : 0.8735 ## Specificity : 0.4762 ## Pos Pred Value : 0.7672 ## Neg Pred Value : 0.6557 ## Prevalence : 0.6640 ## Detection Rate : 0.5800 ## Detection Prevalence : 0.7560 ## Balanced Accuracy : 0.6748 ## ## &#39;Positive&#39; Class : good ## H.5.2 Ejemplo: regresión con el paquete gbm El paquete gbm implementa el algoritmo SGB de Friedman (2002) y admite varios tipos de respuesta considerando distintas funciones de pérdida (aunque en el caso de variables dicotómicas éstas deben tomar valores en \\(\\{0, 1\\}\\)21). La función principal es gbm() y se suelen considerar los siguientes argumentos: gbm( formula, distribution = &quot;bernoulli&quot;, data, n.trees = 100, interaction.depth = 1, n.minobsinnode = 10, shrinkage = 0.1, bag.fraction = 0.5, cv.folds = 0, n.cores = NULL) formula y data (opcional): permiten especificar la respuesta y las variables predictoras de la forma habitual (típicamente respuesta ~ .; también está disponible una interfaz con matrices gbm.fit()). distribution (opcional): texto con el nombre de la distribución (o lista con el nombre en name y parámetros adicionales en los demás componentes) que determina la función de pérdida. Si se omite se establecerá a partir del tipo de la respuesta: \"bernouilli\" (regresión logística) si es una variable dicotómica 0/1, \"multinomial\" (regresión multinomial) si es un factor (no se recomienda) y \"gaussian\" (error cuadrático) en caso contrario. Otras opciones que pueden ser de interés son: \"laplace\" (error absoluto), \"adaboost\" (pérdida exponencial para respuestas dicotómicas 0/1), \"huberized\" (pérdida de Huber para respuestas dicotómicas 0/1), \"poisson\" (regresión de Poisson) y \"quantile\" (regresión cuantil). ntrees: iteraciones/número de árboles que se crecerán; por defecto 100 (se puede emplear la función gbm.perf() para seleccionar un valor óptimo). interaction.depth: profundidad de los árboles; por defecto 1 (modelo aditivo). n.minobsinnode: número mínimo de observaciones en un nodo terminal; por defecto 10. shrinkage: parámetro de regularización \\(\\lambda\\); por defecto 0.1. bag.fraction: proporción de observaciones seleccionadas al azar para crecer cada árbol; por defecto 0.5. cv.folds: número grupos para validación cruzada; por defecto 0 (no se hace validación cruzada). Si se asigna un valor mayor que 1 se realizará validación cruzada y se devolverá el error en la componente $cv.error (se puede emplear para seleccionar hiperparámetros). n.cores: número de núcleos para el procesamiento en paralelo. Como ejemplo consideraremos el conjunto de datos winequality.RData: load(&quot;datos/winequality.RData&quot;) set.seed(1) df &lt;- winequality nobs &lt;- nrow(df) itrain &lt;- sample(nobs, 0.8 * nobs) train &lt;- df[itrain, ] test &lt;- df[-itrain, ] library(gbm) gbm.fit &lt;- gbm(quality ~ ., data = train) ## Distribution not specified, assuming gaussian ... gbm.fit ## gbm(formula = quality ~ ., data = train) ## A gradient boosted model with gaussian loss function. ## 100 iterations were performed. ## There were 11 predictors of which 11 had non-zero influence. El método summary() calcula las medidas de influencia de los predictores y las representa gráficamente: summary(gbm.fit) ## var rel.inf ## alcohol alcohol 40.907998 ## volatile.acidity volatile.acidity 13.839083 ## free.sulfur.dioxide free.sulfur.dioxide 11.488262 ## fixed.acidity fixed.acidity 7.914742 ## citric.acid citric.acid 6.765875 ## total.sulfur.dioxide total.sulfur.dioxide 4.808308 ## residual.sugar residual.sugar 4.758566 ## chlorides chlorides 3.424537 ## sulphates sulphates 3.086036 ## density density 1.918442 ## pH pH 1.088152 Para estudiar el efecto de un predictor se pueden generar gráficos de los efectos parciales mediante el método plot(): plot(gbm.fit, i = &quot;alcohol&quot;) Finalmente podemos evaluar la precisión en la muestra de test empleando el código habitual: pred &lt;- predict(gbm.fit, newdata = test) obs &lt;- test$quality # Con el paquete caret caret::postResample(pred, obs) ## RMSE Rsquared MAE ## 0.7586208 0.3001401 0.6110442 # Con la función accuracy() accuracy &lt;- function(pred, obs, na.rm = FALSE, tol = sqrt(.Machine$double.eps)) { err &lt;- obs - pred # Errores if(na.rm) { is.a &lt;- !is.na(err) err &lt;- err[is.a] obs &lt;- obs[is.a] } perr &lt;- 100*err/pmax(obs, tol) # Errores porcentuales return(c( me = mean(err), # Error medio rmse = sqrt(mean(err^2)), # Raíz del error cuadrático medio mae = mean(abs(err)), # Error absoluto medio mpe = mean(perr), # Error porcentual medio mape = mean(abs(perr)), # Error porcentual absoluto medio r.squared = 1 - sum(err^2)/sum((obs - mean(obs))^2) )) } accuracy(pred, obs) ## me rmse mae mpe mape r.squared ## -0.01463661 0.75862081 0.61104421 -2.00702056 10.69753668 0.29917590 Este procedimiento también está implementado en el paquete caret seleccionando el método \"gbm\", que considera como hiperparámetros: library(caret) modelLookup(&quot;gbm&quot;) ## model parameter label forReg forClass probModel ## 1 gbm n.trees # Boosting Iterations TRUE TRUE TRUE ## 2 gbm interaction.depth Max Tree Depth TRUE TRUE TRUE ## 3 gbm shrinkage Shrinkage TRUE TRUE TRUE ## 4 gbm n.minobsinnode Min. Terminal Node Size TRUE TRUE TRUE Aunque por defecto la función train() solo considera nueve combinaciones de hiperparámetros. Para hacer una búsqueda más completa se podría seguir un procedimiento análogo al empleado con el método anterior: set.seed(1) caret.gbm0 &lt;- train(quality ~ ., method = &quot;gbm&quot;, data = train, trControl = trainControl(method = &quot;cv&quot;, number = 5)) caret.gbm0 ## Stochastic Gradient Boosting ## ## 1000 samples ## 11 predictor ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 800, 801, 800, 800, 799 ## Resampling results across tuning parameters: ## ## interaction.depth n.trees RMSE Rsquared MAE ## 1 50 0.7464098 0.2917796 0.5949686 ## 1 100 0.7258319 0.3171046 0.5751816 ## 1 150 0.7247246 0.3197241 0.5719404 ## 2 50 0.7198195 0.3307665 0.5712468 ## 2 100 0.7175006 0.3332903 0.5647409 ## 2 150 0.7258174 0.3222006 0.5713116 ## 3 50 0.7241661 0.3196365 0.5722590 ## 3 100 0.7272094 0.3191252 0.5754363 ## 3 150 0.7311429 0.3152905 0.5784988 ## ## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1 ## ## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 10 ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were n.trees = 100, interaction.depth = ## 2, shrinkage = 0.1 and n.minobsinnode = 10. caret.gbm1 &lt;- train(quality ~ ., method = &quot;gbm&quot;, data = train, tuneGrid = data.frame(n.trees = 100, interaction.depth = 2, shrinkage = c(0.3, 0.1, 0.05, 0.01, 0.005), n.minobsinnode = 10), trControl = trainControl(method = &quot;cv&quot;, number = 5)) caret.gbm1 ## Stochastic Gradient Boosting ## ## 1000 samples ## 11 predictor ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 800, 800, 801, 799, 800 ## Resampling results across tuning parameters: ## ## shrinkage RMSE Rsquared MAE ## 0.005 0.8154916 0.2419131 0.6245818 ## 0.010 0.7844257 0.2602989 0.6128582 ## 0.050 0.7206972 0.3275463 0.5707273 ## 0.100 0.7124838 0.3407642 0.5631748 ## 0.300 0.7720844 0.2613835 0.6091765 ## ## Tuning parameter &#39;n.trees&#39; was held constant at a value of 100 ## Tuning ## parameter &#39;interaction.depth&#39; was held constant at a value of 2 ## ## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 10 ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were n.trees = 100, interaction.depth = ## 2, shrinkage = 0.1 and n.minobsinnode = 10. varImp(caret.gbm1) ## gbm variable importance ## ## Overall ## alcohol 100.0000 ## volatile.acidity 28.4909 ## free.sulfur.dioxide 24.5158 ## residual.sugar 16.8406 ## fixed.acidity 12.5623 ## density 10.1917 ## citric.acid 9.1542 ## total.sulfur.dioxide 7.2659 ## chlorides 4.5106 ## pH 0.1096 ## sulphates 0.0000 postResample(predict(caret.gbm1, newdata = test), test$quality) ## RMSE Rsquared MAE ## 0.7403768 0.3329751 0.6017281 H.5.3 Ejemplo: XGBoost con el paquete caret El método boosting implementado en el paquete xgboost es uno de los más populares hoy en día. Esta implementación proporciona parámetros adicionales de regularización para controlar la complejidad del modelo y tratar de evitar el sobreajuste. También incluye criterios de parada, para detener la evaluación del modelo cuando los árboles adicionales no ofrecen ninguna mejora. Dispone de una interfaz simple xgboost() y otra más avanzada xgb.train(), que admite funciones de pérdida y evaluación personalizadas. Normalmente es necesario un preprocesado de los datos antes de llamar a estas funciones, ya que requieren de una matriz para los predictores y de un vector para la respuesta (además en el caso de que sea dicotómica debe tomar valores en \\(\\{0, 1\\}\\)). Por tanto es necesario recodificar las variables categóricas como numéricas. Por este motivo puede ser preferible emplear la interfaz de caret. El algoritmo estándar XGBoost, que emplea árboles como modelo base, está implementado en el método \"xgbTree\" de caret22. library(caret) # names(getModelInfo(&quot;xgb&quot;)) modelLookup(&quot;xgbTree&quot;) ## model parameter label forReg forClass ## 1 xgbTree nrounds # Boosting Iterations TRUE TRUE ## 2 xgbTree max_depth Max Tree Depth TRUE TRUE ## 3 xgbTree eta Shrinkage TRUE TRUE ## 4 xgbTree gamma Minimum Loss Reduction TRUE TRUE ## 5 xgbTree colsample_bytree Subsample Ratio of Columns TRUE TRUE ## 6 xgbTree min_child_weight Minimum Sum of Instance Weight TRUE TRUE ## 7 xgbTree subsample Subsample Percentage TRUE TRUE ## probModel ## 1 TRUE ## 2 TRUE ## 3 TRUE ## 4 TRUE ## 5 TRUE ## 6 TRUE ## 7 TRUE Este método considera los siguientes hiperparámetros: \"nrounds\": número de iteraciones boosting. \"max_depth\": profundidad máxima del árbol; por defecto 6. \"eta\": parámetro de regularización \\(\\lambda\\); por defecto 0.3. \"gamma\": mínima reducción de la pérdida para hacer una partición adicional en un nodo del árbol; por defecto 0. \"colsample_bytree\": proporción de predictores seleccionados al azar para crecer cada árbol; por defecto 1. \"min_child_weight\": suma mínima de peso (hessiana) para hacer una partición adicional en un nodo del árbol; por defecto 1. \"subsample\": proporción de observaciones seleccionadas al azar en cada iteración boosting; por defecto 1. Para más información sobre parámetros adicionales se puede consultar la ayuda de xgboost::xgboost() o la lista detallada disponible en la Sección XGBoost Parameters del Manual de XGBoost. Como ejemplo consideraremos el problema de clasificación empleando el conjunto de datos de calidad de vino: load(&quot;datos/winetaste.RData&quot;) set.seed(1) df &lt;- winetaste nobs &lt;- nrow(df) itrain &lt;- sample(nobs, 0.8 * nobs) train &lt;- df[itrain, ] test &lt;- df[-itrain, ] En este caso la función train() considera por defecto 108 combinaciones de hiperparámetros y el tiempo de computación puede ser excesivo. caret.xgb &lt;- train(taste ~ ., method = &quot;xgbTree&quot;, data = train, trControl = trainControl(method = &quot;cv&quot;, number = 5)) caret.xgb ## eXtreme Gradient Boosting ## ## 1000 samples ## 11 predictor ## 2 classes: &#39;good&#39;, &#39;bad&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 799, 801, 801, 799, 800 ## Resampling results across tuning parameters: ## ## eta max_depth colsample_bytree subsample nrounds Accuracy Kappa ## 0.3 1 0.6 0.50 50 0.7479499 0.3997718 ## 0.3 1 0.6 0.50 100 0.7509649 0.4226367 ## 0.3 1 0.6 0.50 150 0.7480199 0.4142399 ## 0.3 1 0.6 0.75 50 0.7389498 0.3775707 ## 0.3 1 0.6 0.75 100 0.7499600 0.4178857 ## 0.3 1 0.6 0.75 150 0.7519900 0.4194354 ## 0.3 1 0.6 1.00 50 0.7479450 0.3933223 ## 0.3 1 0.6 1.00 100 0.7439499 0.3946755 ## 0.3 1 0.6 1.00 150 0.7479699 0.4054549 ## 0.3 1 0.8 0.50 50 0.7279446 0.3514309 ## 0.3 1 0.8 0.50 100 0.7379647 0.3901818 ## 0.3 1 0.8 0.50 150 0.7289797 0.3702869 ## 0.3 1 0.8 0.75 50 0.7419548 0.3853122 ## 0.3 1 0.8 0.75 100 0.7419798 0.3939408 ## 0.3 1 0.8 0.75 150 0.7490050 0.4119554 ## 0.3 1 0.8 1.00 50 0.7469399 0.3903359 ## 0.3 1 0.8 1.00 100 0.7469349 0.3994462 ## 0.3 1 0.8 1.00 150 0.7429499 0.3930019 ## 0.3 2 0.6 0.50 50 0.7469800 0.4072389 ## 0.3 2 0.6 0.50 100 0.7560152 0.4315043 ## 0.3 2 0.6 0.50 150 0.7470550 0.4202096 ## 0.3 2 0.6 0.75 50 0.7419347 0.3991878 ## 0.3 2 0.6 0.75 100 0.7419398 0.3985245 ## 0.3 2 0.6 0.75 150 0.7408999 0.4048017 ## 0.3 2 0.6 1.00 50 0.7529250 0.4183744 ## 0.3 2 0.6 1.00 100 0.7559601 0.4332161 ## 0.3 2 0.6 1.00 150 0.7439798 0.4082169 ## 0.3 2 0.8 0.50 50 0.7479801 0.4039828 ## 0.3 2 0.8 0.50 100 0.7439500 0.4017708 ## 0.3 2 0.8 0.50 150 0.7409099 0.4002330 ## 0.3 2 0.8 0.75 50 0.7549701 0.4309398 ## 0.3 2 0.8 0.75 100 0.7469550 0.4077312 ## 0.3 2 0.8 0.75 150 0.7529701 0.4282530 ## 0.3 2 0.8 1.00 50 0.7509800 0.4151042 ## 0.3 2 0.8 1.00 100 0.7479899 0.4164189 ## 0.3 2 0.8 1.00 150 0.7439498 0.4044785 ## 0.3 3 0.6 0.50 50 0.7529851 0.4322174 ## 0.3 3 0.6 0.50 100 0.7479900 0.4200214 ## 0.3 3 0.6 0.50 150 0.7499800 0.4307546 ## 0.3 3 0.6 0.75 50 0.7499550 0.4263366 ## 0.3 3 0.6 0.75 100 0.7519201 0.4321688 ## 0.3 3 0.6 0.75 150 0.7459449 0.4177412 ## 0.3 3 0.6 1.00 50 0.7529251 0.4220849 ## 0.3 3 0.6 1.00 100 0.7519400 0.4237486 ## 0.3 3 0.6 1.00 150 0.7519500 0.4294623 ## 0.3 3 0.8 0.50 50 0.7510299 0.4327919 ## 0.3 3 0.8 0.50 100 0.7519799 0.4405268 ## 0.3 3 0.8 0.50 150 0.7619652 0.4559423 ## 0.3 3 0.8 0.75 50 0.7470501 0.4131934 ## 0.3 3 0.8 0.75 100 0.7479849 0.4129185 ## 0.3 3 0.8 0.75 150 0.7509850 0.4261251 ## 0.3 3 0.8 1.00 50 0.7449099 0.4008981 ## 0.3 3 0.8 1.00 100 0.7610054 0.4422136 ## 0.3 3 0.8 1.00 150 0.7569803 0.4382787 ## 0.4 1 0.6 0.50 50 0.7370397 0.3774680 ## 0.4 1 0.6 0.50 100 0.7340546 0.3874281 ## 0.4 1 0.6 0.50 150 0.7490550 0.4204110 ## 0.4 1 0.6 0.75 50 0.7330097 0.3695029 ## 0.4 1 0.6 0.75 100 0.7269447 0.3595653 ## 0.4 1 0.6 0.75 150 0.7409999 0.3999882 ## 0.4 1 0.6 1.00 50 0.7389548 0.3787453 ## 0.4 1 0.6 1.00 100 0.7479499 0.4061188 ## 0.4 1 0.6 1.00 150 0.7410049 0.3940049 ## 0.4 1 0.8 0.50 50 0.7269246 0.3647893 ## 0.4 1 0.8 0.50 100 0.7459551 0.4088011 ## 0.4 1 0.8 0.50 150 0.7359947 0.3910800 ## 0.4 1 0.8 0.75 50 0.7369797 0.3798786 ## 0.4 1 0.8 0.75 100 0.7329997 0.3808412 ## 0.4 1 0.8 0.75 150 0.7410149 0.4007794 ## 0.4 1 0.8 1.00 50 0.7429449 0.3889734 ## 0.4 1 0.8 1.00 100 0.7549401 0.4194777 ## 0.4 1 0.8 1.00 150 0.7499600 0.4117257 ## 0.4 2 0.6 0.50 50 0.7340497 0.3817464 ## 0.4 2 0.6 0.50 100 0.7330547 0.3836073 ## 0.4 2 0.6 0.50 150 0.7429900 0.4086515 ## 0.4 2 0.6 0.75 50 0.7490100 0.4065411 ## 0.4 2 0.6 0.75 100 0.7399647 0.4013642 ## 0.4 2 0.6 0.75 150 0.7480149 0.4165452 ## 0.4 2 0.6 1.00 50 0.7519601 0.4189103 ## 0.4 2 0.6 1.00 100 0.7559751 0.4326368 ## 0.4 2 0.6 1.00 150 0.7649804 0.4559090 ## 0.4 2 0.8 0.50 50 0.7430148 0.4088033 ## 0.4 2 0.8 0.50 100 0.7459399 0.4110881 ## 0.4 2 0.8 0.50 150 0.7359897 0.3929835 ## 0.4 2 0.8 0.75 50 0.7509801 0.4207733 ## 0.4 2 0.8 0.75 100 0.7399848 0.3993503 ## 0.4 2 0.8 0.75 150 0.7429548 0.4092104 ## 0.4 2 0.8 1.00 50 0.7609753 0.4402344 ## 0.4 2 0.8 1.00 100 0.7669804 0.4572722 ## 0.4 2 0.8 1.00 150 0.7559651 0.4339887 ## 0.4 3 0.6 0.50 50 0.7440298 0.4091740 ## 0.4 3 0.6 0.50 100 0.7559752 0.4388366 ## 0.4 3 0.6 0.50 150 0.7659354 0.4555764 ## 0.4 3 0.6 0.75 50 0.7560301 0.4384091 ## 0.4 3 0.6 0.75 100 0.7540000 0.4330182 ## 0.4 3 0.6 0.75 150 0.7549501 0.4357856 ## 0.4 3 0.6 1.00 50 0.7449599 0.4072659 ## 0.4 3 0.6 1.00 100 0.7569501 0.4386990 ## 0.4 3 0.6 1.00 150 0.7589451 0.4502683 ## 0.4 3 0.8 0.50 50 0.7420546 0.4035922 ## 0.4 3 0.8 0.50 100 0.7489598 0.4278516 ## 0.4 3 0.8 0.50 150 0.7439448 0.4158271 ## 0.4 3 0.8 0.75 50 0.7509599 0.4200445 ## 0.4 3 0.8 0.75 100 0.7459798 0.4164791 ## 0.4 3 0.8 0.75 150 0.7599402 0.4479586 ## 0.4 3 0.8 1.00 50 0.7569851 0.4333259 ## 0.4 3 0.8 1.00 100 0.7439549 0.4063617 ## 0.4 3 0.8 1.00 150 0.7459649 0.4162883 ## ## Tuning parameter &#39;gamma&#39; was held constant at a value of 0 ## Tuning ## parameter &#39;min_child_weight&#39; was held constant at a value of 1 ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were nrounds = 100, max_depth = 2, eta ## = 0.4, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample ## = 1. caret.xgb$bestTune ## nrounds max_depth eta gamma colsample_bytree min_child_weight subsample ## 89 100 2 0.4 0 0.8 1 1 varImp(caret.xgb) ## xgbTree variable importance ## ## Overall ## alcohol 100.000 ## volatile.acidity 27.693 ## citric.acid 23.788 ## free.sulfur.dioxide 23.673 ## fixed.acidity 20.393 ## residual.sugar 15.734 ## density 10.956 ## chlorides 8.085 ## sulphates 3.598 ## pH 2.925 ## total.sulfur.dioxide 0.000 confusionMatrix(predict(caret.xgb, newdata = test), test$taste) ## Confusion Matrix and Statistics ## ## Reference ## Prediction good bad ## good 147 46 ## bad 19 38 ## ## Accuracy : 0.74 ## 95% CI : (0.681, 0.7932) ## No Information Rate : 0.664 ## P-Value [Acc &gt; NIR] : 0.005841 ## ## Kappa : 0.3671 ## ## Mcnemar&#39;s Test P-Value : 0.001260 ## ## Sensitivity : 0.8855 ## Specificity : 0.4524 ## Pos Pred Value : 0.7617 ## Neg Pred Value : 0.6667 ## Prevalence : 0.6640 ## Detection Rate : 0.5880 ## Detection Prevalence : 0.7720 ## Balanced Accuracy : 0.6690 ## ## &#39;Positive&#39; Class : good ## Se podría seguir una estrategia de búsqueda similar a la empleada en los métodos anteriores. Se puede evitar este inconveniente empleando la interfaz de caret. Otras alternativas son: \"xgbDART\" que también emplean árboles como modelo base, pero incluye el método DART (Vinayak y Gilad-Bachrach, 2015) para evitar sobreajuste (básicamente descarta árboles al azar en la secuencia), y\"xgbLinear\" que emplea modelos lineales. "],["I-svm.html", "I Máquinas de soporte vectorial", " I Máquinas de soporte vectorial Las máquinas de soporte vectorial (support vector machines, SVM) son métodos estadísticos que Vladimir Vapnik empezó a desarrollar a mediados de 1960, inicialmente para problemas de clasificación binaria (problemas de clasificación con dos categorias), basados en la idea de separar los datos mediante hiperplanos. Actualmente existen extensiones dentro de esta metodología para clasificación con más de dos categorías, para regresión y para detección de datos atípicos. El nombre proviene de la utilización de vectores que hacen de soporte para maximizar la separación entre los datos y el hiperplano. La popularidad de las máquinas de soporte vectorial creció a partir de los años 90 cuando los incorpora la comunidad informática. Se considera una metodología muy flexible y con buen rendimiento en un amplio abanico de situaciones, aunque por lo general no es la que consigue los mejores rendimientos. Dos referencias ya clásicas son Vapnik (1998) y Vapnik (2010). Siguiendo a James et al. (2013) distinguiremos en nuestra exposición entre clasificadores de máximo margen (maximal margin classifiers), clasificadores de soporte vectorial (support vector classifiers) y máquinas de soporte vectorial (support vector machines). "],["I-1-clasificadores-de-máximo-margen.html", "I.1 Clasificadores de máximo margen", " I.1 Clasificadores de máximo margen Los clasificadores de máximo margen (maximal margin classifiers; también denominados hard margin classifiers) son un método de clasificación binaria que se utiliza cuando hay una frontera lineal que separa perfectamente los datos de entrenamiento de una categoría de los de la otra. Por conveniencia, etiquetamos las dos categorías como +1/-1, es decir, los valores de la variable respuesta \\(Y \\in \\{-1, 1\\}\\). Y suponemos que existe un hiperplano \\[ \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_p X_p = 0,\\] donde \\(p\\) es el número de variables predictoras, que tiene la propiedad de separar los datos de entrenamiento según la categoría a la que pertenecen, es decir, \\[ y_i(\\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\ldots + \\beta_p x_{pi}) &gt; 0\\] para todo \\(i = 1, 2, \\ldots, n\\), siendo \\(n\\) el número de datos de entrenamiento. Una vez tenemos el hiperplano, clasificar una nueva observación \\(\\mathbf{x}\\) se reduce a calcular el signo de \\[m(\\mathbf{x}) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p\\] Si el signo es positivo, se clasifica como perteneciente a la categoría +1, y si es negativo a la categoría -1. Además, el valor absoluto de \\(m(\\mathbf{x})\\) nos da una idea de la distancia entre la observación y la frontera que define el hiperplano. En concreto \\[\\frac{y_i}{\\sqrt {\\sum_{j=1}^p \\beta_j^2}}(\\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\ldots + \\beta_p x_{pi})\\] sería la distancia de la observación \\(i\\)-ésima al hiperplano. Por supuesto, aunque clasifique los datos de entrenamiento sin error, no hay ninguna garantía de que clasifique bien nuevas observaciones, por ejemplo los datos de test. De hecho, si \\(p\\) es grande es fácil que haya un sobreajuste. Realmente, si existe al menos un hiperplano que separa perfectamente los datos de entrenamiento de las dos categorías, entonces va a haber infinitos. El objetivo es seleccionar un hiperplano. Para ello, dado un hiperplano, se calculan sus distancias a todos los datos de entrenamiento y se define el margen como la menor de esas distancias. El método maximal margin classifier lo que hace es seleccionar, de los infinitos hiperplanos, aquel que tiene el mayor margen. Fijémonos en que siempre va a haber varias observaciones que equidistan del hiperplano de máximo margen, y cuya distancia es precisamente el margen. Esas observaciones reciben el nombre de vectores soporte y son las que dan nombre a esta metodología. Matemáticamente, dadas las \\(n\\) observaciones de entrenamiento \\(\\mathbf{x_1}, \\mathbf{x_2}, \\ldots, \\mathbf{x_n}\\), el clasificador de máximo margen es la solución del problema de optimización \\[max_{\\beta_0, \\beta_1,\\ldots, \\beta_p} M\\] sujeto a \\[\\sum_{j=1}^p \\beta_j^2 = 1\\] \\[ y_i(\\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\ldots + \\beta_p x_{pi}) \\ge M \\ \\ \\forall i\\] Si, como estamos suponiendo en esta sección, los datos de entrenamiento son perfectamente separables mediante un hiperplano, entonces el problema anterior va a tener solución con \\(M&gt;0\\), y \\(M\\) va a ser el margen. Una forma equivalente (y mas conveniente) de formular el problema anterior, utilizando \\(M = 1/\\lVert \\boldsymbol{\\beta} \\rVert\\) con \\(\\boldsymbol{\\beta} = (\\beta_1, \\beta_2, \\ldots, \\beta_p)\\), es \\[\\mbox{min}_{\\beta_0, \\boldsymbol{\\beta}} \\lVert \\boldsymbol{\\beta} \\rVert\\] sujeto a \\[ y_i(\\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\ldots + \\beta_p x_{pi}) \\ge 1 \\ \\ \\forall i\\] El problema anterior de optimización es convexo (función objetivo cuadrática con restricciones lineales). Hay una característica de este método que es de destacar: así como en otros métodos, si se modifica cualquiera de los datos se modifica también el modelo, en este caso el modelo solo depende de los (pocos) datos que son vector soporte, y la modificación de cualquier otro dato no afecta a la construcción del modelo (siempre que, al moverse el dato, no cambie el margen). "],["I-2-clasificadores-de-soporte-vectorial.html", "I.2 Clasificadores de soporte vectorial", " I.2 Clasificadores de soporte vectorial Los clasificadores de soporte vectorial (support vector classifiers; también denominados soft margin classifiers) fueron introducidos en Costes y Vapnik (1995). Son una extensión del problema anterior que se utiliza cuando se desea clasificar mediante un hiperplano pero no existe ninguno que separe perfectamente los datos de entrenamiento según su categoría. En este caso no queda más remedio que admitir errores en la clasificación de algunos datos de entrenamiento (como hemos visto que pasa con todas las metodologías), que van a estar en el lado equivocado del hiperplano. Y en lugar de hablar de un margen se habla de un margen débil (soft margin). Este enfoque, consistente en aceptar que algunos datos de entrenamiento van a estar mal clasificados, puede ser preferible aunque exista un hiperplano que resuelva el problema de la sección anterior, ya que los clasificadores de soporte vectorial son más robustos que los clasificadores de máximo margen. Veamos la formulación matemática del problema: \\[\\mbox{max}_{\\beta_0, \\beta_1,\\ldots, \\beta_p, \\epsilon_1,\\ldots, \\epsilon_n} M\\] sujeto a \\[\\sum_{j=1}^p \\beta_j^2 = 1\\] \\[ y_i(\\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\ldots + \\beta_p x_{pi}) \\ge M(1 - \\epsilon_i) \\ \\ \\forall i\\] \\[\\sum_{i=1}^n \\epsilon_i \\le K\\] \\[\\epsilon_i \\ge 0 \\ \\ \\forall i\\] Las variables \\(\\epsilon_i\\) son las variables de holgura (slack variables). Quizás resultase más intuitivo introducir las holguras en términos absolutos, como \\(M -\\epsilon_i\\), pero eso daría lugar a un problema no convexo, mientras que escribiendo la restricción en términos relativos como \\(M(1 - \\epsilon_i)\\) el problema pasa a ser convexo. Pero en esta formulación el elemento clave es la introducción del hiperparámetro \\(K\\), necesariamente no negativo, que se puede interpretar como la tolerancia al error. De hecho, es fácil ver que no puede haber más de \\(K\\) datos de entrenamiento incorrectamente clasificados, ya que si un dato está mal clasificado entonces \\(\\epsilon_i &gt; 1\\). En el caso extremo de utilizar \\(K = 0\\), estaríamos en el caso de un hard margin classifier. La elección del valor de \\(K\\) también se puede interpretar como una penalización por la complejidad del modelo, y por tanto en términos del balance entre el sesgo y la varianza: valores pequeños van a dar lugar a modelos muy complejos, con mucha varianza y poco sesgo (con el consiguiente riesgo de sobreajuste); y valores grandes a modelos con mucho sesgo y poca varianza. El hiperparámetro \\(K\\) se puede seleccionar de modo óptimo por los procedimientos ya conocidos, tipo bootstrap o validación cruzada. Una forma equivalente de formular el problema (cuadrático con restricciones lineales) es \\[\\mbox{min}_{\\beta_0, \\boldsymbol{\\beta}} \\lVert \\boldsymbol{\\beta} \\rVert\\] sujeto a \\[ y_i(\\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\ldots + \\beta_p x_{pi}) \\ge 1 - \\epsilon_i \\ \\ \\forall i\\] \\[\\sum_{i=1}^n \\epsilon_i \\le K\\] \\[\\epsilon_i \\ge 0 \\ \\ \\forall i\\] En la práctica, por una conveniencia de cálculo, se utiliza la siguiente formulación, también equivalente, \\[\\mbox{min}_{\\beta_0, \\boldsymbol{\\beta}} \\frac{1}{2}\\lVert \\boldsymbol{\\beta} \\rVert^2 + C \\sum_{i=1}^n \\epsilon_i\\] sujeto a \\[ y_i(\\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\ldots + \\beta_p x_{pi}) \\ge 1 - \\epsilon_i \\ \\ \\forall i\\] \\[\\epsilon_i \\ge 0 \\ \\ \\forall i\\] Aunque el problema a resolver es el mismo, y por tanto también la solución, hay que tener cuidado con la interpretación, pues el hiperparámetro \\(K\\) se ha sustituido por \\(C\\). Este nuevo parámetro es el que nos vamos a encontrar en los ejercicios prácticos y tiene una interpretación inversa a \\(K\\). El parámetro \\(C\\) es la penalización por mala clasificación (coste que supone que un dato de entrenamiento esté mal clasificado), y por tanto el hard margin classifier se obtiene para valores muy grandes (\\(C = \\infty\\) se corresponde con \\(K = 0\\)). Esto es algo confuso, ya que no se corresponde con la interpretación habitual de penalización por complejidad. En este contexto, los vectores soporte van a ser no solo los datos de entrenamiento que están (correctamente clasificados) a una distancia \\(M\\) del hiperplano, sino también aquellos que están incorrectamente clasificados e incluso los que están a una distancia inferior a \\(M\\). Como se comentó en la sección anterior, estos son los datos que definen el modelo, que es por tanto robusto a las observaciones que están lejos del hiperplano. Aunque no vamos a entrar en detalles sobre como se obtiene la solución del problema de optimización, sí resulta interesante destacar que el clasificador de soporte vectorial \\[m(\\mathbf{x}) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p\\] puede representarse como \\[m(\\mathbf{x}) = \\beta_0 + \\sum_{i=1}^n \\alpha_i \\mathbf{x}^t \\mathbf{x}_i\\] donde \\(\\mathbf{x}^t \\mathbf{x}_i\\) es el producto escalar entre el vector \\(\\mathbf{x}\\) del dato a clasificar y el vector \\(\\mathbf{x}_i\\) del dato de entrenamiento \\(i\\)-ésimo. Asimismo, los coeficientes \\(\\beta_0, \\alpha_1, \\ldots, \\alpha_n\\) se obtienen (exclusivamente) a partir de los productos escalares \\(\\mathbf{x}_i^t \\mathbf{x}_j\\) de los distintos pares de datos de entrenamiento y de las respuestas \\(y_i\\). Y más aún, el sumatorio anterior se puede reducir a los índices que corresponden a vectores soporte (\\(i\\in S\\)), al ser los demás coeficientes nulos: \\[m(\\mathbf{x}) = \\beta_0 + \\sum_{i\\in S} \\alpha_i \\mathbf{x}^t \\mathbf{x}_i\\] "],["I-3-máquinas-de-soporte-vectorial.html", "I.3 Máquinas de soporte vectorial", " I.3 Máquinas de soporte vectorial De la misma manera que en el capítulo dedicado a árboles se comentó que estos serán efectivos en la medida en la que los datos se separen adecuadamente utilizando particiones basadas en rectángulos, los dos métodos de clasificación que hemos visto hasta ahora serán efectivos si hay una frontera lineal que separe los datos de las dos categorías. En caso contrario, un clasificador de soporte vectorial resultará inadecuado. Una solución natural es sustituir el hiperplano, lineal en esencia, por otra función que dependa de las variables predictoras \\(X_1,X_2, \\ldots, X_n\\), utilizando por ejemplo una expresión polinómica o incluso una expresión que no sea aditiva en los predictores. Pero esta solución puede resultar muy compleja computacionalmente. En Boser et al. (1992) se propuso sustituir, en todos los cálculos que conducen a la expresión \\[m(\\mathbf{x}) = \\beta_0 + \\sum_{i\\in S} \\alpha_i \\mathbf{x}^t \\mathbf{x}_i\\] los productos escalares \\(\\mathbf{x}^t \\mathbf{x}_i\\), \\(\\mathbf{x}_i^t \\mathbf{x}_j\\) por funciones alternativas de los datos que reciben el nombre de funciones kernel, obteniendo la máquina de soporte vectorial \\[m(\\mathbf{x}) = \\beta_0 + \\sum_{i\\in S} \\alpha_i K(\\mathbf{x}, \\mathbf{x}_i)\\] Algunas de las funciones kernel más utilizadas son: Kernel lineal \\[K(\\mathbf{x}, \\mathbf{y}) = \\mathbf{x}^t \\mathbf{y}\\] Kernel polinómico \\[K(\\mathbf{x}, \\mathbf{y}) = (1 + \\gamma \\mathbf{x}^t \\mathbf{y})^d\\] Kernel radial \\[K(\\mathbf{x}, \\mathbf{y}) = \\mbox{exp} (-\\gamma \\| \\mathbf{x} - \\mathbf{y} \\|^2)\\] Tangente hiperbólica \\[K(\\mathbf{x}, \\mathbf{y}) = \\mbox{tanh} (1 + \\gamma \\mathbf{x}^t \\mathbf{y})\\] Antes de construir el modelo, es recomendable centrar y reescalar los datos para evitar que los valores grandes ahoguen al resto de los datos. Por supuesto, tiene que hacerse la misma transformación a todos los datos, incluidos los datos de test. La posibilidad de utilizar distintos kernels da mucha flexibilidad a esta metodología, pero es muy importante seleccionar adecuadamente los parámetros de la función kernel (\\(\\gamma,d\\)) y el parámetro \\(C\\) para evitar sobreajustes. I.3.1 Clasificación con más de dos categorías La metodología support vector machine está específicamente diseñada para clasificar cuando hay exactamente dos categorías. En la literatura se pueden encontrar varias propuestas para extenderla al caso de más de dos categorías, aunque las dos más populares son también las más sencillas. La primera opción consiste en construir tantos modelos como parejas de categorías hay, en un enfoque de uno contra uno. Dada una nueva observación a clasificar, se mira en cada uno de los modelos en que categoría la clasifica. Finalmente se hace un recuento y gana la categoría con más votos. La alternativa es llevar a cabo un enfoque de uno contra todos. Para cada categoría se contruye el modelo que considera esa categoría frente a todas las demás agrupadas como una sola y, para la observación a clasificar, se considera su distancia con la frontera. Se clasifica la observación como perteneciente a la categoría con mayor distancia. I.3.2 Regresión Aunque la metodología SVM está concebida para problemas de clasificación, ha habido varios intentos de adaptar su filosofía a problemas de regresión. En esta sección vamos a comentar muy por encima el enfoque seguido en Drucker et al. (1997), con un fuerte enfoque en la robustez. Recordemos que, en el contexto de la clasificación, el modelo SVM va a depender de unos pocos datos: los vectores soporte. En regresión, si se utiliza RSS como criterio de error, todos los datos van a influir en el modelo y además, al estar los errores al cuadrado, los valores atípicos van a tener mucha influencia, muy superior a la que se tendría si se utilizase, por ejemplo, el valor absoluto. Una alternativa, poco intuitiva pero efectiva, es fijar los hiperparámetros \\(\\epsilon,c &gt; 0\\) como umbral y coste, respectivamente, y definir la función de pérdidas \\[ L_{\\epsilon,c} (x) = \\left\\{ \\begin{array}{ll} 0 &amp; \\mbox{si } |x|&lt; \\epsilon \\\\ (|x| - \\epsilon)c &amp; \\mbox{en otro caso} \\end{array} \\right. \\] En un problema de regresión lineal, SVM estima los parámetros del modelo \\[m(\\mathbf{x}) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p\\] minimizando \\[\\sum_{i=1}^n L_{\\epsilon,c} (y_i - \\hat y_i) + \\sum_{j=1}^p \\beta_j^2\\] Para hacer las cosas aún más confusas, hay autores que utilizan una formulación, equivalente, en la que el parámetro aparece en el segundo sumando como \\(\\lambda = 1/c\\). En la práctica, es habitual fijar el valor de \\(\\epsilon\\) y seleccionar el valor de \\(c\\) (equivalentemente, \\(\\lambda\\)) por validación cruzada, por ejemplo. El modelo puede escribirse en función de los vectores soporte, que son aquellas observaciones cuyo residuo excede el umbral \\(\\epsilon\\): \\[m(\\mathbf{x}) = \\beta_0 + \\sum_{i\\in S} \\alpha_i \\mathbf{x}^t \\mathbf{x}_i\\] Finalmente, utilizando una función kernel, el modelo de regresión SVM es \\[m(\\mathbf{x}) = \\beta_0 + \\sum_{i\\in S} \\alpha_i K(\\mathbf{x}, \\mathbf{x}_i)\\] I.3.3 Ventajas e incovenientes Ventajas: Son muy flexibles (pueden adaptarse a fronteras no lineales complejas), por lo que en muchos casos se obtienen buenas predicciones (en otros pueden producir malos resultados). Al suavizar el margen, utilizando un parámetro de coste \\(C\\), son relativamente robustas frente a valores atípicos. Inconvenientes: Los modelos ajustados son difíciles de interpretar (caja negra), habrá que recurrir a herramientas generales como las descritas en la Sección F.5. Pueden requerir mucho tiempo de computación cuando \\(n &gt;&gt; p\\), ya que hay que estimar (en principio) tantos parámetros como número de observaciones en los datos de entrenamiento, aunque finalmente la mayoría de ellos se anularán (en cualquier caso habría que factorizar la matriz \\(K_{ij} = K(\\mathbf{x}_i, \\mathbf{x}_j)\\) de dimensión \\(n \\times n\\)). Están diseñados para predictores numéricos (emplean distancias), por lo que habrá que realizar un preprocesado de las variables explicativas categóricas (para transformarlas en variables indicadoras). "],["I-4-svm-con-el-paquete-kernlab.html", "I.4 SVM con el paquete kernlab", " I.4 SVM con el paquete kernlab Hay varios paquetes que implementan este procedimiento (e.g. e1071, svmpath, Hastie et al., 2004), aunque se considera que el más completo es kernlab (Karatzoglou et al., 2004). La función principal es ksvm() y se suelen considerar los siguientes argumentos: ksvm(formula, data, scaled = TRUE, type, kernel =&quot;rbfdot&quot;, kpar = &quot;automatic&quot;, C = 1, epsilon = 0.1, prob.model = FALSE, class.weights, cross = 0) formula y data (opcional): permiten especificar la respuesta y las variables predictoras de la forma habitual (e.g. respuesta ~ .; también admite matrices). scaled: vector lógico indicando que predictores serán reescalados; por defecto se reescalan todas las variables no binarias (y se almacenan los valores empleados para ser usados en posteriores predicciones). type (opcional): cadena de texto que permite seleccionar los distintos métodos de clasificación, de regresión o de detección de atípicos implementados (ver ?ksvm); por defecto se establece a partir del tipo de la respuesta: \"C-svc\", clasificación con parámetro de coste, si es un factor y \"eps-svr\", regresión épsilon, si la respuesta es numérica. kernel: función núcleo. Puede ser una función definida por el usuario o una cadena de texto que especifique una de las implementadas en el paquete (ver ?kernels); por defecto \"rbfdot\", kernel radial gausiano. kpar: lista con los hiperparámetros del núcleo. En el caso de \"rbfdot\", además de una lista con un único componente \"sigma\" (inversa de la ventana), puede ser \"automatic\" (valor por defecto) e internamente emplea la función sigest() para seleccionar un valor adecuado. C: (hiper)parámetro \\(C\\) que especifica el coste de la violación de las restricciones; por defecto 1. epsilon: (hiper)parámetro \\(\\epsilon\\) empleado en la función de pérdidas de los métodos de regresión; por defecto 0.1. prob.model: si se establece a TRUE (por defecto es FALSE), se emplean los resultados de la clasificación para ajustar un modelo para estimar las probabilidades (y se podrán calcular con el método predict()). class.weights: vector (con las clases como nombres) con los pesos de una mala clasificación en cada clase. cross: número grupos para validación cruzada; por defecto 0 (no se hace validación cruzada). Si se asigna un valor mayor que 1 se realizará validación cruzada y se devolverá el error en la componente @cross (se puede acceder con la función cross(); y se puede emplear para seleccionar hiperparámetros). Como ejemplo consideraremos el problema de clasificación con los datos de calidad de vino: load(&quot;datos/winetaste.RData&quot;) # Partición de los datos set.seed(1) df &lt;- winetaste nobs &lt;- nrow(df) itrain &lt;- sample(nobs, 0.8 * nobs) train &lt;- df[itrain, ] test &lt;- df[-itrain, ] library(kernlab) set.seed(1) # Para la selección de sigma = mean(sigest(fmedv ~ ., data = train)[-2]) svm &lt;- ksvm(taste ~ ., data = train, kernel = &quot;rbfdot&quot;, prob.model = TRUE) svm ## Support Vector Machine object of class &quot;ksvm&quot; ## ## SV type: C-svc (classification) ## parameter : cost C = 1 ## ## Gaussian Radial Basis kernel function. ## Hyperparameter : sigma = 0.0751133799772488 ## ## Number of Support Vectors : 594 ## ## Objective Function Value : -494.1409 ## Training error : 0.198 ## Probability model included. # plot(svm, data = train) produce un error # packageVersion(&quot;kernlab&quot;) 0.9.29 Podemos evaluar la precisión en la muestra de test empleando el procedimiento habitual: pred &lt;- predict(svm, newdata = test) caret::confusionMatrix(pred, test$taste) ## Confusion Matrix and Statistics ## ## Reference ## Prediction good bad ## good 147 45 ## bad 19 39 ## ## Accuracy : 0.744 ## 95% CI : (0.6852, 0.7969) ## No Information Rate : 0.664 ## P-Value [Acc &gt; NIR] : 0.003886 ## ## Kappa : 0.3788 ## ## Mcnemar&#39;s Test P-Value : 0.001778 ## ## Sensitivity : 0.8855 ## Specificity : 0.4643 ## Pos Pred Value : 0.7656 ## Neg Pred Value : 0.6724 ## Prevalence : 0.6640 ## Detection Rate : 0.5880 ## Detection Prevalence : 0.7680 ## Balanced Accuracy : 0.6749 ## ## &#39;Positive&#39; Class : good ## Para obtener las estimaciones de las probabilidades, habría que establecer type = \"probabilities\" al predecir (devolverá una matriz con columnas correspondientes a los niveles)23: p.est &lt;- predict(svm, newdata = test, type = &quot;probabilities&quot;) head(p.est) ## good bad ## [1,] 0.4761934 0.5238066 ## [2,] 0.7089338 0.2910662 ## [3,] 0.8893454 0.1106546 ## [4,] 0.8424003 0.1575997 ## [5,] 0.6640875 0.3359125 ## [6,] 0.3605543 0.6394457 Este procedimiento está implementado en el método \"svmRadial\" de caret y considera como hiperparámetros: library(caret) # names(getModelInfo(&quot;svm&quot;)) # 17 métodos modelLookup(&quot;svmRadial&quot;) ## model parameter label forReg forClass probModel ## 1 svmRadial sigma Sigma TRUE TRUE TRUE ## 2 svmRadial C Cost TRUE TRUE TRUE En este caso la función train() por defecto evaluará únicamente tres valores del hiperparámetro C = c(0.25, 0.5, 1) y fijará el valor de sigma. Alternativamente podríamos establecer la rejilla de búsqueda, por ejemplo: tuneGrid &lt;- data.frame(sigma = kernelf(svm)@kpar$sigma, # Emplea clases S4 C = c(0.5, 1, 5)) set.seed(1) caret.svm &lt;- train(taste ~ ., data = train, method = &quot;svmRadial&quot;, preProcess = c(&quot;center&quot;, &quot;scale&quot;), trControl = trainControl(method = &quot;cv&quot;, number = 5), tuneGrid = tuneGrid, prob.model = TRUE) caret.svm ## Support Vector Machines with Radial Basis Function Kernel ## ## 1000 samples ## 11 predictor ## 2 classes: &#39;good&#39;, &#39;bad&#39; ## ## Pre-processing: centered (11), scaled (11) ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 800, 801, 800, 800, 799 ## Resampling results across tuning parameters: ## ## C Accuracy Kappa ## 0.5 0.7549524 0.4205204 ## 1.0 0.7599324 0.4297468 ## 5.0 0.7549374 0.4192217 ## ## Tuning parameter &#39;sigma&#39; was held constant at a value of 0.07511338 ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were sigma = 0.07511338 and C = 1. varImp(caret.svm) ## ROC curve variable importance ## ## Importance ## alcohol 100.000 ## density 73.616 ## chlorides 60.766 ## volatile.acidity 57.076 ## total.sulfur.dioxide 45.500 ## fixed.acidity 42.606 ## pH 34.972 ## sulphates 25.546 ## citric.acid 6.777 ## residual.sugar 6.317 ## free.sulfur.dioxide 0.000 confusionMatrix(predict(caret.svm, newdata = test), test$taste) ## Confusion Matrix and Statistics ## ## Reference ## Prediction good bad ## good 147 45 ## bad 19 39 ## ## Accuracy : 0.744 ## 95% CI : (0.6852, 0.7969) ## No Information Rate : 0.664 ## P-Value [Acc &gt; NIR] : 0.003886 ## ## Kappa : 0.3788 ## ## Mcnemar&#39;s Test P-Value : 0.001778 ## ## Sensitivity : 0.8855 ## Specificity : 0.4643 ## Pos Pred Value : 0.7656 ## Neg Pred Value : 0.6724 ## Prevalence : 0.6640 ## Detection Rate : 0.5880 ## Detection Prevalence : 0.7680 ## Balanced Accuracy : 0.6749 ## ## &#39;Positive&#39; Class : good ## Otras opciones son \"votes\" y \"decision\" para obtener matrices con el número de votos o los valores de \\(m(\\mathbf{x})\\). "],["J-class-otros.html", "J Otros métodos de clasificación", " J Otros métodos de clasificación En los métodos de clasificación que hemos visto en los capítulos anteriores, uno de los objetivos era estimar la probabilidad a posteriori \\(P(Y = k | \\mathbf{X}=\\mathbf{x})\\) de que la observación \\(\\mathbf{x}\\) pertenezca a la categoría \\(k\\), pero en ningún caso nos preocupábamos por la distribución de las variables predictoras. En la terminología de ML estos métodos se conocen con el nombre de discriminadores (discriminative methods). Otro ejemplo de método discriminador es la regresión logística. En este capítulo vamos a ver métodos que reciben el nombre genérico de métodos generadores (generative methods). Se caracterizan porque calculan las probabilidades a posteriori utilizando la distribución conjunta de \\((\\mathbf{X}, Y)\\) y el teorema de Bayes: \\[P(Y = k | \\mathbf{X}=\\mathbf{x}) = \\frac{P(Y = k) f_k(\\mathbf{x})}{\\sum_{l=1}^K P(Y = l) f_l(\\mathbf{x})}\\] donde \\(f_k(\\mathbf{x})\\) es la función de densidad del vector aleatorio \\(\\mathbf{X}=(X_1, X_2, \\ldots, X_p)\\) para una observación perteneciente a la clase \\(k\\), es decir, es una forma abreviada de escribir \\(f(\\mathbf{X}=\\mathbf{x} | Y = k)\\). En la jerga bayesiana a esta función se la conoce como verosimilitud (es la función de verosimilitud sin más que considerar que la observación muestral \\(\\mathbf{x}\\) es fija y la variable es \\(k\\)) y resumen la fórmula anterior como \\[posterior \\propto prior \\times verosimilitud\\] Una vez estimadas las probabilidades a priori \\(P(Y = k)\\) y las densidades (verosimilitudes) \\(f_k(\\mathbf{x})\\), tenemos las probabilidades a posteriori. Para estimar las funciones de densidad se puede utilizar un método paramétrico o un método no paramétrico. En el primer caso, lo más habitual es modelizar la distribución del vector de variables predictoras como normales multivariantes. A continuación vamos a ver tres casos particulares de este enfoque, siempre suponiendo normalidad. "],["J-1-análisis-discriminate-lineal.html", "J.1 Análisis discriminate lineal", " J.1 Análisis discriminate lineal El análisis lineal discrimintante (LDA) se inicia en Fisher (1936) pero es Welch (1939) quien lo enfoca utilizando el teorema de Bayes. Asumiendo que \\(X | Y = k \\sim N(\\mu_k, \\Sigma)\\), es decir, que todas las categorías comparten la misma matriz \\(\\Sigma\\), se obtienen las funciones discriminantes, lineales en \\(\\mathbf{x}\\), \\[\\mathbf{x}^t \\Sigma^{-1} \\mu_k - \\frac{1}{2} \\mu_k^t \\Sigma^{-1} \\mu_k + \\mbox{log}(P(Y = k)) \\] La dificultad técnica del método LDA reside en el cálculo de \\(\\Sigma^{-1}\\). Cuando hay más variables predictoras que datos, o cuando las variables predictoras están fuertemente correlacionadas, hay un problema. Una solución pasa por aplicar análisis de componentes principales (PCA) para reducir la dimensión y tener predictores incorrelados antes de utilizar LDA. Aunque la solución anterior se utiliza mucho, hay que tener en cuenta que la reducción de la dimensión se lleva a cabo sin tener en cuenta la información de las categorías, es decir, la estructura de los datos en categorías. Una alternativa consiste en utilizar partial least squares discriminant analysis (PLSDA, Berntsson y Wold, 1986). La idea consiste en realizar una regresión PLS siendo las categorías la respuesta, con el objetivo de reducir la dimensión a la vez que se maximiza la correlación con las respuestas. Una generalización de LDA es el mixture discriminant analysis (Hastie y Tibshirani, 1996) en el que, siempre con la misma matriz \\(\\Sigma\\), se contempla la posibilidad de que dentro de cada categoría haya múltiples subcategorías que unicamente difieren en la media. Las distribuciones dentro de cada clase se agregan mediante una mixtura de las distribuciones multivariantes. J.1.1 Ejemplo MASS::lda load(&quot;datos/winetaste.RData&quot;) # Partición de los datos set.seed(1) df &lt;- winetaste nobs &lt;- nrow(df) itrain &lt;- sample(nobs, 0.8 * nobs) train &lt;- df[itrain, ] test &lt;- df[-itrain, ] library(MASS) ld &lt;- lda(taste ~ ., data = train) ld ## Call: ## lda(taste ~ ., data = train) ## ## Prior probabilities of groups: ## good bad ## 0.662 0.338 ## ## Group means: ## fixed.acidity volatile.acidity citric.acid residual.sugar chlorides ## good 6.726888 0.2616994 0.3330211 6.162009 0.04420242 ## bad 7.030030 0.3075148 0.3251775 6.709024 0.05075740 ## free.sulfur.dioxide total.sulfur.dioxide density pH sulphates ## good 34.75831 132.7568 0.9935342 3.209668 0.4999396 ## bad 35.41124 147.4615 0.9950789 3.166331 0.4763905 ## alcohol ## good 10.786959 ## bad 9.845611 ## ## Coefficients of linear discriminants: ## LD1 ## fixed.acidity -4.577255e-02 ## volatile.acidity 5.698858e+00 ## citric.acid -5.894231e-01 ## residual.sugar -2.838910e-01 ## chlorides -6.083210e+00 ## free.sulfur.dioxide 1.039366e-03 ## total.sulfur.dioxide -8.952115e-04 ## density 5.642314e+02 ## pH -2.103922e+00 ## sulphates -2.400004e+00 ## alcohol -1.996112e-01 plot(ld) ld.pred &lt;- predict(ld, newdata = test) pred &lt;- ld.pred$class caret::confusionMatrix(pred, test$taste) ## Confusion Matrix and Statistics ## ## Reference ## Prediction good bad ## good 146 49 ## bad 20 35 ## ## Accuracy : 0.724 ## 95% CI : (0.6641, 0.7785) ## No Information Rate : 0.664 ## P-Value [Acc &gt; NIR] : 0.0247239 ## ## Kappa : 0.3238 ## ## Mcnemar&#39;s Test P-Value : 0.0007495 ## ## Sensitivity : 0.8795 ## Specificity : 0.4167 ## Pos Pred Value : 0.7487 ## Neg Pred Value : 0.6364 ## Prevalence : 0.6640 ## Detection Rate : 0.5840 ## Detection Prevalence : 0.7800 ## Balanced Accuracy : 0.6481 ## ## &#39;Positive&#39; Class : good ## p.est &lt;-ld.pred$posterior "],["J-2-análisis-discriminante-cuadrático.html", "J.2 Análisis discriminante cuadrático", " J.2 Análisis discriminante cuadrático El análisis discriminante cuadrático (QDA) relaja la suposición de que todas las categorías tengan la misma estructura de covarianzas, es decir, \\(X | Y = k \\sim N(\\mu_k, \\Sigma_k)\\), obteniendo como solución \\[-\\frac{1}{2} (\\mathbf{x} - \\mu_k)^t \\Sigma^{-1} (\\mathbf{x} - \\mu_k) - \\frac{1}{2} \\mbox{log}(|\\Sigma_k|) + \\mbox{log}(P(Y = k)) \\] Vemos que este método da lugar a fronteras discriminantes cuadráticas. Si el número de variables predictoras es próximo al tamaño muestral, en la prácticas QDA se vuelve impracticable, ya que el número de variables predictoras tiene que ser menor que el numero de datos en cada una de las categorías. Una recomendación básica es utilizar LDA y QDA únicamente cuando hay muchos más datos que predictores. Y al igual que en LDA, si dentro de las clases los predictores presentan mucha colinealidad el modelo va a funcionar mal. Al ser QDA una generalización de LDA podemos pensar que siempre va a ser preferible, pero eso no es cierto, ya que QDA requiere estimar muchos más parámetros que LDA y por tanto tiene más riesgo de sobreajustar. Al ser menos flexible, LDA da lugar a modelos más simples: menos varianza pero más sesgo. LDA suele funcionar mejor que QDA cuando hay pocos datos y es por tanto muy importante reducir la varianza. Por el contrario, QDA es recomendable cuando hay muchos datos. Una solución intermedia entre LDA y QDA es el análisis discriminante regularizado (RDA, Friedman, 1989), que utiliza el hiperparámetro \\(\\lambda\\) para definir la matriz \\[\\Sigma_{k,\\lambda}&#39; = \\lambda\\Sigma_k + (1 - \\lambda) \\Sigma \\] También hay una versión con dos hiperparámetros, \\(\\lambda\\) y \\(\\gamma\\), \\[\\Sigma_{k,\\lambda,\\gamma}&#39; = (1 - \\gamma) \\Sigma_{k,\\lambda}&#39; + \\gamma \\frac{1}{p} \\mbox{tr} (\\Sigma_{k,\\lambda}&#39;)I \\] J.2.1 Ejemplo MASS::qda qd &lt;- qda(taste ~ ., data = train) qd ## Call: ## qda(taste ~ ., data = train) ## ## Prior probabilities of groups: ## good bad ## 0.662 0.338 ## ## Group means: ## fixed.acidity volatile.acidity citric.acid residual.sugar chlorides ## good 6.726888 0.2616994 0.3330211 6.162009 0.04420242 ## bad 7.030030 0.3075148 0.3251775 6.709024 0.05075740 ## free.sulfur.dioxide total.sulfur.dioxide density pH sulphates ## good 34.75831 132.7568 0.9935342 3.209668 0.4999396 ## bad 35.41124 147.4615 0.9950789 3.166331 0.4763905 ## alcohol ## good 10.786959 ## bad 9.845611 qd.pred &lt;- predict(qd, newdata = test) pred &lt;- qd.pred$class caret::confusionMatrix(pred, test$taste) ## Confusion Matrix and Statistics ## ## Reference ## Prediction good bad ## good 147 40 ## bad 19 44 ## ## Accuracy : 0.764 ## 95% CI : (0.7064, 0.8152) ## No Information Rate : 0.664 ## P-Value [Acc &gt; NIR] : 0.0003762 ## ## Kappa : 0.4363 ## ## Mcnemar&#39;s Test P-Value : 0.0092202 ## ## Sensitivity : 0.8855 ## Specificity : 0.5238 ## Pos Pred Value : 0.7861 ## Neg Pred Value : 0.6984 ## Prevalence : 0.6640 ## Detection Rate : 0.5880 ## Detection Prevalence : 0.7480 ## Balanced Accuracy : 0.7047 ## ## &#39;Positive&#39; Class : good ## p.est &lt;-qd.pred$posterior "],["J-3-naive-bayes.html", "J.3 Naive Bayes", " J.3 Naive Bayes El modelo naive Bayes simplifica los modelos anteriores asumiendo que las variables predictoras son independientes. Esta es una suposición extremadamente fuerte y en la práctica difícilmente nos encontraremos con un problema en el que las variables sean independientes, pero a cambio se va a reducir mucho la complejidad del modelo. Esta simplicidad del modelo le va a permitir manejar un gran número de predictores, incluso con un tamaño muestral moderado, situaciones en las que puede ser imposible utilizar LDA o QDA. Otra ventaja asociada con su simplicidad es que el cálculo del modelo se va a poder hacer muy rápido incluso para tamaños muestrales muy grandes. Además, y quizás esto sea lo más sorprendente, en ocasiones su rendimiento es muy competitivo. Asumiendo normalidad, este modelo no es más que un caso particular de QDA con matrices \\(\\Sigma_k\\) diagonales. Cuando las variables predictoras son categóricas, lo más habitual es modelizar naive Bayes utilizando distribuciones multinomiales. J.3.1 Ejemplo e1071::naiveBayes library(e1071) nb &lt;- naiveBayes(taste ~ ., data = train) nb ## ## Naive Bayes Classifier for Discrete Predictors ## ## Call: ## naiveBayes.default(x = X, y = Y, laplace = laplace) ## ## A-priori probabilities: ## Y ## good bad ## 0.662 0.338 ## ## Conditional probabilities: ## fixed.acidity ## Y [,1] [,2] ## good 6.726888 0.8175101 ## bad 7.030030 0.9164467 ## ## volatile.acidity ## Y [,1] [,2] ## good 0.2616994 0.08586935 ## bad 0.3075148 0.11015113 ## ## citric.acid ## Y [,1] [,2] ## good 0.3330211 0.1231345 ## bad 0.3251775 0.1334682 ## ## residual.sugar ## Y [,1] [,2] ## good 6.162009 4.945483 ## bad 6.709024 5.251402 ## ## chlorides ## Y [,1] [,2] ## good 0.04420242 0.02237654 ## bad 0.05075740 0.03001672 ## ## free.sulfur.dioxide ## Y [,1] [,2] ## good 34.75831 14.87336 ## bad 35.41124 19.26304 ## ## total.sulfur.dioxide ## Y [,1] [,2] ## good 132.7568 38.05871 ## bad 147.4615 47.34668 ## ## density ## Y [,1] [,2] ## good 0.9935342 0.00285949 ## bad 0.9950789 0.00256194 ## ## pH ## Y [,1] [,2] ## good 3.209668 0.1604529 ## bad 3.166331 0.1472261 ## ## sulphates ## Y [,1] [,2] ## good 0.4999396 0.11564067 ## bad 0.4763905 0.09623778 ## ## alcohol ## Y [,1] [,2] ## good 10.786959 1.2298425 ## bad 9.845611 0.8710844 pred &lt;- predict(nb, newdata = test) caret::confusionMatrix(pred, test$taste) ## Confusion Matrix and Statistics ## ## Reference ## Prediction good bad ## good 136 47 ## bad 30 37 ## ## Accuracy : 0.692 ## 95% CI : (0.6307, 0.7486) ## No Information Rate : 0.664 ## P-Value [Acc &gt; NIR] : 0.19255 ## ## Kappa : 0.2734 ## ## Mcnemar&#39;s Test P-Value : 0.06825 ## ## Sensitivity : 0.8193 ## Specificity : 0.4405 ## Pos Pred Value : 0.7432 ## Neg Pred Value : 0.5522 ## Prevalence : 0.6640 ## Detection Rate : 0.5440 ## Detection Prevalence : 0.7320 ## Balanced Accuracy : 0.6299 ## ## &#39;Positive&#39; Class : good ## p.est &lt;- predict(nb, newdata = test, type = &quot;raw&quot;) "],["K-modelos-lineales.html", "K Modelos lineales y extensiones", " K Modelos lineales y extensiones "],["L-reg-np.html", "L Regresión no paramétrica", " L Regresión no paramétrica Se trata de métodos que no suponen ninguna forma concreta de la media condicional (i.e. no se hacen suposiciones paramétricas sobre el efecto de las variables explicativas): \\[Y=m\\left( X_1, \\ldots, X_p \\right) + \\varepsilon\\] siendo \\(m\\) una función cualquiera (se asume que es una función suave de los predictores). La idea detrás de la mayoría de estos métodos es ajustar localmente un modelo de regresión (este capítulo se podría haber titulado modelos locales). Suponiendo que disponemos de suficiente información en un entorno de la posición de predicción (el número de observaciones debe ser relativamente grande), podríamos pensar en predecir la respuesta a partir de lo que ocurre en las observaciones cercanas. Nos centraremos principalmente en el caso de regresión, pero la mayoría de estos métodos se pueden extender para el caso de clasificación (por ejemplo considerando una función de enlace y realizando el ajuste localmente por máxima verosimilitud). Los métodos de regresión basados en: árboles de decisión, bosques aleatorios, bagging, boosting y máquinas de soporte vectorial, vistos en capítulos anteriores, entrarían también dentro de esta clasificación. "],["L-1-reg-local.html", "L.1 Regresión local", " L.1 Regresión local En este tipo de métodos se incluirían: vecinos más próximos, regresión tipo núcleo y loess (o lowess). También se podrían incluir los splines de regresión (regression splines), pero se tratarán en la siguiente sección, ya que también se pueden ver como una extensión de un modelo lineal global. Con muchos de estos procedimientos no se obtiene una expresión cerrada del modelo ajustado y (en principio) es necesario disponer de la muestra de entrenamiento para calcular predicciones, por lo que en AE también se denominan métodos basados en memoria. L.1.1 Vecinos más próximos Uno de los métodos más conocidos de regresión local es el denominado k-vecinos más cercanos (k-nearest neighbors; KNN), que ya se empleó como ejemplo en la Sección F.4 (la maldición de la dimensionalidad). Se trata de un método muy simple, pero que en la práctica puede ser efectivo en muchas ocasiones. Se basa en la idea de que localmente la media condicional (la predicción óptima) es constante. Concretamente, dados un entero \\(k\\) (hiperparámetro) y un conjunto de entrenamiento \\(\\mathcal{T}\\), para obtener la predicción correspondiente a un vector de valores de las variables explicativas \\(\\mathbf{x}\\), el método de regresión KNN promedia las observaciones en un vecindario \\(\\mathcal{N}_k(\\mathbf{x}, \\mathcal{T})\\) formado por las \\(k\\) observaciones más cercanas a \\(\\mathbf{x}\\): \\[\\hat{Y}(\\mathbf{x}) = \\hat{m}(\\mathbf{x}) = \\frac{1}{k} \\sum_{i \\in \\mathcal{N}_k(\\mathbf{x}, \\mathcal{T})} Y_i\\] Se puede emplear la misma idea en el caso de clasificación, las frecuencias relativas en el vecindario serían las estimaciones de las probabilidades de las clases (lo que sería equivalente a considerar las variables indicadoras de las categorías) y normalmente la predicción sería la moda (la clase más probable). Para seleccionar el vecindario es necesario especificar una distancia, por ejemplo: \\[d(\\mathbf{x}_0, \\mathbf{x}_i) = \\left( \\sum_{j=1}^p \\left| x_{j0} - x_{ji} \\right|^d \\right)^{\\frac{1}{d}}\\] Normalmente se considera la distancia euclídea (\\(d=2\\)) o la de Manhatan (\\(d=1\\)) si los predictores son muméricos (también habría distancias diseñadas para predictores categóricos). En cualquier caso la recomendación es estandarizar previamente los predictores para que no influya su escala en el cálculo de las distancias. Como ya se mostró en al final del Capítulo F, este método está implementado en la función knnreg() (Sección F.4) y en el método \"knn\" del paquete caret (Sección F.6). Como ejemplo adicional emplearemos el conjunto de datos MASS::mcycle que contiene mediciones de la aceleración de la cabeza en una simulación de un accidente de motocicleta, utilizado para probar cascos protectores (considerando el conjunto de datos completo como si fuese la muestra de entrenamiento). data(mcycle, package = &quot;MASS&quot;) library(caret) # Ajuste de los modelos fit1 &lt;- knnreg(accel ~ times, data = mcycle, k = 5) # 5 observaciones más cercanas (5% de los datos) fit2 &lt;- knnreg(accel ~ times, data = mcycle, k = 10) fit3 &lt;- knnreg(accel ~ times, data = mcycle, k = 20) plot(accel ~ times, data = mcycle, col = &#39;darkgray&#39;) newx &lt;- seq(1 , 60, len = 200) newdata &lt;- data.frame(times = newx) lines(newx, predict(fit1, newdata), lty = 3) lines(newx, predict(fit2, newdata), lty = 2) lines(newx, predict(fit3, newdata)) legend(&quot;topright&quot;, legend = c(&quot;5-NN&quot;, &quot;10-NN&quot;, &quot;20-NN&quot;), lty = c(3, 2, 1), lwd = 1) Figura L.1: Predicciones con el método KNN y distintos vecindarios El hiperparámetro \\(k\\) (número de vecinos más cercanos) determina la complejidad del modelo, de forma que valores más pequeños de \\(k\\) se corresponden con modelos más complejos (en el caso extremo \\(k = 1\\) se interpolarían las observaciones). Este parámetro se puede seleccionar empleando alguno de los métodos descritos en la Sección F.3.3 (por ejemplo mediante validación con k grupos como se mostró en la Sección F.6). L.1.2 Regresión polinómica local En el caso univariante, para cada \\(x_0\\) se ajusta un polinomio de grado \\(d\\): \\[\\beta_0+\\beta_{1}\\left(x - x_0\\right) + \\cdots + \\beta_{d}\\left( x-x_0\\right)^{d}\\] por mínimos cuadrados ponderados, con pesos \\[w_{i} = K_h(x - x_0) = \\frac{1}{h}K\\left(\\frac{x-x_0}{h}\\right)\\] donde \\(K\\) es una función núcleo (normalmente una densidad simétrica en torno al cero) y \\(h&gt;0\\) es un parámetro de suavizado, llamado ventana, que regula el tamaño del entorno que se usa para llevar a cabo el ajuste (esta ventana también se puede suponer local, \\(h \\equiv h(x_0)\\); por ejemplo el método KNN se puede considerar un caso particular, con \\(d=0\\) y \\(K\\) la densidad de una \\(\\mathcal{U}(-1, 1)\\)). A partir de este ajuste24: La estimación en \\(x_0\\) es \\(\\hat{m}_{h}(x_0)=\\hat{\\beta}_0\\). Podemos obtener también estimaciones de las derivadas: \\(\\widehat{m_{h}^{(r)}}(x_0) = r!\\hat{\\beta}_{r}\\). Por tanto, la estimación polinómica local de grado \\(d\\), \\(\\hat{m}_{h}(x)=\\hat{\\beta}_0\\), se obtiene al minimizar: \\[\\min_{\\beta_0 ,\\beta_1, \\ldots, \\beta_d} \\sum_{i=1}^{n}\\left\\{ Y_{i} - \\beta_0 - \\beta_1(x - X_i) - \\ldots -\\beta_d(x - X_i)^d \\right\\}^{2} K_{h}(x - X_i)\\] Explícitamente: \\[\\hat{m}_{h}(x) = \\mathbf{e}_{1}^{t} \\left( X_{x}^{t} {W}_{x} X_{x} \\right)^{-1} X_{x}^{t} {W}_{x}\\mathbf{Y} \\equiv {s}_{x}^{t}\\mathbf{Y}\\] donde \\(\\mathbf{e}_{1} = \\left( 1, \\cdots, 0\\right)^{t}\\), \\(X_{x}\\) es la matriz con \\((1,x - X_i, \\ldots, (x - X_i)^d)\\) en la fila \\(i\\), \\(W_{x} = \\mathtt{diag} \\left( K_{h}(x_{1} - x), \\ldots, K_{h}(x_{n} - x) \\right)\\) es la matriz de pesos, e \\(\\mathbf{Y} = \\left( Y_1, \\cdots, Y_n\\right)^{t}\\) es el vector de observaciones de la respuesta. Se puede pensar que se obtiene aplicando un suavizado polinómico a \\((X_i, Y_i)\\): \\[\\hat{\\mathbf{Y}} = S\\mathbf{Y}\\] siendo \\(S\\) la matriz de suavizado con \\(\\mathbf{s}_{X_{i}}^{t}\\) en la fila \\(i\\) (este tipo de métodos también se denominan suavizadores lineales). Habitualmente se considera: \\(d=0\\): Estimador Nadaraya-Watson. \\(d=1\\): Estimador lineal local. Desde el punto de vista asintótico ambos estimadores tienen un comportamiento similar25, pero en la práctica suele ser preferible el estimador lineal local, sobre todo porque se ve menos afectado por el denominado efecto frontera (Sección F.4). Aunque el paquete base de R incluye herramientas para la estimación tipo núcleo de la regresión (ksmooth(), loess()), recomiendan el uso del paquete KernSmooth (Wand y Ripley, 2020). La ventana \\(h\\) es el (hiper)parámetro de mayor importancia en la predicción y para seleccionarlo se suelen emplear métodos de validación cruzada (Sección F.3.3) o tipo plug-in (reemplazando las funciones desconocidas que aparecen en la expresión de la ventana asintóticamente óptima por estimaciones; e.g. función dpill() del paquete KernSmooth). Por ejemplo, usando el criterio de validación cruzada dejando uno fuera (LOOCV) se trataría de minimizar: \\[CV(h)=\\frac{1}{n}\\sum_{i=1}^n(y_i-\\hat{m}_{-i}(x_i))^2\\] siendo \\(\\hat{m}_{-i}(x_i)\\) la predicción obtenida eliminando la observación \\(i\\)-ésima. Al igual que en el caso de regresión lineal, este error también se puede obtener a partir del ajuste con todos los datos: \\[CV(h)=\\frac{1}{n}\\sum_{i=1}^n\\left(\\frac{y_i-\\hat{m}(x_i)}{1 - S_{ii}}\\right)^2\\] siendo \\(S_{ii}\\) el elemento \\(i\\)-ésimo de la diagonal de la matriz de suavizado (esto en general es cierto para cualquier suavizador lineal). Alternativamente se podría emplear validación cruzada generalizada (Craven y Wahba, 1979): \\[GCV(h)=\\frac{1}{n}\\sum_{i=1}^n\\left(\\frac{y_i-\\hat{m}(x_i)}{1 - \\frac{1}{n}tr(S)}\\right)^2\\] (sustituyendo \\(S_{ii}\\) por su promedio). Además, la traza de la matriz de suavizado \\(tr(S)\\) es lo que se conoce como el número efectivo de parámetros (\\(n - tr(S)\\) sería una aproximación de los grados de libertad del error). Continuando con el ejemplo del conjunto de datos MASS::mcycle emplearemos la función locpoly() del paquete KernSmooth para obtener estimaciones lineales locales26 con una venta seleccionada mediante un método plug-in: # data(mcycle, package = &quot;MASS&quot;) x &lt;- mcycle$times y &lt;- mcycle$accel library(KernSmooth) h &lt;- dpill(x, y) # Método plug-in de Ruppert, Sheather y Wand (1995) fit &lt;- locpoly(x, y, bandwidth = h) # Estimación lineal local plot(x, y, col = &#39;darkgray&#39;) lines(fit) Hay que tener en cuenta que el paquete KernSmooth no implementa los métodos predict() y residuals(): pred &lt;- approx(fit, xout = x)$y # pred &lt;- predict(fit) resid &lt;- y - pred # resid &lt;- residuals(fit) Tampoco calcula medidas de bondad de ajuste, aunque podríamos calcular medidas de la precisión de las predicciones de la forma habitual (en este caso de la muestra de entrenamiento): accuracy &lt;- function(pred, obs, na.rm = FALSE, tol = sqrt(.Machine$double.eps)) { err &lt;- obs - pred # Errores if(na.rm) { is.a &lt;- !is.na(err) err &lt;- err[is.a] obs &lt;- obs[is.a] } perr &lt;- 100*err/pmax(obs, tol) # Errores porcentuales return(c( me = mean(err), # Error medio rmse = sqrt(mean(err^2)), # Raíz del error cuadrático medio mae = mean(abs(err)), # Error absoluto medio mpe = mean(perr), # Error porcentual medio mape = mean(abs(perr)), # Error porcentual absoluto medio r.squared = 1 - sum(err^2)/sum((obs - mean(obs))^2) # Pseudo R-cuadrado )) } accuracy(pred, y) ## me rmse mae mpe mape ## -2.712378e-01 2.140005e+01 1.565921e+01 -2.460832e+10 7.559223e+10 ## r.squared ## 8.023864e-01 El caso multivariante es análogo, aunque habría que considerar una matriz de ventanas simétrica \\(H\\). También hay extensiones para el caso de predictores categóricos (nominales o ordinales) y para el caso de distribuciones de la respuesta distintas de la normal (máxima verosimilitud local). Otros paquetes de R incluyen más funcionalidades (sm, locfit, npsp), pero hoy en día el paquete np es el que se podría considerar más completo. L.1.3 Regresión polinómica local robusta También hay versiones robustas del ajuste polinómico local tipo núcleo. Estos métodos surgieron en el caso bivariante (\\(p=1\\)), por lo que también se denominan suavizado de diagramas de dispersión (scatterplot smoothing; e.g. función lowess(), locally weighted scatterplot smoothing, del paquete base). Posteriormente se extendieron al caso multivariante (e.g. función loess()). Son métodos muy empleados en análisis descriptivo (no supervisado) y normalmente se emplean ventanas locales tipo vecinos más cercanos (por ejemplo a través de un parámetro spam que determina la proporción de observaciones empleadas en el ajuste). Como ejemplo emplearemos la función loess() con ajuste robusto (habrá que establecer family = \"symmetric\" para emplear M-estimadores, por defecto con 4 iteraciones, en lugar de mínimos cuadrados ponderados), seleccionando previamente spam por validación cruzada (LOOCV) pero empleando como criterio de error la mediana de los errores en valor absoluto (median absolute deviation, MAD)27. cv.loess &lt;- function(formula, datos, span, ...) { n &lt;- nrow(datos) cv.pred &lt;- numeric(n) for (i in 1:n) { modelo &lt;- loess(formula, datos[-i, ], span = span, control = loess.control(surface = &quot;direct&quot;), ...) # control = loess.control(surface = &quot;direct&quot;) permite extrapolaciones cv.pred[i] &lt;- predict(modelo, newdata = datos[i, ]) } return(cv.pred) } ventanas &lt;- seq(0.1, 0.5, len = 10) np &lt;- length(ventanas) cv.error &lt;- numeric(np) for(p in 1:np){ cv.pred &lt;- cv.loess(accel ~ times, mcycle, ventanas[p], family = &quot;symmetric&quot;) # cv.error[p] &lt;- mean((cv.pred - mcycle$accel)^2) cv.error[p] &lt;- median(abs(cv.pred - mcycle$accel)) } plot(ventanas, cv.error) imin &lt;- which.min(cv.error) span.cv &lt;- ventanas[imin] points(span.cv, cv.error[imin], pch = 16) # Ajuste con todos los datos plot(accel ~ times, data = mcycle, col = &#39;darkgray&#39;) fit &lt;- loess(accel ~ times, mcycle, span = span.cv, family = &quot;symmetric&quot;) lines(mcycle$times, predict(fit)) Se puede pensar que se están estimando los coeficientes de un desarrollo de Taylor de \\(m(x_0)\\). Asintóticamente el estimador lineal local tiene un sesgo menor que el de Nadaraya-Watson (pero del mismo orden) y la misma varianza (e.g. Fan and Gijbels, 1996). La función KernSmooth::locpoly() también admite la estimación de derivadas. En este caso habría dependencia entre las observaciones y los criterios habituales como validación cruzada tenderán a seleccionar ventanas pequeñas, i.e. a infrasuavizar. "],["L-2-splines.html", "L.2 Splines", " L.2 Splines Otra alternativa consiste en trocear los datos en intervalos, fijando unos puntos de corte \\(z_i\\) (denominados nudos; knots), con \\(i = 1, \\ldots, k\\), y ajustar un polinomio en cada segmento (lo que se conoce como regresión segmentada, piecewise regression). De esta forma sin embargo habrá discontinuidades en los puntos de corte, pero podrían añadirse restricciones adicionales de continuidad (o incluso de diferenciabilidad) para evitarlo (e.g. paquete segmented). L.2.1 Regression splines Cuando en cada intervalo se ajustan polinomios de orden \\(d\\) y se incluyen restricciones de forma que las derivadas sean continuas hasta el orden \\(d-1\\) se obtienen los denominados splines de regresión (regression splines). Puede verse que este tipo de ajustes equivalen a transformar la variable predictora \\(X\\), considerando por ejemplo la base de potencias truncadas (truncated power basis): \\[1, x, \\ldots, x^d, (x-z_1)_+^d,\\ldots,(x-z_k)_+^d\\] siendo \\((x - z)_+ = \\max(0, x - z)\\), y posteriormente realizar un ajuste lineal: \\[m(x) = \\beta_0 + \\beta_1 b_1(x) + \\beta_2 b_2(x) + \\ldots + \\beta_{k+d} b_{k+d}(x)\\] Típicamente se seleccionan polinomios de grado \\(d=3\\), lo que se conoce como splines cúbicos, y nodos equiespaciados. Además, se podrían emplear otras bases equivalentes. Por ejemplo, para evitar posibles problemas computacionales con la base anterior, se suele emplear la denominada base \\(B\\)-spline (de Boor, 1978; implementada en la función bs() del paquete splines). nknots &lt;- 9 # nodos internos; 10 intervalos knots &lt;- seq(min(x), max(x), len = nknots + 2)[-c(1, nknots + 2)] # knots &lt;- quantile(x, 1:nknots/(nknots + 1)) # bs(x, df = nknots + degree + intercept) library(splines) fit1 &lt;- lm(y ~ bs(x, knots = knots, degree = 1)) fit2 &lt;- lm(y ~ bs(x, knots = knots, degree = 2)) fit3 &lt;- lm(y ~ bs(x, knots = knots)) # degree = 3 plot(x, y, col = &#39;darkgray&#39;) newx &lt;- seq(min(x), max(x), len = 200) newdata &lt;- data.frame(x = newx) lines(newx, predict(fit1, newdata), lty = 3) lines(newx, predict(fit2, newdata), lty = 2) lines(newx, predict(fit3, newdata)) abline(v = knots, lty = 3, col = &#39;darkgray&#39;) legend(&quot;topright&quot;, legend = c(&quot;d=1 (df=11)&quot;, &quot;d=2 (df=12)&quot;, &quot;d=3 (df=13)&quot;), lty = c(3, 2, 1)) El grado del polinomio, pero sobre todo el número de nodos, determinarán la flexibilidad del modelo. Se podrían considerar el número de parámetros en el ajuste lineal, los grados de libertad, como medida de la complejidad (en la función bs() se puede especificar df en lugar de knots, y estos se generarán a partir de los cuantiles de x). Como ya se comentó, al aumentar el grado del modelo polinómico se incrementa la variabilidad de las predicciones, especialmente en la frontera. Para tratar de evitar este problema se suelen emplear los splines naturales, que son splines de regresión con restricciones adicionales de forma que el ajuste sea lineal en los intervalos extremos (lo que en general produce estimaciones más estables en la frontera y mejores extrapolaciones). Estas restricciones reducen la complejidad (los grados de libertad del modelo), y al igual que en el caso de considerar únicamente las restricciones de continuidad y diferenciabilidad, resultan equivalentes a considerar una nueva base en un ajuste sin restricciones. Por ejemplo, se puede emplear la función splines::ns() para ajustar un spline natural (cúbico por defecto): plot(x, y, col = &#39;darkgray&#39;) fit4 &lt;- lm(y ~ ns(x, knots = knots)) lines(newx, predict(fit4, newdata)) lines(newx, predict(fit3, newdata), lty = 2) abline(v = knots, lty = 3, col = &#39;darkgray&#39;) legend(&quot;topright&quot;, legend = c(&quot;ns (d=3, df=11)&quot;, &quot;bs (d=3, df=13)&quot;), lty = c(1, 2)) La dificultad está en la selección de los nodos \\(z_i\\). Si se consideran equiespaciados (o se emplea otro criterio como los cuantiles), se podría seleccionar su número (equivalentemente los grados de libertad) empleando algún método de validación cruzada. Sin embargo, sería preferible considerar más nodos donde aparentemente hay más variaciones en la función de regresión y menos donde es más estable, esta es la idea de la regresión spline adaptativa descrita en la Sección L.4. Otra alternativa son los splines penalizados, descritos al final de esta sección. L.2.2 Smoothing splines Los splines de suavizado (smoothing splines) se obtienen como la función \\(s(x)\\) suave (dos veces diferenciable) que minimiza la suma de cuadrados residual más una penalización que mide su rugosidad: \\[\\sum_{i=1}^{n} (y_i - s(x_i))^2 + \\lambda \\int s^{\\prime\\prime}(x)^2 dx\\] siendo \\(0 \\leq \\lambda &lt; \\infty\\) el (hiper)parámetro de suavizado. Puede verse que la solución a este problema, en el caso univariante, es un spline natural cúbico con nodos en \\(x_1, \\ldots, x_n\\) y restricciones en los coeficientes determinadas por el valor de \\(\\lambda\\) (es una versión regularizada de un spline natural cúbico). Por ejemplo si \\(\\lambda = 0\\) se interpolarán las observaciones y cuando \\(\\lambda \\rightarrow \\infty\\) el ajuste tenderá a una recta (con segunda derivada nula). En el caso multivariante \\(p&gt; 1\\) la solución da lugar a los denominados thin plate splines28. Al igual que en el caso de la regresión polinómica local (Sección L.1.2), estos métodos son suavizadores lineales: \\[\\hat{\\mathbf{Y}} = S_{\\lambda}\\mathbf{Y}\\] y para seleccionar el parámetro de suavizado \\(\\lambda\\) podemos emplear los criterios de validación cruzada (dejando uno fuera), minimizando: \\[CV(\\lambda)=\\frac{1}{n}\\sum_{i=1}^n\\left(\\frac{y_i-\\hat{s}_{\\lambda}(x_i)}{1 - \\{ S_{\\lambda}\\}_{ii}}\\right)^2\\] siendo \\(\\{ S_{\\lambda}\\}_{ii}\\) el elemento \\(i\\)-ésimo de la diagonal de la matriz de suavizado, o validación cruzada generalizada (GCV), minimizando: \\[GCV(\\lambda)=\\frac{1}{n}\\sum_{i=1}^n\\left(\\frac{y_i-\\hat{s}_{\\lambda}(x_i)}{1 - \\frac{1}{n}tr(S_{\\lambda})}\\right)^2\\] Análogamente, el número efectivo de parámetros o grados de libertad29 \\(df_{\\lambda}=tr(S_{\\lambda})\\) sería una medida de la complejidad del modelo equivalente a \\(\\lambda\\) (muchas implementaciones permiten seleccionar la complejidad empleando \\(df\\)). Este método de suavizado está implementado en la función smooth.spline() del paquete base y por defecto emplea GCV para seleccionar el parámetro de suavizado (aunque también admite CV y se puede especificar lambda o df)30. sspline.gcv &lt;- smooth.spline(x, y) sspline.cv &lt;- smooth.spline(x, y, cv = TRUE) plot(x, y, col = &#39;darkgray&#39;) lines(sspline.gcv) lines(sspline.cv, lty = 2) Cuando el número de observaciones es muy grande, y por tanto el número de nodos, pueden aparecer problemas computacionales al emplear estos métodos. L.2.3 Splines penalizados Los splines penalizados (penalized splines) combinan las dos aproximaciones anteriores. Incluyen una penalización (que depende de la base considerada) y el número de nodos puede ser mucho menor que el número de observaciones (son un tipo de low-rank smoothers). De esta forma se obtienen modelos spline con mejores propiedades, con un menor efecto frontera y en los que se evitan problemas en la selección de los nodos. Unos de los más empleados son los \\(P\\)-splines (Eilers and Marx, 1996) que emplean una base \\(B\\)-spline con una penalización simple (basada en los cuadrados de diferencias de coeficientes consecutivos \\((\\beta_{i+1} - \\beta_i)^2\\)). Además, un modelo spline penalizado se puede representar como un modelo lineal mixto, lo que permite emplear herramientas desarrolladas para este tipo de modelos (por ejemplo la implementadas en el paquete nlme, del que depende mgcv, que por defecto emplea splines penalizados). Para más detalles ver por ejemplo las secciones 5.2 y 5.3 de Wood (2017). Están relacionados con las funciones radiales. También hay versiones con un número reducido de nodos denominados low-rank thin plate regression splines empleados en el paquete mgcv. Esto también permitiría generalizar los criterios AIC o BIC. Además de predicciones, el correspondiente método predict() también permite obtener estimaciones de las derivadas. "],["L-3-modelos-aditivos-1.html", "L.3 Modelos aditivos", " L.3 Modelos aditivos Se supone que: \\[Y= \\beta_{0} + f_1(X_1) + f_2(X_2) + \\ldots + f_p(X_p) + \\varepsilon\\] con \\(f_{i},\\) \\(i=1,...,p,\\) funciones cualesquiera. De esta forma se consigue mucha mayor flexibilidad que con los modelos lineales pero manteniendo la interpretabilidad de los efectos de los predictores. Adicionalmente se puede considerar una función de enlace, obteniéndose los denominados modelos aditivos generalizados (GAM). Para más detalles sobre este tipo modelos ver por ejemplo Hastie y Tibshirani (1990) o Wood (2017). Los modelos lineales (generalizados) serían un caso particular considerando \\(f_{i}(x) = \\beta_{i}x\\). Además, se podrían considerar cualquiera de los métodos de suavizado descritos anteriormente para construir las componentes no paramétricas (por ejemplo si se emplean splines naturales de regresión el ajuste se reduciría al de un modelo lineal). Se podrían considerar distintas aproximaciones para el modelado de cada componente (modelos semiparamétricos) y realizar el ajuste mediante backfitting (se ajusta cada componente de forma iterativa, empleando los residuos obtenidos al mantener las demás fijas). Si en las componentes no paramétricas se emplea únicamente splines de regresión (con o sin penalización), se puede reformular el modelo como un GLM (regularizado si hay penalización) y ajustarlo fácilmente adaptando herramientas disponibles (penalized re-weighted iterative least squares, PIRLS). De entre todos los paquetes de R que implementan estos modelos destacan: gam: Admite splines de suavizado (univariantes, s()) y regresión polinómica local (multivariante, lo()), pero no dispone de un método para la selección automática de los parámetros de suavizado (se podría emplear un criterio por pasos para la selección de componentes). Sigue la referencia: Hastie, T.J. y Tibshirani, R.J. (1990). Generalized Additive Models. Chapman &amp; Hall. mgcv: Admite una gran variedad de splines de regresión y splines penalizados (s(); por defecto emplea thin plate regression splines penalizados multivariantes), con la opción de selección automática de los parámetros de suavizado mediante distintos criterios. Además de que se podría emplear un método por pasos, permite la selección de componentes mediante regularización. Al ser más completo que el anterior sería el recomendado en la mayoría de los casos (ver ?mgcv::mgcv.package para una introducción al paquete). Sigue la referencia: Wood, S.N. (2017). Generalized Additive Models: An Introduction with R. Chapman &amp; Hall/CRC L.3.1 Ajuste: función gam La función gam() del paquete mgcv permite ajustar modelos aditivos generalizados empleando suavizado mediante splines: library(mgcv) ajuste &lt;- gam(formula, family = gaussian, data, method = &quot;GCV.Cp&quot;, select = FALSE, ...) (también dispone de la función bam() para el ajuste de estos modelos a grandes conjuntos de datos y de la función gamm() para el ajuste de modelos aditivos generalizados mixtos). El modelo se establece a partir de la formula empleando s() para especificar las componentes suaves (ver help(s) y Sección L.3.5). Algunas posibilidades de uso son las que siguen: Modelo lineal: ajuste &lt;- gam(y ~ x1 + x2 + x3) Modelo (semiparamétrico) aditivo con efectos no paramétricos para x1 y x2, y un efecto lineal para x3: ajuste &lt;- gam(y ~ s(x1) + s(x2) + x3) Modelo no aditivo (con interacción): ajuste &lt;- gam(y ~ s(x1, x2)) Modelo (semiparamétrico) con distintas combinaciones : ajuste &lt;- gam(y ~ s(x1, x2) + s(x3) + x4) L.3.2 Ejemplo En esta sección utilizaremos como ejemplo el conjunto de datos Prestige de la librería carData. Se tratará de explicar prestige (puntuación de ocupaciones obtenidas a partir de una encuesta) a partir de income (media de ingresos en la ocupación) y education (media de los años de educación). data(Prestige, package = &quot;carData&quot;) library(mgcv) modelo &lt;- gam(prestige ~ s(income) + s(education), data = Prestige) summary(modelo) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## prestige ~ s(income) + s(education) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 46.8333 0.6889 67.98 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(income) 3.118 3.877 14.61 &lt;2e-16 *** ## s(education) 3.177 3.952 38.78 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.836 Deviance explained = 84.7% ## GCV = 52.143 Scale est. = 48.414 n = 102 # coef(modelo) # El resultado es un modelo lineal en transformaciones de los predictores En este caso el método plot() representa los efectos (parciales) estimados de cada predictor: par.old &lt;- par(mfrow = c(1, 2)) plot(modelo, shade = TRUE) # par(par.old) En general se representa cada componente no paramétrica (salvo que se especifique all.terms = TRUE), incluyendo gráficos de contorno para el caso de componentes bivariantes (correspondientes a interacciones entre predictores). Se dispone también de un método predict() para calcular las predicciones de la forma habitual (por defecto devuelve las correspondientes a las observaciones modelo$fitted.values y para nuevos datos hay que emplear el argumento newdata). L.3.3 Superficies de predicción En el caso bivariante, para representar las estimaciones (la superficie de predicción) obtenidas con el modelo se pueden utilizar las funciones persp() o versiones mejoradas como plot3D::persp3D. Estas funciones requieren que los valores de entrada estén dispuestos en una rejilla bidimensional. Para generar esta rejilla se puede emplear la función expand.grid(x,y) que crea todas las combinaciones de los puntos dados en x e y. inc &lt;- with(Prestige, seq(min(income), max(income), len = 25)) ed &lt;- with(Prestige, seq(min(education), max(education), len = 25)) newdata &lt;- expand.grid(income = inc, education = ed) # Representamos la rejilla plot(income ~ education, Prestige, pch = 16) abline(h = inc, v = ed, col = &quot;grey&quot;) # Se calculan las predicciones pred &lt;- predict(modelo, newdata) # Se representan pred &lt;- matrix(pred, nrow = 25) # persp(inc, ed, pred, theta = -40, phi = 30) plot3D::persp3D(inc, ed, pred, theta = -40, phi = 30, ticktype = &quot;detailed&quot;, xlab = &quot;Income&quot;, ylab = &quot;Education&quot;, zlab = &quot;Prestige&quot;) Alternativamente se podrían emplear las funciones contour(), filled.contour(), plot3D::image2D o similares: # contour(inc, ed, pred, xlab = &quot;Income&quot;, ylab = &quot;Education&quot;) filled.contour(inc, ed, pred, xlab = &quot;Income&quot;, ylab = &quot;Education&quot;, key.title = title(&quot;Prestige&quot;)) Puede ser más cómodo emplear el paquete modelr (emplea gráficos ggplot2) para trabajar con modelos y predicciones. L.3.4 Comparación y selección de modelos Además de las medidas de bondad de ajuste como el coeficiente de determinación ajustado, también se puede emplear la función anova para la comparación de modelos (y seleccionar las componentes por pasos de forma interactiva). Por ejemplo, viendo el gráfico de los efectos se podría pensar que el efecto de education podría ser lineal: # plot(modelo) modelo0 &lt;- gam(prestige ~ s(income) + education, data = Prestige) summary(modelo0) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## prestige ~ s(income) + education ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.2240 3.7323 1.132 0.261 ## education 3.9681 0.3412 11.630 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(income) 3.58 4.441 13.6 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.825 Deviance explained = 83.3% ## GCV = 54.798 Scale est. = 51.8 n = 102 anova(modelo0, modelo, test=&quot;F&quot;) ## Analysis of Deviance Table ## ## Model 1: prestige ~ s(income) + education ## Model 2: prestige ~ s(income) + s(education) ## Resid. Df Resid. Dev Df Deviance F Pr(&gt;F) ## 1 95.559 4994.6 ## 2 93.171 4585.0 2.3886 409.58 3.5418 0.0257 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 En este caso aceptaríamos que el modelo original es significativamente mejor. Alternativamente, podríamos pensar que hay interacción: modelo2 &lt;- gam(prestige ~ s(income, education), data = Prestige) summary(modelo2) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## prestige ~ s(income, education) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 46.8333 0.7138 65.61 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(income,education) 4.94 6.303 75.41 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.824 Deviance explained = 83.3% ## GCV = 55.188 Scale est. = 51.974 n = 102 # plot(modelo2, se = FALSE) # plot(modelo2, scheme = 2) En este caso el coeficiente de determinación ajustado es menor y ya no tendría sentido realizar el contraste. Ademas se pueden seleccionar componentes del modelo (mediante regularización) empleando el parámetro select = TRUE. example(gam.selection) ## ## gm.slc&gt; ## an example of automatic model selection via null space penalization ## gm.slc&gt; library(mgcv) ## ## gm.slc&gt; set.seed(3);n&lt;-200 ## ## gm.slc&gt; dat &lt;- gamSim(1,n=n,scale=.15,dist=&quot;poisson&quot;) ## simulate data ## Gu &amp; Wahba 4 term additive model ## ## gm.slc&gt; dat$x4 &lt;- runif(n, 0, 1);dat$x5 &lt;- runif(n, 0, 1) ## spurious ## ## gm.slc&gt; b&lt;-gam(y~s(x0)+s(x1)+s(x2)+s(x3)+s(x4)+s(x5),data=dat, ## gm.slc+ family=poisson,select=TRUE,method=&quot;REML&quot;) ## ## gm.slc&gt; summary(b) ## ## Family: poisson ## Link function: log ## ## Formula: ## y ~ s(x0) + s(x1) + s(x2) + s(x3) + s(x4) + s(x5) ## ## Parametric coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.21758 0.04082 29.83 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df Chi.sq p-value ## s(x0) 1.7655088 9 5.264 0.0392 * ## s(x1) 1.9271040 9 65.356 &lt;2e-16 *** ## s(x2) 6.1351414 9 156.204 &lt;2e-16 *** ## s(x3) 0.0002849 9 0.000 0.4181 ## s(x4) 0.0003044 9 0.000 0.9703 ## s(x5) 0.1756926 9 0.195 0.3018 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.545 Deviance explained = 51.6% ## -REML = 430.78 Scale est. = 1 n = 200 ## ## gm.slc&gt; plot(b,pages=1) L.3.5 Diagnosis del modelo La función gam.check() realiza una diagnosis del modelo: gam.check(modelo) ## ## Method: GCV Optimizer: magic ## Smoothing parameter selection converged after 4 iterations. ## The RMS GCV score gradient at convergence was 9.783945e-05 . ## The Hessian was positive definite. ## Model rank = 19 / 19 ## ## Basis dimension (k) checking results. Low p-value (k-index&lt;1) may ## indicate that k is too low, especially if edf is close to k&#39;. ## ## k&#39; edf k-index p-value ## s(income) 9.00 3.12 0.98 0.42 ## s(education) 9.00 3.18 1.03 0.54 Lo ideal sería observar normalidad en los dos gráficos de la izquierda, falta de patrón en el superior derecho, y ajuste a una recta en el inferior derecho. En este caso parece que el modelo se comporta adecuadamente. Como se deduce del resultado anterior, podría ser recomendable modificar la dimensión k de la base utilizada construir la componente no paramétrica, este valor se puede interpretar como el grado máximo de libertad permitido en ese componente, aunque normalmente no influye demasiado en el resultado (puede influir en el tiempo de computación). También se podría chequear concurvidad (concurvity; generalización de la multicolinealidad) entre las componentes del modelo: concurvity(modelo) ## para s(income) s(education) ## worst 3.107241e-23 0.5931528 0.5931528 ## observed 3.107241e-23 0.4065402 0.4398639 ## estimate 3.107241e-23 0.3613674 0.4052251 Esta función devuelve tres medidas por componente, que tratan de medir la proporción de variación de esa componente que está contenida en el resto (similares al complementario de la tolerancia; un valor próximo a 1 indicaría que puede haber problemas de concurvidad). L.3.6 GAM en caret El soporte de GAM en caret es como poco deficiente library(caret) names(getModelInfo(&quot;gam&quot;)) # 4 métodos ## [1] &quot;gam&quot; &quot;gamboost&quot; &quot;gamLoess&quot; &quot;gamSpline&quot; modelLookup(&quot;gam&quot;) ## model parameter label forReg forClass probModel ## 1 gam select Feature Selection TRUE TRUE TRUE ## 2 gam method Method TRUE TRUE TRUE modelLookup(&quot;gamLoess&quot;) ## model parameter label forReg forClass probModel ## 1 gamLoess span Span TRUE TRUE TRUE ## 2 gamLoess degree Degree TRUE TRUE TRUE L.3.7 Ejercicios Continuando con los datos de MASS:mcycle, emplear mgcv::gam() para ajustar un spline penalizado para predecir accel a partir de times con las opciones por defecto y representar el ajuste obtenido. Comparar el ajuste con el obtenido empleando un spline penalizado adaptativo (bs=\"ad\"; ver ?adaptive.smooth). Empleando el conjunto de datos airquality, crear una muestra de entrenamiento y otra de test, buscar un modelo aditivo que resulte adecuado para explicar sqrt(Ozone) a partir de Temp, Wind y Solar.R. Es preferible suponer que hay una interacción entre Temp y Wind? "],["L-4-mars.html", "L.4 Regresión spline adaptativa multivariante", " L.4 Regresión spline adaptativa multivariante La regresión spline adaptativa multivariante, en inglés multivariate adaptive regression splines (MARS; Friedman, 1991), es un procedimiento adaptativo para problemas de regresión que puede verse como una generalización tanto de la regresión lineal por pasos (stepwise linear regression) como de los árboles de decisión CART. El modelo MARS es un spline multivariante lineal: \\[m(\\mathbf{x}) = \\beta_0 + \\sum_{m=1}^M \\beta_m h_m(\\mathbf{x})\\] (es un modelo lineal en transformaciones \\(h_m(\\mathbf{x})\\) de los predictores originales), donde las bases \\(h_m(\\mathbf{x})\\) se construyen de forma adaptativa empleando funciones bisagra (hinge functions) \\[ h(x) = (x)_+ = \\left\\{ \\begin{array}{ll} x &amp; \\mbox{si } x &gt; 0 \\\\ 0 &amp; \\mbox{si } x \\leq 0 \\end{array} \\right.\\] y considerando como posibles nodos los valores observados de los predictores (en el caso univariante se emplean las bases de potencias truncadas con \\(d=1\\) descritas en la Sección L.2.1, pero incluyendo también su versión simetrizada). Vamos a empezar explicando el modelo MARS aditivo (sin interacciones), que funciona de forma muy parecida a los árboles de decisión CART, y después lo extenderemos al caso con interacciones. Asumimos que todas las variables predictoras son numéricas. El proceso de construcción del modelo es un proceso iterativo hacia delante (forward) que empieza con el modelo \\[\\hat m(\\mathbf{x}) = \\hat \\beta_0 \\] donde \\(\\hat \\beta_0\\) es la media de todas las respuestas, para a continuación considerar todos los puntos de corte (knots) posibles \\(x_{ji}\\) con \\(i = 1, 2, \\ldots, n\\), \\(j = 1, 2, \\ldots, p\\), es decir, todas las observaciones de todas las variables predictoras de la muestra de entrenamiento. Para cada punto de corte \\(x_{ji}\\) (combinación de variable y observación) se consideran dos bases: \\[h_1(\\mathbf{x}) = h(x_j - x_{ji}) \\\\ h_2(\\mathbf{x}) = h(x_{ji} - x_j)\\] y se construye el nuevo modelo \\[\\hat m(\\mathbf{x}) = \\hat \\beta_0 + \\hat \\beta_1 h_1(\\mathbf{x}) + \\hat \\beta_2 h_2(\\mathbf{x})\\] La estimación de los parámetros \\(\\beta_0, \\beta_1, \\beta_2\\) se realiza de la forma estándar en regresión lineal, minimizando \\(\\mbox{RSS}\\). De este modo se construyen muchos modelos alternativos y entre ellos se selecciona aquel que tenga un menor error de entrenamiento. En la siguiente iteración se conservan \\(h_1(\\mathbf{x})\\) y \\(h_2(\\mathbf{x})\\) y se añade una pareja de términos nuevos siguiendo el mismo procedimiento. Y así sucesivamente, añadiendo de cada vez dos nuevos términos. Este procedimiento va creando un modelo lineal segmentado (piecewise) donde cada nuevo término modeliza una porción aislada de los datos originales. El tamaño de cada modelo es el número términos (funciones \\(h_m\\)) que este incorpora. El proceso iterativo se para cuando se alcanza un modelo de tamaño \\(M\\), que se consigue después de incorporar \\(M/2\\) cortes. Este modelo depende de \\(M+1\\) parámetros \\(\\beta_m\\) con \\(m=0,1,\\ldots,M\\). El objetivo es alcanzar un modelo lo suficientemente grande para que sobreajuste los datos, para a continuación proceder a su poda en un proceso de eliminación de variables hacia atrás (backward deletion) en el que se van eliminando las variables de una en una (no por parejas, como en la construcción). En cada paso de poda se elimina el término que produce el menor incremento en el error. Así, para cada tamaño \\(\\lambda = 0,1,\\ldots, M\\) se obtiene el mejor modelo estimado \\(\\hat{m}_{\\lambda}\\). La selección óptima del valor del hiperparámetro \\(\\lambda\\) puede realizarse por los procedimientos habituales tipo validación cruzada. Una alternativa mucho más rápida es utilizar validación cruzada generalizada (GCV) que es una aproximación de la validación cruzada leave-one-out mediante la fórmula \\[\\mbox{GCV} (\\lambda) = \\frac{\\mbox{RSS}}{(1-M(\\lambda)/n)^2}\\] donde \\(M(\\lambda)\\) es el número de parámetros efectivos del modelo, que depende del número de términos más el número de puntos de corte utilizados penalizado por un factor (2 en el caso aditivo que estamos explicando, 3 cuando hay interacciones). Hemos explicado una caso particular de MARS: el modelo aditivo. El modelo general sólo se diferencia del caso aditivo en que se permiten interacciones, es decir, multiplicaciones entre las variables \\(h_m(\\mathbf{x})\\). Para ello, en cada iteración durante la fase de construcción del modelo, además de considerar todos los puntos de corte, también se consideran todas las combinaciones con los términos incorporados previamente al modelo, denominados términos padre. De este modo, si resulta seleccionado un término padre \\(h_l(\\mathbf{x})\\) (incluyendo \\(h_0(\\mathbf{x}) = 1\\)) y un punto de corte \\(x_{ji}\\), después de analizar todas las posibilidades, al modelo anterior se le agrega \\[\\hat \\beta_{m+1} h_l(\\mathbf{x}) h(x_j - x_{ji}) + \\hat \\beta_{m+2} h_l(\\mathbf{x}) h(x_{ji} - x_j)\\] Recordando que en cada caso se vuelven a estimar todos los parámetros \\(\\beta_i\\). Al igual que \\(\\lambda\\), también el grado de interacción máxima permitida se considera un hiperparámetro del problema, aunque lo habitual es trabajar con grado 1 (modelo aditivo) o interacción de grado 2. Una restricción adicional que se impone al modelo es que en cada producto no puede aparecer más de una vez la misma variable \\(X_j\\). Aunque el procedimiento de construcción del modelo realiza búsquedas exhaustivas y en consecuencia puede parecer computacionalmente intratable, en la práctica se realiza de forma razonablemente rápida, al igual que ocurría en CART. Una de las principales ventajas de MARS es que realiza una selección automática de las variables predictoras. Aunque inicialmente pueda haber muchos predictores, y este método es adecuado para problemas de alta dimensión, en el modelo final van a aparecer muchos menos (pueden aparecer más de una vez). Además, si se utiliza un modelo aditivo su interpretación es directa, e incluso permitiendo interacciones de grado 2 el modelo puede ser interpretado. Otra ventaja es que no es necesario realizar un prepocesado de los datos, ni filtrando variables ni transformando los datos. Que haya predictores con correlaciones altas no va a afectar a la construcción del modelo (normalmente seleccionará el primero), aunque sí puede dificultar su interpretación. Aunque hemos supuesto al principio de la explicación que los predictores son numéricos, se pueden incorporar variables predictoras cualitativas siguiendo los procedimientos estándar. Por último, se puede realizar una cuantificación de la importancia de las variables de forma similar a como se hace en CART. En conclusión, MARS utiliza splines lineales con una selección automática de los puntos de corte mediante un algoritmo avaricioso similar al empleado en los árboles CART, tratando de añadir más puntos de corte donde aparentemente hay más variaciones en la función de regresión y menos puntos donde esta es más estable. L.4.1 MARS con el paquete earth Actualmente el paquete de referencia para MARS es earth (Enhanced Adaptive Regression Through Hinges)31. La función principal es earth() y se suelen considerar los siguientes argumentos: earth(formula, data, glm = NULL, degree = 1, ...) formula y data (opcional): permiten especificar la respuesta y las variables predictoras de la forma habitual (e.g. respuesta ~ .; también admite matrices). Admite respuestas multidimensionales (ajustará un modelo para cada componente) y categóricas (las convierte en multivariantes), también predictores categóricos, aunque no permite datos faltantes. glm: lista con los parámetros del ajuste GLM (e.g. glm = list(family = binomial)). degree: grado máximo de interacción; por defecto 1 (modelo aditivo). Otros parámetros que pueden ser de interés (afectan a la complejidad del modelo en el crecimiento, a la selección del modelo final o al tiempo de computación; para más detalles ver help(earth)): nk: número máximo de términos en el crecimiento del modelo (dimensión \\(M\\) de la base); por defecto min(200, max(20, 2 * ncol(x))) + 1 (puede ser demasiado pequeña si muchos de los predictores influyen en la respuesta). thresh: umbral de parada en el crecimiento (se interpretaría como cp en los árboles CART); por defecto 0.001 (si se establece a 0 la única condición de parada será alcanzar el valor máximo de términos nk). fast.k: número máximo de términos padre considerados en cada paso durante el crecimiento; por defecto 20, si se establece a 0 no habrá limitación. linpreds: índice de variables que se considerarán con efecto lineal. nprune: número máximo de términos (incluida la intersección) en el modelo final (después de la poda); por defecto no hay límite (se podrían incluir todos los creados durante el crecimiento). pmethod: método empleado para la poda; por defecto \"backward\". Otras opciones son: \"forward\", \"seqrep\", \"exhaustive\" (emplea los métodos de selección implementados en paquete leaps), \"cv\" (validación cruzada, empleando nflod) y \"none\" para no realizar poda. nfold: número de grupos de validación cruzada; por defecto 0 (no se hace validación cruzada). varmod.method: permite seleccionar un método para estimar las varianzas y por ejemplo poder realizar contrastes o construir intervalos de confianza (para más detalles ver ?varmod o la vignette Variance models in earth). Utilizaremos como ejemplo inicial los datos de MASS:mcycle: # data(mcycle, package = &quot;MASS&quot;) library(earth) mars &lt;- earth(accel ~ times, data = mcycle) # mars summary(mars) ## Call: earth(formula=accel~times, data=mcycle) ## ## coefficients ## (Intercept) -90.992956 ## h(19.4-times) 8.072585 ## h(times-19.4) 9.249999 ## h(times-31.2) -10.236495 ## ## Selected 4 of 6 terms, and 1 of 1 predictors ## Termination condition: RSq changed by less than 0.001 at 6 terms ## Importance: times ## Number of terms at each degree of interaction: 1 3 (additive model) ## GCV 1119.813 RSS 133670.3 GRSq 0.5240328 RSq 0.5663192 plot(mars) plot(accel ~ times, data = mcycle, col = &#39;darkgray&#39;) lines(mcycle$times, predict(mars)) Como con las opciones por defecto el ajuste no es muy bueno (aunque podría ser suficiente), podríamos forzar la complejidad del modelo en el crecimiento (minspan = 1 permite que todas las observaciones sean potenciales nodos): mars2 &lt;- earth(accel ~ times, data = mcycle, minspan = 1, thresh = 0) summary(mars2) ## Call: earth(formula=accel~times, data=mcycle, minspan=1, thresh=0) ## ## coefficients ## (Intercept) -6.274366 ## h(times-14.6) -25.333056 ## h(times-19.2) 32.979264 ## h(times-25.4) 153.699248 ## h(times-25.6) -145.747392 ## h(times-32) -30.041076 ## h(times-35.2) 13.723887 ## ## Selected 7 of 12 terms, and 1 of 1 predictors ## Termination condition: Reached nk 21 ## Importance: times ## Number of terms at each degree of interaction: 1 6 (additive model) ## GCV 623.5209 RSS 67509.03 GRSq 0.7349776 RSq 0.7809732 plot(accel ~ times, data = mcycle, col = &#39;darkgray&#39;) lines(mcycle$times, predict(mars2)) Como siguiente ejemplo consideramos los datos de carData::Prestige: # data(Prestige, package = &quot;carData&quot;) mars &lt;- earth(prestige ~ education + income + women, data = Prestige, degree = 2, nk = 40) summary(mars) ## Call: earth(formula=prestige~education+income+women, data=Prestige, degree=2, ## nk=40) ## ## coefficients ## (Intercept) 19.9845240 ## h(education-9.93) 5.7683265 ## h(income-3161) 0.0085297 ## h(income-5795) -0.0080222 ## h(women-33.57) 0.2154367 ## h(income-5299) * h(women-4.14) -0.0005163 ## h(income-5795) * h(women-4.28) 0.0005409 ## ## Selected 7 of 31 terms, and 3 of 3 predictors ## Termination condition: Reached nk 40 ## Importance: education, income, women ## Number of terms at each degree of interaction: 1 4 2 ## GCV 53.08737 RSS 3849.355 GRSq 0.8224057 RSq 0.8712393 plot(mars) Para representar los efectos de las variables importa las herramientas del paquete plotmo (del mismo autor; válido también para la mayoría de los modelos tratados en este libro, incluyendo mgcv::gam()). plotmo(mars) ## plotmo grid: education income women ## 10.54 5930 13.6 Podríamos obtener la importancia de las variables: varimp &lt;- evimp(mars) varimp ## nsubsets gcv rss ## education 6 100.0 100.0 ## income 5 36.0 40.3 ## women 3 16.3 22.0 plot(varimp) Siempre podríamos considerar este modelo de partida para seleccionar componentes de un modelo GAM más flexible: # library(mgcv) gam &lt;- gam(prestige ~ s(education) + s(income) + s(women), data = Prestige, select = TRUE) summary(gam) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## prestige ~ s(education) + s(income) + s(women) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 46.8333 0.6461 72.49 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(education) 2.349 9 9.926 &lt; 2e-16 *** ## s(income) 6.289 9 7.420 &lt; 2e-16 *** ## s(women) 1.964 9 1.309 0.00149 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.856 Deviance explained = 87.1% ## GCV = 48.046 Scale est. = 42.58 n = 102 gam2 &lt;- gam(prestige ~ s(education) + s(income, women), data = Prestige) summary(gam2) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## prestige ~ s(education) + s(income, women) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 46.833 0.679 68.97 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(education) 2.802 3.489 25.09 &lt;2e-16 *** ## s(income,women) 4.895 6.286 10.03 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.841 Deviance explained = 85.3% ## GCV = 51.416 Scale est. = 47.032 n = 102 anova(gam, gam2, test = &quot;F&quot;) ## Analysis of Deviance Table ## ## Model 1: prestige ~ s(education) + s(income) + s(women) ## Model 2: prestige ~ s(education) + s(income, women) ## Resid. Df Resid. Dev Df Deviance F Pr(&gt;F) ## 1 88.325 3849.1 ## 2 91.225 4388.3 -2.9001 -539.16 4.3661 0.00705 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 plotmo(gam2) ## plotmo grid: education income women ## 10.54 5930 13.6 plot(gam2, scheme = 2, select = 2) Pregunta: ¿Observas algo extraño en el contraste ANOVA anterior? L.4.2 MARS con el paquete caret Emplearemos como ejemplo el conjunto de datos earth::Ozone1: # data(ozone1, package = &quot;earth&quot;) df &lt;- ozone1 set.seed(1) nobs &lt;- nrow(df) itrain &lt;- sample(nobs, 0.8 * nobs) train &lt;- df[itrain, ] test &lt;- df[-itrain, ] caret implementa varios métodos basados en earth: library(caret) # names(getModelInfo(&quot;[Ee]arth&quot;)) # 4 métodos modelLookup(&quot;earth&quot;) ## model parameter label forReg forClass probModel ## 1 earth nprune #Terms TRUE TRUE TRUE ## 2 earth degree Product Degree TRUE TRUE TRUE Consideramos una rejilla de búsqueda personalizada: tuneGrid &lt;- expand.grid(degree = 1:2, nprune = floor(seq(2, 20, len = 10))) set.seed(1) caret.mars &lt;- train(O3 ~ ., data = train, method = &quot;earth&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 10), tuneGrid = tuneGrid) caret.mars ## Multivariate Adaptive Regression Spline ## ## 264 samples ## 9 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 238, 238, 238, 236, 237, 239, ... ## Resampling results across tuning parameters: ## ## degree nprune RMSE Rsquared MAE ## 1 2 4.842924 0.6366661 3.803870 ## 1 4 4.558953 0.6834467 3.488040 ## 1 6 4.345781 0.7142046 3.413213 ## 1 8 4.256592 0.7295113 3.220256 ## 1 10 4.158604 0.7436812 3.181941 ## 1 12 4.128416 0.7509562 3.142176 ## 1 14 4.069714 0.7600561 3.061458 ## 1 16 4.058769 0.7609245 3.058843 ## 1 18 4.058769 0.7609245 3.058843 ## 1 20 4.058769 0.7609245 3.058843 ## 2 2 4.842924 0.6366661 3.803870 ## 2 4 4.652783 0.6725979 3.540031 ## 2 6 4.462122 0.7039134 3.394627 ## 2 8 4.188539 0.7358147 3.209399 ## 2 10 3.953353 0.7658754 2.988747 ## 2 12 4.028546 0.7587781 3.040408 ## 2 14 4.084860 0.7514781 3.076990 ## 2 16 4.091340 0.7510666 3.081559 ## 2 18 4.091340 0.7510666 3.081559 ## 2 20 4.091340 0.7510666 3.081559 ## ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were nprune = 10 and degree = 2. ggplot(caret.mars, highlight = TRUE) Podemos analizar el modelo final con las herramientas de earth: summary(caret.mars$finalModel) ## Call: earth(x=matrix[264,9], y=c(4,13,16,3,6,2...), keepxy=TRUE, degree=2, ## nprune=10) ## ## coefficients ## (Intercept) 11.6481994 ## h(dpg-15) -0.0743900 ## h(ibt-110) 0.1224848 ## h(17-vis) -0.3363332 ## h(vis-17) -0.0110360 ## h(101-doy) -0.1041604 ## h(doy-101) -0.0236813 ## h(wind-3) * h(1046-ibh) -0.0023406 ## h(humidity-52) * h(15-dpg) -0.0047940 ## h(60-humidity) * h(ibt-110) -0.0027632 ## ## Selected 10 of 21 terms, and 7 of 9 predictors (nprune=10) ## Termination condition: Reached nk 21 ## Importance: humidity, ibt, dpg, doy, wind, ibh, vis, temp-unused, ... ## Number of terms at each degree of interaction: 1 6 3 ## GCV 13.84161 RSS 3032.585 GRSq 0.7846289 RSq 0.8199031 # plotmo(caret.mars$finalModel, caption = &#39;ozone$O3 (caret &quot;earth&quot; method)&#39;) plotmo(caret.mars$finalModel, degree2 = 0, caption = &#39;ozone$O3 (efectos principales)&#39;) ## plotmo grid: vh wind humidity temp ibh dpg ibt vis doy ## 5770 5 64.5 62 2046.5 24 169.5 100 213.5 plotmo(caret.mars$finalModel, degree1 = 0, caption = &#39;ozone$O3 (interacciones)&#39;) Finalmente medimos la precisión con el procedimiento habitual: pred &lt;- predict(caret.mars, newdata = test) accuracy(pred, test$O3) ## me rmse mae mpe mape r.squared ## 0.4817913 4.0952444 3.0764376 -14.1288949 41.2602037 0.7408061 Desarrollado a partir de la función mda::mars() de T. Hastie y R. Tibshirani. Utiliza este nombre porque MARS está registrado para un uso comercial por Salford Systems. "],["L-5-projection-pursuit.html", "L.5 Projection pursuit", " L.5 Projection pursuit Projection pursuit (Friedman y Tukey, 1974) es una técnica de análisis exploratorio de datos multivariantes que busca proyecciones lineales de los datos en espacios de dimensión baja, siguiendo una idea originalmente propuesta en Kruskal (1969). Inicialmente se presentó como una técnica gráfica y por ese motivo buscaba proyecciones de dimensión 1 o 2 (proyecciones en rectas o planos), resultando que las direcciones interesantes son aquellas con distribución no normal. La motivación es que cuando se realizan transformaciones lineales lo habitual es que el resultado tenga la apariencia de una distribución normal (por el teorema central del límite), lo cual oculta las singularidades de los datos originales. Se supone que los datos son una trasformación lineal de componentes no gaussianas (variables latentes) y la idea es deshacer esta transformación mediante la optimización de una función objetivo, que en este contexto recibe el nombre de projection index. Aunque con orígenes distintos, projection pursuit es muy similar a independent component analysis (Comon, 1994), una técnica de reducción de la dimensión que, en lugar de buscar como es habitual componentes incorreladas (ortogonales), busca componentes independientes y con distribución no normal (ver por ejemplo la documentación del paquete fastICA). Hay extensiones de projection pursuit para regresión, clasificación, estimación de la función de densidad, etc. L.5.1 Regresión por projection pursuit En el método original de projection pursuit regression (PPR; Friedman y Stuetzle, 1981) se considera el siguiente modelo semiparamétrico \\[m(\\mathbf{x}) = \\sum_{m=1}^M g_m (\\alpha_{1m}x_1 + \\alpha_{2m}x_2 + \\ldots + \\alpha_{pm}x_p)\\] siendo \\(\\boldsymbol{\\alpha}_m = (\\alpha_{1m}, \\alpha_{2m}, \\ldots, \\alpha_{pm})\\) vectores de parámetros (desconocidos) de módulo unitario y \\(g_m\\) funciones suaves (desconocidas), denominadas funciones ridge. Con esta aproximación se obtiene un modelo muy general que evita los problemas de la maldición de la dimensionalidad. De hecho se trata de un aproximador universal, con \\(M\\) suficientemente grande y eligiendo adecuadamente las componentes se podría aproximar cualquier función continua. Sin embargo el modelo resultante puede ser muy difícil de interpretar, salvo el caso de \\(M=1\\) que se corresponde con el denominado single index model empleado habitualmente en Econometría, pero que solo es algo más general que el modelo de regresión lineal múltiple. El ajuste se este tipo de modelos es en principio un problema muy complejo. Hay que estimar las funciones univariantes \\(g_m\\) (utilizando un método de suavizado) y los parámetros \\(\\alpha_{im}\\), utilizando como criterio de error \\(\\mbox{RSS}\\). En la práctica se resuelve utilizando un proceso iterativo en el que se van fijando sucesivamente los valores de los parámetros y las funciones ridge (si son estimadas empleando un método que también proporcione estimaciones de su derivada, las actualizaciones de los parámetros se pueden obtener por mínimos cuadrados ponderados). También se han desarrollado extensiones del método original para el caso de respuesta multivariante: \\[m_i(\\mathbf{x}) = \\beta_{i0} + \\sum_{m=1}^M \\beta_{im} g_m (\\alpha_{1m}x_1 + \\alpha_{2m}x_2 + \\ldots + \\alpha_{pm}x_p)\\] reescalando las funciones rigde de forma que tengan media cero y varianza unidad sobre las proyecciones de las observaciones. Este procedimiento de regresión está muy relacionado con las redes de neuronas artificiales que se tratarán en el siguiente capítulo y que han sido de mayor objeto de estudio y desarrollo en los último años. L.5.2 Implementación en R El método PPR (con respuesta multivariante) está implementado en la función ppr() del paquete base de R32, y es empleada por el método \"ppr\" de caret. Esta función: ppr(formula, data, nterms, max.terms = nterms, optlevel = 2, sm.method = c(&quot;supsmu&quot;, &quot;spline&quot;, &quot;gcvspline&quot;), bass = 0, span = 0, df = 5, gcvpen = 1, ...) va añadiendo términos ridge hasta un máximo de max.terms y posteriormente emplea un método hacia atrás para seleccionar nterms (el argumento optlevel controla como se vuelven a reajustar los términos en cada iteración). Por defecto emplea el super suavizador de Friedman (función supsmu(), con parámetros bass y spam), aunque también admite splines (función smooth.spline(), fijando los grados de libertad con df o seleccionándolos mediante GCV). Para más detalles ver help(ppr). Continuaremos con el ejemplo del conjunto de datos earth::Ozone1. En primer lugar ajustamos un modelo PPR con dos términos (incrementando el suavizado por defecto de supsmu() siguiendo la recomendación de Venables y Ripley, 2002): ppreg &lt;- ppr(O3 ~ ., nterms = 2, data = train, bass = 2) summary(ppreg) ## Call: ## ppr(formula = O3 ~ ., data = train, nterms = 2, bass = 2) ## ## Goodness of fit: ## 2 terms ## 4033.668 ## ## Projection direction vectors (&#39;alpha&#39;): ## term 1 term 2 ## vh -0.016617786 0.047417127 ## wind -0.317867945 -0.544266150 ## humidity 0.238454606 -0.786483702 ## temp 0.892051760 -0.012563393 ## ibh -0.001707214 -0.001794245 ## dpg 0.033476907 0.285956216 ## ibt 0.205536326 0.026984921 ## vis -0.026255153 -0.014173612 ## doy -0.044819013 -0.010405236 ## ## Coefficients of ridge terms (&#39;beta&#39;): ## term 1 term 2 ## 6.790447 1.531222 oldpar &lt;- par(mfrow = c(1, 2)) plot(ppreg) par(oldpar) Evaluamos las predicciones en la muestra de test: pred &lt;- predict(ppreg, newdata = test) obs &lt;- test$O3 plot(pred, obs, main = &quot;Observado frente a predicciones&quot;, xlab = &quot;Predicción&quot;, ylab = &quot;Observado&quot;) abline(a = 0, b = 1) abline(lm(obs ~ pred), lty = 2) accuracy(pred, obs) ## me rmse mae mpe mape r.squared ## 0.4819794 3.2330060 2.5941476 -6.1203121 34.8728543 0.8384607 Empleando el método \"ppr\" de caret para seleccionar el número de términos: library(caret) modelLookup(&quot;ppr&quot;) ## model parameter label forReg forClass probModel ## 1 ppr nterms # Terms TRUE FALSE FALSE set.seed(1) caret.ppr &lt;- train(O3 ~ ., data = train, method = &quot;ppr&quot;, # bass = 2, trControl = trainControl(method = &quot;cv&quot;, number = 10)) caret.ppr ## Projection Pursuit Regression ## ## 264 samples ## 9 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 238, 238, 238, 236, 237, 239, ... ## Resampling results across tuning parameters: ## ## nterms RMSE Rsquared MAE ## 1 4.366022 0.7069042 3.306658 ## 2 4.479282 0.6914678 3.454853 ## 3 4.624943 0.6644089 3.568929 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was nterms = 1. ggplot(caret.ppr, highlight = TRUE) summary(caret.ppr$finalModel) ## Call: ## ppr(x = as.matrix(x), y = y, nterms = param$nterms) ## ## Goodness of fit: ## 1 terms ## 4436.727 ## ## Projection direction vectors (&#39;alpha&#39;): ## vh wind humidity temp ibh dpg ## -0.016091543 -0.167891347 0.351773894 0.907301452 -0.001828865 0.026901492 ## ibt vis doy ## 0.148021198 -0.026470384 -0.035703896 ## ## Coefficients of ridge terms (&#39;beta&#39;): ## term 1 ## 6.853971 plot(caret.ppr$finalModel) # varImp(caret.ppr) # emplea una medida genérica de importancia pred &lt;- predict(caret.ppr, newdata = test) accuracy(pred, obs) ## me rmse mae mpe mape r.squared ## 0.3135877 3.3652891 2.7061615 -10.7532705 33.8333646 0.8249710 Para ajustar un modelo single index también se podría emplear la función npindex() del paquete np (que implementa el método de Ichimura, 1993, considerando un estimador local constante), aunque en este caso ni el tiempo de computación ni el resultado es satisfactorio: library(np) # bw &lt;- npindexbw(O3 ~ ., data = train) # Error in terms.formula(formula): &#39;.&#39; in formula and no &#39;data&#39; argument # formula &lt;- reformulate(setdiff(colnames(train), &quot;O3&quot;), response=&quot;O3&quot;) bw &lt;- npindexbw(O3 ~ vh + wind + humidity + temp + ibh + dpg + ibt + vis + doy, data = train, optim.method = &quot;BFGS&quot;, nmulti = 1) # Por defecto nmulti = 5 ## Multistart 1 of 1...\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b summary(bw) ## ## Single Index Model ## Regression data (264 observations, 9 variable(s)): ## ## vh wind humidity temp ibh dpg ibt vis ## Beta: 1 4.338446 6.146688 10.44244 0.0926648 3.464211 5.017786 -0.5646063 ## doy ## Beta: -1.048745 ## Bandwidth: 16.54751 ## Optimisation Method: BFGS ## Regression Type: Local-Constant ## Bandwidth Selection Method: Ichimura ## Formula: O3 ~ vh + wind + humidity + temp + ibh + dpg + ibt + vis + doy ## Bandwidth Type: Fixed ## Objective Function Value: 18.87884 (achieved on multistart 1) ## ## Continuous Kernel Type: Second-Order Gaussian ## No. Continuous Explanatory Vars.: 1 ## Estimation Time: 8.94 seconds sindex &lt;- npindex(bws = bw, gradients = TRUE) summary(sindex) ## ## Single Index Model ## Regression Data: 264 training points, in 9 variable(s) ## ## vh wind humidity temp ibh dpg ibt vis ## Beta: 1 4.338446 6.146688 10.44244 0.0926648 3.464211 5.017786 -0.5646063 ## doy ## Beta: -1.048745 ## Bandwidth: 16.54751 ## Kernel Regression Estimator: Local-Constant ## ## Residual standard error: 3.520037 ## R-squared: 0.806475 ## ## Continuous Kernel Type: Second-Order Gaussian ## No. Continuous Explanatory Vars.: 1 plot(bw) pred &lt;- predict(sindex, newdata = test) accuracy(pred, obs) ## me rmse mae mpe mape r.squared ## 0.1712457 4.3725067 3.1789199 -10.2320531 35.2010284 0.7045213 Basada en la función ppreg() de S-PLUS e implementado en R por B.D. Ripley inicialmente para el paquete MASS. "],["M-neural-nets.html", "M Redes neuronales", " M Redes neuronales Las redes neuronales (McCulloch y Pitts, 1943), también conocidas como redes de neuronas artificiales (artificial neural network; ANN), son una metodología de aprendizaje supervisado que destaca porque da lugar a modelos con un número muy elevado de parámetros, adecuada para abordar problemas con estructuras subyacentes muy complejas, pero de muy difícil interpretación. Con la aparición de los métodos SVM y boosting, ANN perdió popularidad, pero en los últimos años ha vuelto a ganarla, también gracias al aumento de las capacidades de computación. El diseño y el entrenamiento de una ANN suele requerir de más tiempo y experimentación que otros algoritmos de AE/ML. Además el gran número de hiperparámetros lo convierte en un problema de optimización complicado. En este capítulo se va a hacer una breve introducción a estos métodos, para poder emplearlos con solvencia en la práctica sería muy recomendable profundizar más en esta metodología (por ejemplo se podría consultar Chollet y Allaire, 2018, para un tratamiento más detallado). En los métodos de aprendizaje supervisado se realizan una o varias transformaciones del espacio de las variables predictoras buscando una representación óptima de los datos, para así poder conseguir una buena predicción. Los modelos que realizan una o dos transformaciones reciben el nombre de modelos superficiales (shallow models). Por el contrario, cuando se realizan muchas transformaciones se habla de aprendizaje profundo (deep learning). No nos debemos dejar engañar por la publicidad: que un aprendizaje sea profundo no significa que sea mejor que el superficial. Aunque es verdad que ahora mismo la metodología que está de moda son las redes neuronales profundas (deep neural networks), hay que ser muy consciente de que dependiendo del contexto será más conveniente un tipo de modelos u otro. Se trata de una metodología adecuada para problemas muy complejos y no tanto para problemas con pocas observaciones o pocos predictores. Hay que tener en cuenta que no existe ninguna metodología que sea transversalmente la mejor (lo que se conoce como el teorema no free lunch; Wolpert y Macready, 1997). Una red neuronal básica va a realizar dos transformaciones de los datos, y por tanto es un modelo con tres capas: una capa de entrada (input layer) consistente en las variables originales \\(\\mathbf{X} = (X_1,X_2,\\ldots, X_p)\\), otra capa oculta (hidden layer) con \\(M\\) nodos, y la capa de salida (output layer) con la predicción (o predicciones) final \\(m(\\mathbf{X})\\). Para que las redes neuronales tengan un rendimiento aceptable se requiere disponer de tamaños muestrales grandes, debido a que son modelos hiperparametrizados (y por tanto de difícil interpretación). Son modelos muy demandantes computacionalmente y solo desde fechas recientes es viable utilizarlos con un número elevado de capas (deep neural networks). Son modelos muy sensibles a las escala de los predictores y requieren un preprocesado en el que se homogeneicen sus escalas. Una de las fortalezas de las redes neuronales es que sus modelos son muy robustos frente a predictores irrelevantes. Esto la convierte en una metodología muy interesante cuando se dispone de datos de dimensión muy alta. Otros métodos requieren un preprocesado muy costoso, pero las redes neuronales lo realizan de forma automática en las capas intermedias, que de forma sucesiva se van centrado en aspectos relevantes de los datos. Y una de sus debilidades es que conforme aumentan las capas se hace más difícil la interpretación del modelo, hasta convertirse en una auténtica caja negra. Hay distintas formas de construir redes neuronales. La básica recibe el nombre de feedforward (o también multilayer perceptron). Otras formas, con sus campos de aplicación principales, son: Convolutional neural networks para reconocimiento de imagen y vídeo. Recurrent neural networks para reconocimiento de voz. Long short-term memory neural networks para traducción automática. "],["M-1-single-hidden-layer-feedforward-network.html", "M.1 Single-hidden-layer feedforward network", " M.1 Single-hidden-layer feedforward network La red neuronal más simple es la single-hidden-layer feedforward network, también conocida como single layer perceptron. Se trata de una red feedforward con una única capa oculta que consta de \\(M\\) variables ocultas \\(h_m\\) (los nodos que conforman la capa, también llamados unidades ocultas). Cada variable \\(h_m\\) es una combinación lineal de las variables predictoras, con parámetros \\(\\omega_{jm}\\) (los parámetros \\(\\omega_{0m}\\) reciben el nombre de parámetros sesgo) \\[\\omega_{0m} + \\omega_{1m} x_1 + \\omega_{2m} x_2 + \\ldots + \\omega_{pm} x_p\\] transformada por una función no lineal, denominada función de activación, típicamente la función logística (denominada función sigmoidal, sigmoid function, en este contexto) \\[\\phi(u) = \\frac{1}{1 + e^{-u}} = \\frac{e^u}{1 + e^u}\\] (la idea es que cada neurona aprende un resultado binario). De este modo tenemos que \\[h_{m}(\\mathbf{x}) = \\phi\\left( \\omega_{0m} + \\omega_{1m} x_1 + \\omega_{2m} x_2 + \\ldots + \\omega_{pm} x_p \\right)\\] El modelo final es una combinación lineal de las variables ocultas \\[m(\\mathbf{x}) = \\gamma_0 + \\gamma_1 h_1 + \\gamma_2 h_2 + \\ldots + \\gamma_M h_M\\] aunque también se puede considerar una función de activación en el nodo final para adaptar la predicción a distintos tipos de respuestas (en regresión sería normalmente la identidad) y distintas funciones de activación en los nodos intermedios (ver por ejemplo Wikipedia: Activation function para un listado de distintas funciones de activación). Por tanto, el modelo \\(m\\) es un modelo de regresión no lineal en dos etapas con \\(M(p + 1) + M + 1\\) parámetros (también llamados pesos). Por ejemplo, con 200 variables predictoras y 10 variables ocultas, hay nada menos que 2021 parámetros. Como podemos comprobar, incluso con el modelo más sencillo y una cantidad moderada de variables predictoras y ocultas, el número de parámetros a estimar es muy grande. Por eso decimos que estos modelos están hiperparametrizados. La estimación de los parámetros (el aprendizaje) se realiza minimizando una función de pérdidas, típicamente \\(\\mbox{RSS}\\). La solución exacta de este problema de optimización suele ser imposible en la práctica (es un problema no convexo), por lo que se resuelve mediante un algoritmo heurístico de descenso de gradientes (que utiliza las derivadas de las funciones de activación), llamado en este contexto backpropagation (Werbos, 1975), que va a converger a un óptimo local, pero difícilmente al óptimo global. Por este motivo, el modelo resultante va a ser muy sensible a la solución inicial, que generalmente se selecciona de forma aleatoria con valores próximos a cero (si se empezase dando a los parámetros valores nulos, el algoritmo no se movería). El algoritmo va cogiendo los datos de entrenamiento por lotes (de 32, 64) llamados batch, y recibe el nombre de epoch cada vez que el algoritmo completa el procesado de todos los datos; por tanto, el número de epochs es el número de veces que el algoritmo procesa la totalidad de los datos. Una forma de mitigar la inestabilidad de la estimación del modelo es generando muchos modelos (que se consiguen con soluciones iniciales diferentes) y promediando las predicciones; una alternativa es utilizar bagging. El algoritmo depende de un parámetro en cada iteración, que representa el ratio de aprendizaje (learning rate). Por razones matemáticas, se selecciona una sucesión que converja a cero. Otro problema inherente a la heurística de tipo gradiente es que se ve afectada negativamente por la correlación entre las variables predictoras. Cuando hay correlaciones muy altas, es usual preprocesar los datos, o bien eliminando variables predictoras o bien utilizando PCA. Naturalmente, al ser las redes neuronales unos modelos con tantos parámetros tienen una gran tendencia al sobreajuste. Una forma de mitigar este problema es implementar la misma idea que se utiliza en la regresión ridge de penalizar los parámetros y que en este contexto recibe el nombre de reducción de los pesos (weight decay) \\[\\mbox{min}_{\\boldsymbol{\\omega}, \\boldsymbol{\\gamma}}\\ \\mbox{RSS} + \\lambda \\left(\\sum_{m=1}^m \\sum_{j=0}^p \\omega_{jm}^2 + \\sum_{m=0}^M \\gamma_m^2 \\right)\\] En esta modelización del problema, hay dos hiperparámetros cuyos valores deben ser seleccionados: el parámetro regularizador \\(\\lambda\\) (con frecuencia un número entre 0 y 0.1) y el número de nodos \\(M\\). Es frecuente seleccionar \\(M\\) a mano (un valor alto, entre 5 y 100) y \\(\\lambda\\) por validación cruzada, confiando en que el proceso de regularización forzará a que muchos pesos (parámetros) sean próximos a cero. Además, al depender la penalización de una suma de pesos es imprescindible que sean comparables, es decir, hay que reescalar las variables predictoras antes de empezar a construir el modelo. La extensión natural de este modelo es utilizar más de una capa de nodos (variables ocultas). En cada capa, los nodos están conectados con los nodos de la capa precedente. Observemos que el modelo single-hidden-layer feedforward network tiene la misma forma que el de la projection pursuit regression (Sección (ppr)), sin más que considerar \\(\\alpha_m = \\omega_m/\\| \\omega_m \\|\\), con \\(\\omega_m = (\\omega_{1m}, \\omega_{2m}, \\ldots, \\omega_{pm})\\), y \\[g_m (\\alpha_{1m}x_1 + \\alpha_{2m}x_2 + \\ldots + \\alpha_{pm}x_p) = \\gamma_m \\phi(\\omega_{0m} + \\omega_{1m} x_1 + \\omega_{2m} x_2 + \\ldots + \\omega_{pm} x_p)\\] Sin embargo, hay que destacar una diferencia muy importante: en una red neuronal, el analista fija la función \\(\\phi\\) (lo más habitual es utilizar la función logística), mientras que las funciones ridge \\(g_m\\) se consideran como funciones no paramétricas desconocidas que hay que estimar. "],["M-2-clasificación-con-ann.html", "M.2 Clasificación con ANN", " M.2 Clasificación con ANN En un problema de clasificación con dos categorías, si se emplea una variable binaria para codificar la respuesta, bastará con considerar una función logística como función de activación en el nodo final (de esta forma se estará estimando la probabilidad de éxito). En el caso general, en lugar de construir un único modelo \\(m(\\mathbf{x})\\), se construyen tantos como categorías, aunque habrá que seleccionar una función de activación adecuada en los nodos finales. Por ejemplo, en el caso de una single-hidden-layer feedforward network, para cada categoría \\(i\\), se construye el modelo \\(T_i\\) como ya se explicó antes \\[T_i(\\mathbf{x}) = \\gamma_{0i} + \\gamma_{1i} h_1 + \\gamma_{2i} h_2 + \\ldots + \\gamma_{Mi} h_M \\] y a continuación se transforman los resultados de los \\(k\\) modelos para obtener estimaciones válidas de las probabilidades \\[m_i(\\mathbf{x}) = \\tilde{\\phi}_i (T_1(\\mathbf{x}), T_2(\\mathbf{x}),\\ldots, T_k(\\mathbf{x})) \\] donde \\(\\tilde{\\phi}_i\\) es la función softmax \\[\\tilde{\\phi}_i (u_1,u_2,\\ldots,u_k) = \\frac{e^{u_i}}{\\sum_{j=1}^k e^{u_j}}\\] Como criterio de error se suele utilizar la entropía aunque se podrían considerar otros. Desde este punto de vista la regresión logística (multinomial) sería un caso particular. "],["M-3-implementación-en-r-1.html", "M.3 Implementación en R", " M.3 Implementación en R Hay numerosos paquetes que implementan métodos de este tipo, aunque por simplicidad consideraremos el paquete nnet que implementa redes neuronales feed fordward con una única capa oculta y está incluido en el paquete base de R. Para el caso de redes más complejas se puede utilizar por ejemplo el paquete neuralnet, pero en el caso de grandes volúmenes de datos o aprendizaje profundo la recomendación sería emplear paquetes computacionalmente más eficientes (con computación en paralelo empleando CPUs o GPUs) como keras, h2o o sparlyr, entre otros. La función principal nnet() se suele emplear con los siguientes argumentos: nnet(formula, data, size, Wts, linout = FALSE, skip = FALSE, rang = 0.7, decay = 0, maxit = 100, ...) formula y data (opcional): permiten especificar la respuesta y las variables predictoras de la forma habitual (e.g. respuesta ~ .; también implementa una interfaz con matrices x e y). Admite respuestas multidimensionales (ajustará un modelo para cada componente) y categóricas (las convierte en multivariantes si tienen más de dos categorías y emplea softmax en los nodos finales). Teniendo en cuenta que por defecto los pesos iniciales se asignan al azar (Wts &lt;- runif(nwts, -rang, rang)) la recomendación sería reescalar los predictores en el intervalo \\([0, 1]\\), sobre todo si se emplea regularización (decay &gt; 0). size: número de nodos en la capa oculta. linout: permite seleccionar la identidad como función de activación en los nodos finales; por defecto FALSE y empleará la función logística o softmax en el caso de factores con múltiples niveles (si se emplea la interfaz de fórmula, con matrices habrá que establecer softmax = TRUE). skip: permite añadir pesos adicionales entre la capa de entrada y la de salida (saltándose la capa oculta); por defecto FALSE. decay: parámetro \\(\\lambda\\) de regularización de los pesos (weight decay); por defecto 0. Para emplear este parámetro los predictores deberían estar en la misma escala. maxit: número máximo de iteraciones; por defecto 100. Como ejemplo consideraremos el conjunto de datos earth::Ozone1 empleado en el capítulo anterior: data(ozone1, package = &quot;earth&quot;) df &lt;- ozone1 set.seed(1) nobs &lt;- nrow(df) itrain &lt;- sample(nobs, 0.8 * nobs) train &lt;- df[itrain, ] test &lt;- df[-itrain, ] En este caso emplearemos el método \"nnet\" de caret para preprocesar los datos y seleccionar el número de nodos en la capa oculta y el parámetro de regularización. Como emplea las opciones por defecto de nnet() (diseñadas para clasificación), estableceremos linout = TRUE (la alternativa sería transformar la respuesta a rango 1) y aumentaremos el número de iteraciones (aunque seguramente sigue siendo demasiado pequeño). library(caret) # Buscar &quot;Neural Network&quot;: 10 métodos # getModelInfo(&quot;nnet&quot;) modelLookup(&quot;nnet&quot;) ## model parameter label forReg forClass probModel ## 1 nnet size #Hidden Units TRUE TRUE TRUE ## 2 nnet decay Weight Decay TRUE TRUE TRUE tuneGrid &lt;- expand.grid(size = 2*1:5, decay = c(0, 0.001, 0.01)) set.seed(1) caret.nnet &lt;- train(O3 ~ ., data = train, method = &quot;nnet&quot;, preProc = c(&quot;range&quot;), # Reescalado en [0,1] tuneGrid = tuneGrid, trControl = trainControl(method = &quot;cv&quot;, number = 10), linout = TRUE, maxit = 200, trace = FALSE) ggplot(caret.nnet, highlight = TRUE) Analizamos el modelo resultante: summary(caret.nnet$finalModel) ## a 9-4-1 network with 45 weights ## options were - linear output units decay=0.01 ## b-&gt;h1 i1-&gt;h1 i2-&gt;h1 i3-&gt;h1 i4-&gt;h1 i5-&gt;h1 i6-&gt;h1 i7-&gt;h1 i8-&gt;h1 i9-&gt;h1 ## -8.66 3.74 -5.50 -18.11 -12.83 6.49 14.39 -4.53 14.48 -1.96 ## b-&gt;h2 i1-&gt;h2 i2-&gt;h2 i3-&gt;h2 i4-&gt;h2 i5-&gt;h2 i6-&gt;h2 i7-&gt;h2 i8-&gt;h2 i9-&gt;h2 ## -2.98 1.78 0.00 1.58 1.96 -0.60 0.63 2.46 2.36 -19.69 ## b-&gt;h3 i1-&gt;h3 i2-&gt;h3 i3-&gt;h3 i4-&gt;h3 i5-&gt;h3 i6-&gt;h3 i7-&gt;h3 i8-&gt;h3 i9-&gt;h3 ## 25.23 -50.14 9.74 -3.66 -5.61 4.21 -11.17 39.34 -20.18 0.37 ## b-&gt;h4 i1-&gt;h4 i2-&gt;h4 i3-&gt;h4 i4-&gt;h4 i5-&gt;h4 i6-&gt;h4 i7-&gt;h4 i8-&gt;h4 i9-&gt;h4 ## -3.90 4.94 -1.08 1.50 1.52 -0.54 0.14 -1.27 0.98 -1.54 ## b-&gt;o h1-&gt;o h2-&gt;o h3-&gt;o h4-&gt;o ## -5.32 4.19 -14.03 7.50 38.75 Podemos representarlo gráficamente empleando el paquete NeuralNetTools: library(NeuralNetTools) plotnet(caret.nnet$finalModel) Por último evaluamos las predicciones en la muestra de test: pred &lt;- predict(caret.nnet, newdata = test) obs &lt;- test$O3 plot(pred, obs, main = &quot;Observado frente a predicciones&quot;, xlab = &quot;Predicción&quot;, ylab = &quot;Observado&quot;) abline(a = 0, b = 1) abline(lm(obs ~ pred), lty = 2) accuracy &lt;- function(pred, obs, na.rm = FALSE, tol = sqrt(.Machine$double.eps)) { err &lt;- obs - pred # Errores if(na.rm) { is.a &lt;- !is.na(err) err &lt;- err[is.a] obs &lt;- obs[is.a] } perr &lt;- 100*err/pmax(obs, tol) # Errores porcentuales return(c( me = mean(err), # Error medio rmse = sqrt(mean(err^2)), # Raíz del error cuadrático medio mae = mean(abs(err)), # Error absoluto medio mpe = mean(perr), # Error porcentual medio mape = mean(abs(perr)), # Error porcentual absoluto medio r.squared = 1 - sum(err^2)/sum((obs - mean(obs))^2) # Pseudo R-cuadrado )) } accuracy(pred, obs) ## me rmse mae mpe mape r.squared ## 0.3321276 3.0242169 2.4466958 -7.4095987 32.8000107 0.8586515 "],["referencias-1.html", "Referencias", " Referencias "],["bibliografía-básica.html", "Bibliografía básica", " Bibliografía básica James, G., Witten, D., Hastie, T. y Tibshirani, R. (2013). An Introduction to Statistical Learning: with Aplications in R. Springer. Kuhn, M. y Johnson, K. (2013). Applied predictive modeling. Springer. Williams, G. (2011). Data Mining with Rattle and R. Springer. "],["bibliografía-complementaria-1.html", "Bibliografía complementaria", " Bibliografía complementaria Libros Bellman, R.E. (1961). Adaptive Control Processes, Princeton University Press. Burger, S.V. (2018). Introduction to machine learning with R: Rigorous mathematical analysis. OReilly. Breiman, L., Friedman, J., Stone, C.J. y Olshen, R.A. (1984). Classification and regression trees. CRC press. Chollet, F. y Allaire, J.J. (2018). Deep Learning with R. Manning Publications. De Boor, C. (1978). A practical guide to splines. Springer-Verlag. Efron, B. y Hastie, T. (2016). Computer age statistical inference. Cambridge University Press. Fan, J. y Gijbels, I. (1996). Local polynomial modelling and its applications. Chapman &amp; Hall. Faraway, J.J. (2014). Linear models with R. CRC press. Fernández-Casal, R. y Cao, R. (2020). Simulación Estadística. https://rubenfcasal.github.io/simbook. Fernández-Casal, R., Roca-Pardiñas, J. y Costa, J. (2019). Introducción al Análisis de Datos con R. https://rubenfcasal.github.io/intror. Hair, J.F., Black, W.C., Babin, B.J., Anderson, R.E. y Tatham, R.L. (1998). Multivariate data analysis. Prentice Hall. Hastie, T.J. y Tibshirani, R.J. (1990). Generalized Additive Models. Chapman &amp; Hall. Hastie, T., Tibshirani, R. y Friedman, J. (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer. Hastie, T., Tibshirani, R. y Wainwright, M. (2015). Statistical learning with sparsity: the lasso and generalizations. CRC press. Irizarry, R.A. (2019). Introduction to Data Science: Data Analysis and Prediction Algorithms with R. CRC Press. Molnar, C. (2020). Interpretable Machine Learning. A Guide for Making Black Box Models Explainable. Lulu.com. Torgo, L. (2011). Data Mining with R: Learning with Case Studies. Chapman &amp; Hall/CRC Press. Vapnik, V. (1998). Statistical Learning Theory. Wiley. Vapnik, V. (2010). The Nature of Statistical Learning Theory. Springer. Venables, W.N. y Ripley, B.D. (2002). Modern applied statistics with S. Springer. Werbos, P.J. (1975). Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences, Harvard University. Wood, S.N. (2017). Generalized Additive Models: An Introduction with R. Chapman &amp; Hall/CRC Artículos Agor, J. y Özaltn, O.Y. (2019). Feature selection for classification models via bilevel optimization. Computers &amp; Operations Research, 106, 156-168. Berntsson, P. y Wold, S. (1986). Comparison Between X-ray Crystallographic Data and Physiochemical Parameters with Respect to Their Information About the Calcium Channel Antagonist Activity of 4-Phenyl-1,4- Dihydropyridines. Quantitative Structure-Activity Relationships, 5, 4550. Biecek, P. (2018). DALEX: explainers for complex predictive models in R. The Journal of Machine Learning Research, 19(1), 3245-3249. Boser, B., Guyon, I. y Vapnik, V. (1992). A Training Algorithm for Optimal Margin Classifiers. Proceedings of the Fifth Annual Workshop on Computational Learning Theory, 144152. Breiman, L. (1996). Bagging predictors. Machine Learning, 24(2), 123-140. Breiman, L. (2001). Random Forests. Machine Learning, 45(1), 532. Breiman, L. (2001). Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author). Statistical Science, 16, 199-231. Comon, P. (1994). Independent component analysis, a new concept?. Signal processing, 36(3), 287-314. Cortes, C. y Vapnik, V. (1995). SupportVector Networks. Machine Learning, 20(3), 273297. Craven, P. y Wahba, G. (1979). Smoothing Noisy Data with Spline Functions. Numerische Mathematik, 31, pp. 377-403. Culp, M., Johnson, K. y Michailidis, G. (2006). ada: an R Package for Stochastic Boosting. Journal of Statistical Software, 17(2), 1-27. Dietterich, T.G. (2000). An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging, Boosting, and Randomization. Machine Learning, 40(2), 139158. Drucker, H., Burges, C., Kaufman, L., Smola, A. y Vapnik, V. (1997). Support Vector Regression Machines. Advances in Neural Information Processing Systems, 155161. Dunson D.B. (2018). Statistics in the big data era: Failures of the machine. Statistics and Probability Letters, 136, 4-9. Efron, B., Hastie, T., Johnstone, I. y Tibshirani, R. (2004). Least Angle Regression. Annals of Statistics, 32(2), 407499. Eilers, P.H.C. y Marx, B.D. (1996). Flexible smoothing with B-splines and penalties. Statistical Science, 11(2), 89-102. Fisher, R.A. (1936). The use of multiple measurements in taxonomic problems. Annals of Eugenics, 7(2), 179-188. Freund, Y. y Schapire, R. (1996). Experiments with a New Boosting Algorithm. Machine Learning: Proceedings of the Thirteenth International Conference, 148156. Friedman, J.H. (1989). Regularized Discriminant Analysis. Journal of the American Statistical Association, 84(405), 165175. Friedman, J.H. (1991). Multivariate Adaptive Regression Splines. Annals of Statistics, 167. Friedman, J.H. (2001). Greedy Function Approximation: A Gradient Boosting Machine. Annals of Statistics, 29, 11891232. Friedman, J.H. (2002). Stochastic Gradient Boosting. Computational Statistics and Data Analysis, 38(4), 367-378. Friedman, J., Hastie, T. y Tibshirani, R. (2000). Additive Logistic Regression: A statistical view of boosting. Annals of Statistics, 28(2), 337-374. Friedman, J.H. y Popescu, B.E. (2008). Predictive learning via rule ensembles. The Annals of Applied Statistics, 2(3), 916-954. Friedman, J.H. y Stuetzle, W. (1981). Projection Pursuit Regression. Journal of the American Statistical Association, 76, 817823. Friedman, J.H. y Tukey, J.W. (1974). A Projection Pursuit Algorithm for Exploratory Data Analysis. IEEE Transactions on Computers, Series C, 23, 881890. Goldstein, A., Kapelner, A., Bleich, J. y Pitkin, E. (2015). Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation. Journal of Computational and Graphical Statistics, 24(1), 44-65. Greenwell, B.M. (2017). pdp: An R Package for Constructing Partial Dependence Plots. The R Journal, 9(1), 421-436. Hastie, T., Rosset, S., Tibshirani, R. y Zhu, J. (2004). The entire regularization path for the support vector machine. Journal of Machine Learning Research, 5, 1391-1415. Hastie, T. y Tibshirani, R. (1996). Discriminant Analysis by GaussianMixtures. Journal of the Royal Statistical Society, Series B, 155176. Hoerl, A. (1970). Ridge Regression: Biased Estimation for Nonorthogonal Problems. Technometrics, 12(1), 5567. Ichimura, H. (1993). Semiparametric least squares (SLS) and weighted SLS estimation of single-index models. Journal of Econometrics, 58, 71-120. Karatzoglou, A., Smola, A., Hornik, K. y Zeileis, A. (2004). kernlab-an S4 package for kernel methods in R. Journal of Statistical Software, 11(9), 1-20. Kearns, M. y Valiant, L. (1989). Cryptographic Limitations on Learning Boolean Formulae and Finite Automata. Proceedings of the Twenty-First Annual ACM Symposium on Theory of Computing. Kruskal, J.B. (1969). Toward a practical method which helps uncover the structure of a set of observations by finding the line transformation which optimizes a new index of condensation. En Milton, R.C. y Nelder, J.A. (eds.), Statistical Computation, Academic Press, 427440. Kuhn, M. (2008). Building predictive models in R using the caret package. Journal of Statistical Software, 28(5), 1-26. Lauro, C. (1996). Computational statistics or statistical computing, is that the question?, Computational Statistics &amp; Data Analysis, 23 (1), 191-193. Liaw, A. y Wiener, M. (2002). Classification and regression by randomForest. R News, 2(3), 18-22. Loh, W.Y. (2002). Regression tress with unbiased variable selection and interaction detection. Statistica Sinica, 361-386. Massy, W. (1965). Principal Components Regression in Exploratory Statistical Research. Journal of the American Statistical Association, 60, 234246. McCulloch, W. y Pitts, W. (1943). A Logical Calculus of Ideas Immanent in Nervous Activity. Bulletin of Mathematical Biophysics, 5, 115133. Mevik, B.H. y Wehrens, R. (2007). The pls Package: Principal Component and Partial Least Squares Regression in R. Journal of Statistical Software, 18(2), 1-23. Tibshirani, R. (1996). Regression Shrinkage and Selection via the lasso. Journal of the Royal Statistical Society, Series B, 58(1), 267288. Shannon C (1948). A Mathematical Theory of Communication. The Bell System Technical Journal, 27, 379423. Strumbelj, E. y Kononenko, I. (2010). An efficient explanation of individual classifications using game theory. Journal of Machine Learning Research, 11, 1-18. Valiant, L. (1984). A Theory of the Learnable. Communications of the ACM, 27, 11341142. Vinayak, R.K. y Gilad-Bachrach, R. (2015). Dart: Dropouts meet multiple additive regression trees. Proceedings of Machine Learning Research, 38, 489-497. Welch, B. (1939). Note on Discriminant Functions. Biometrika, 31, 218220. Wold, S., Martens, H. y Wold, H. (1983). The Multivariate Calibration Problem in Chemistry Solved by the PLS Method. Proceedings from the Conference on Matrix Pencils. SpringerVerlag. Wolpert, D.H. y Macready, W.G. (1997). No free lunch theorems for optimization. IEEE Transactions on Evolutionary Computation, 1(1), 67-82. Zou, H. y Hastie, T. (2005). Regularization and Variable Selection via the Elastic Net. Journal of the Royal Statistical Society, Series B, 67(2), 301320. "]]
