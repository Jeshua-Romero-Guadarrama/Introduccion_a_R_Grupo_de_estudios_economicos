head(cars)
cars[, c("distancia")] <- cars[, 2]/3.2808
head(cars)
coches <- cars[, c("velocidad", "distancia")]
# head(coches)
str(coches)
x <- c(2.5, 4.3, 1.2, 3.1, 5.0) # valores originales
ii <- order(x)
ii    # vector de ordenación
x[ii] # valores ordenados
ii <- order(cars$dist) # Vector de índices de ordenación
cars2 <- cars[ii, ]    # Datos ordenados por dist
head(cars2)
subset(cars, cars$dist > 85) # datos con dis>85
subset(cars, cars$speed > 10 & cars$speed < 15 & cars$dist > 45) # speed en (10,15) y dist>45
ii <- cars$dist > 85
cars[ii, ]   # dis>85
ii <- cars$speed > 10 & cars$speed < 15 & cars$dist > 45
cars[ii, ]  # speed en (10,15) y dist>45
it <- which(ii)
str(it)
cars[it, 1:2]
# rownames(cars[it, 1:2])
id <- which(!ii)
str(cars[id, 1:2])
# Se podría p.e. emplear cars[id, ] para predecir cars[it, ]$speed
# ?which.min
an <- function(a1, r, n) {
a1 * r^(n - 1)
}
an(a1 = 1, r = 2, n = 5)
an(a1 = 4, r = -2, n = 6)
an(a1 = -50, r = 4, n = 6)
an(a1 = 1, r = 2, n = 1:5)    # a1, ..., a5
an(a1 = 1, r = 2, n = 10:15)  # a10, ..., a15
Sn <- function(a1, r, n) {
a1 * (r^n - 1) / (r - 1)
}
Sn(a1 = 1, r = 2, n = 5)
an(a1 = 1, r = 2, n = 1:5)    # Valores de la progresión
Sn(a1 = 1, r = 2, n = 1:5)    # Suma de los valores
# cumsum(an(a1 = 1, r = 2, n = 1:5))
an(1, 2, 5)
an(a1 = 1, r = 2, n = 5)
an(r = 2, n = 5, a1 = 1)
an(n = 5, r = 2, a1 = 1)
xy2 <- function(x = 2, y = 3) { x * y^2 }
xy2()
xy2(x = 1, y = 4)
xy2(y = 4)
Density.Plot <- function(datos, ...) { plot(density(datos), ...) }
data(cars)
Density.Plot(is.numeric(cars$speed))
an <- function(a1, r, n) {
a1 * r^(n - 1)
}
an(a1 = 1, r = 2, n = 5)
an(a1 = 4, r = -2, n = 6)
an(a1 = -50, r = 4, n = 6)
an(a1 = 1, r = 2, n = 1:5)    # a1, ..., a5
an(a1 = 1, r = 2, n = 10:15)  # a10, ..., a15
Sn <- function(a1, r, n) {
a1 * (r^n - 1) / (r - 1)
}
Sn(a1 = 1, r = 2, n = 5)
an(a1 = 1, r = 2, n = 1:5)    # Valores de la progresión
Sn(a1 = 1, r = 2, n = 1:5)    # Suma de los valores
# cumsum(an(a1 = 1, r = 2, n = 1:5))
an(1, 2, 5)
an(a1 = 1, r = 2, n = 5)
an(r = 2, n = 5, a1 = 1)
an(n = 5, r = 2, a1 = 1)
xy2 <- function(x = 2, y = 3) { x * y^2 }
xy2()
xy2(x = 1, y = 4)
xy2(y = 4)
Density.Plot <- function(datos, ...) { plot(density(datos), ...) }
data(cars)
Density.Plot(cars[,c("speed")])
Density.Plot(cars[,c("speed")], col = 'red', xlab = "velocidad", ylab = "distancia")
args(an)
args(xy2)
str(args(Density.Plot))
an
an <- function(a1, r, n) { a1 * r^(n - 1) }
Sn <- function(a1, r, n) { a1 * (r^n - 1) / (r - 1) }
asn <- function(a1 = 1, r = 2, n = 5) {
A <- an(a1, r, n)
S <- Sn(a1, r, n)
ii <- 1:n
AA <- an(a1, r, ii)
SS <- Sn(a1, r, ii)
return(list(an = A, Sn = S, salida = data.frame(valores = AA, suma = SS)))
}
asn()
res <- asn()
res$an
res$Sn
res$salida
DNI <- function(numero) {
letras <- c("T", "R", "W", "A", "G", "M", "Y", "F",
"P", "D", "X", "B", "N", "J", "Z", "S",
"Q", "V", "H", "L", "C", "K", "E")
return(letras[numero %% 23 + 1])
}
DNI(50247828)
dado <- function(n = 100) {
lanzamientos <- sample(1:6, n, rep = TRUE)
frecuencias <- table(lanzamientos) / n
barplot(frecuencias, main = paste("Número de lanzamientos=", n))
abline(h = 1 / 6, col = 'red', lwd = 2)
return(frecuencias)
}
dado(100)
dado(500)
dado(10000)
fun <- function() print(x)
x <- 1
fun()
x <- 1
fun2 <- function() {
x <- 2
print(x)
}
fun2()
x
x <- 1
y <- 3
fun2 <- function() {
x <- 2
y <<- 5
print(x)
print(y)
}
fun2()
x # No cambió su valor
y # Cambió su valor
multiplo2 = function(x) {
if (x %% 2 == 0) {
print(paste(x,'es múltiplo de dos'))
} else {
print(paste(x,'no es múltiplo de dos'))
}
}
multiplo2(5)
multiplo2(-2.3)
multiplo2(10)
x <- seq(-2, 2, 0.5)
n <- length(x)
y <- numeric(n) # Es necesario crear el objeto para acceder a los componentes...
for (i in 1:n) { y[i] <- x[i] ^ 2 }
x
y
x^2
for(i in 1:5) print(i)
cuadrado <- 0
n <- 0
while (cuadrado <= 5000) {
n <- n + 1
cuadrado <- n^2
}
cuadrado
n
n^2
x <- c(1, 2, 3, 4)
y <- c(0, 0, 5, 1)
n <- length(x)
z <- numeric(n)
for (i in 1:n) {
z[i] <- x[i] + y[i]
}
z
z <- x + y
z
x <- matrix(1:9, nrow = 3)
x
apply(x, 1, sum)    # Suma por filas
apply(x, 2, sum)    # Suma por columnas
apply(x, 2, min)    # Mínimo de las columnas
apply(x, 2, range)  # Rango (mínimo y máximo) de las columnas
data(ChickWeight)
head(ChickWeight)
peso <- ChickWeight$weight
dieta <- ChickWeight$Diet
levels(dieta) <- c("Dieta 1", "Dieta 2", "Dieta 3", "Dieta 4")
tapply(peso, dieta, mean)  # Peso medio por dieta
tapply(peso, dieta, summary)
provincia <- as.factor(c(1, 3, 4, 2, 4, 3, 2, 1, 4, 3, 2))
levels(provincia) = c("A Coruña", "Lugo", "Orense", "Pontevedra")
hijos <- c(1, 2, 0, 3, 4, 1, 0, 0, 2, 3, 1)
data.frame(provincia, hijos)
tapply(hijos, provincia, mean) # Número medio de hijos por provincia
library(car)
plot(prestige ~ income, data = Prestige, col = 'darkgray')
plot(prestige ~ income, data = Prestige, col = 'darkgray')
# Ajuste lineal
abline(lm(prestige ~ income, data = Prestige))
# Ajuste cuadrático
modelo <- lm(prestige ~ income + I(income^2), data = Prestige)
parest <- coef(modelo)
curve(parest[1] + parest[2]*x + parest[3]*x^2, lty = 2, add = TRUE)
# Ajuste cúbico
modelo <- lm(prestige ~ poly(income, 3), data = Prestige)
valores <- seq(0, 26000, len = 100)
pred <- predict(modelo, newdata = data.frame(income = valores))
lines(valores, pred, lty = 3)
legend("bottomright", c("Lineal","Cuadrático","Cúbico"), lty = 1:3)
cv.lm <- function(formula, datos) {
n <- nrow(datos)
cv.pred <- numeric(n)
for (i in 1:n) {
modelo <- lm(formula, datos[-i, ])
cv.pred[i] <- predict(modelo, newdata = datos[i, ])
}
return(cv.pred)
}
grado <- 1:5
cv.error <- numeric(5)
for(p in grado){
cv.pred <- cv.lm(prestige ~ poly(income, p), Prestige)
cv.error[p] <- mean((cv.pred - Prestige$prestige)^2)
}
plot(grado, cv.error, pch=16)
grado[which.min(cv.error)]
plot(prestige ~ income, Prestige, col = 'darkgray')
fit <- loess(prestige ~ income, Prestige, span = 0.25)
valores <- seq(0, 25000, 100)
pred <- predict(fit, newdata = data.frame(income = valores))
lines(valores, pred)
plot(prestige ~ income, Prestige, col = 'darkgray')
fit <- loess(prestige ~ income, Prestige, span = 0.5)
valores <- seq(0, 25000, 100)
pred <- predict(fit, newdata = data.frame(income = valores))
lines(valores, pred)
cv.loess <- function(formula, datos, p) {
n <- nrow(datos)
cv.pred <- numeric(n)
for (i in 1:n) {
modelo <- loess(formula, datos[-i, ], span = p,
control = loess.control(surface = "direct"))
# control = loess.control(surface = "direct") permite extrapolaciones
cv.pred[i] <- predict(modelo, newdata = datos[i, ])
}
return(cv.pred)
}
ventanas <- seq(0.2, 1, len = 10)
np <- length(ventanas)
cv.error <- numeric(np)
for(p in 1:np){
cv.pred <- cv.loess(prestige ~ income, Prestige, ventanas[p])
cv.error[p] <- mean((cv.pred - Prestige$prestige)^2)
# cv.error[p] <- median(abs(cv.pred - Prestige$prestige))
}
plot(ventanas, cv.error)
span <- ventanas[which.min(cv.error)]
span
plot(prestige ~ income, Prestige, col = 'darkgray')
fit <- loess(prestige ~ income, Prestige, span = span)
valores <- seq(0, 25000, 100)
pred <- predict(fit, newdata = data.frame(income = valores))
lines(valores, pred)
View(cars)
load("datos/winetaste.RData")
set.seed(1)
df <- winetaste
nobs <- nrow(df)
itrain <- sample(nobs, 0.8 * nobs)
train <- df[itrain, ]
test <- df[-itrain, ]
library(randomForest)
set.seed(4) # NOTA: Fijamos esta semilla para ilustrar dependencia
bagtrees <- randomForest(taste ~ ., data = train, mtry = ncol(train) - 1)
bagtrees
plot(bagtrees, main = "Tasas de error")
legend("topright", colnames(bagtrees$err.rate), lty = 1:5, col = 1:6)
# View(getTree(bagtrees, 1, labelVar=TRUE))
split_var_1 <- sapply(seq_len(bagtrees$ntree),
function(i) getTree(bagtrees, i, labelVar=TRUE)[1, "split var"])
table(split_var_1)
pred <- predict(bagtrees, newdata = test)
caret::confusionMatrix(pred, test$taste)
# load("datos/winetaste.RData")
# set.seed(1)
# df <- winetaste
# nobs <- nrow(df)
# itrain <- sample(nobs, 0.8 * nobs)
# train <- df[itrain, ]
# test <- df[-itrain, ]
set.seed(1)
rf <- randomForest(taste ~ ., data = train)
rf
plot(rf, main = "Tasas de error")
legend("topright", colnames(rf$err.rate), lty = 1:5, col = 1:6)
importance(rf)
varImpPlot(rf)
pred <- predict(rf, newdata = test)
caret::confusionMatrix(pred, test$taste)
split_var_1 <- sapply(seq_len(rf$ntree),
function(i) getTree(rf, i, labelVar=TRUE)[1, "split var"])
table(split_var_1)
# install.packages("pdp")
library(pdp)
pdp1 <- partial(rf, "alcohol")
plotPartial(pdp1)
pdp2 <- partial(rf, c("alcohol", "density"))
load("datos/winetaste.RData")
set.seed(1)
df <- winetaste
nobs <- nrow(df)
itrain <- sample(nobs, 0.8 * nobs)
train <- df[itrain, ]
test <- df[-itrain, ]
library(randomForest)
set.seed(4) # NOTA: Fijamos esta semilla para ilustrar dependencia
bagtrees <- randomForest(taste ~ ., data = train, mtry = ncol(train) - 1)
bagtrees
plot(bagtrees, main = "Tasas de error")
legend("topright", colnames(bagtrees$err.rate), lty = 1:5, col = 1:6)
# View(getTree(bagtrees, 1, labelVar=TRUE))
split_var_1 <- sapply(seq_len(bagtrees$ntree),
function(i) getTree(bagtrees, i, labelVar=TRUE)[1, "split var"])
table(split_var_1)
pred <- predict(bagtrees, newdata = test)
caret::confusionMatrix(pred, test$taste)
# load("datos/winetaste.RData")
# set.seed(1)
# df <- winetaste
# nobs <- nrow(df)
# itrain <- sample(nobs, 0.8 * nobs)
# train <- df[itrain, ]
# test <- df[-itrain, ]
set.seed(1)
rf <- randomForest(taste ~ ., data = train)
rf
plot(rf, main = "Tasas de error")
legend("topright", colnames(rf$err.rate), lty = 1:5, col = 1:6)
importance(rf)
varImpPlot(rf)
pred <- predict(rf, newdata = test)
caret::confusionMatrix(pred, test$taste)
split_var_1 <- sapply(seq_len(rf$ntree),
function(i) getTree(rf, i, labelVar=TRUE)[1, "split var"])
table(split_var_1)
# install.packages("pdp")
library(pdp)
pdp1 <- partial(rf, "alcohol")
plotPartial(pdp1)
pdp2 <- partial(rf, c("alcohol", "density"))
load("datos/winetaste.RData")
set.seed(1)
df <- winetaste
nobs <- nrow(df)
itrain <- sample(nobs, 0.8 * nobs)
train <- df[itrain, ]
test <- df[-itrain, ]
library(randomForest)
set.seed(4) # NOTA: Fijamos esta semilla para ilustrar dependencia
bagtrees <- randomForest(taste ~ ., data = train, mtry = ncol(train) - 1)
bagtrees
plot(bagtrees, main = "Tasas de error")
legend("topright", colnames(bagtrees$err.rate), lty = 1:5, col = 1:6)
# View(getTree(bagtrees, 1, labelVar=TRUE))
split_var_1 <- sapply(seq_len(bagtrees$ntree),
function(i) getTree(bagtrees, i, labelVar=TRUE)[1, "split var"])
table(split_var_1)
pred <- predict(bagtrees, newdata = test)
caret::confusionMatrix(pred, test$taste)
# load("datos/winetaste.RData")
# set.seed(1)
# df <- winetaste
# nobs <- nrow(df)
# itrain <- sample(nobs, 0.8 * nobs)
# train <- df[itrain, ]
# test <- df[-itrain, ]
set.seed(1)
rf <- randomForest(taste ~ ., data = train)
rf
plot(rf, main = "Tasas de error")
legend("topright", colnames(rf$err.rate), lty = 1:5, col = 1:6)
importance(rf)
varImpPlot(rf)
pred <- predict(rf, newdata = test)
caret::confusionMatrix(pred, test$taste)
split_var_1 <- sapply(seq_len(rf$ntree),
function(i) getTree(rf, i, labelVar=TRUE)[1, "split var"])
table(split_var_1)
# install.packages("pdp")
library("pdp")
pdp1 <- partial(rf, "alcohol")
plotPartial(pdp1)
pdp2 <- partial(rf, c("alcohol", "density"))
plotPartial(pdp2)
library(caret)
# str(getModelInfo("rf", regex = FALSE))
modelLookup("rf")
# load("datos/winetaste.RData")
# set.seed(1)
# df <- winetaste
# nobs <- nrow(df)
# itrain <- sample(nobs, 0.8 * nobs)
# train <- df[itrain, ]
# test <- df[-itrain, ]
set.seed(1)
rf.caret <- train(taste ~ ., data = train, method = "rf")
plot(rf.caret)
mtry.class <- sqrt(ncol(train) - 1)
tuneGrid <- data.frame(mtry = floor(c(mtry.class/2, mtry.class, 2*mtry.class)))
set.seed(1)
rf.caret <- train(taste ~ ., data = train,
method = "rf", tuneGrid = tuneGrid)
plot(rf.caret)
load("datos/winetaste.RData")
# Reordenar alfabéticamente los niveles de winetaste$taste
# winetaste$taste <- factor(winetaste$taste, sort(levels(winetaste$taste)))
winetaste$taste <- factor(as.character(winetaste$taste))
# Partición de los datos
set.seed(1)
df <- winetaste
nobs <- nrow(df)
itrain <- sample(nobs, 0.8 * nobs)
train <- df[itrain, ]
test <- df[-itrain, ]
library(ada)
ada.boost <- ada(taste ~ ., data = train, type = "real",
control = rpart.control(maxdepth = 2, cp = 0, minsplit = 10, xval = 0),
iter = 100, nu = 0.05)
ada.boost
plot(ada.boost)
pred <- predict(ada.boost, newdata = test)
caret::confusionMatrix(pred, test$taste, positive = "good")
p.est <- predict(ada.boost, newdata = test, type = "probs")
head(p.est)
library(caret)
modelLookup("ada")
set.seed(1)
caret.ada0 <- train(taste ~ ., method = "ada", data = train,
trControl = trainControl(method = "cv", number = 5))
caret.ada0
confusionMatrix(predict(caret.ada0, newdata = test), test$taste, positive = "good")
set.seed(1)
caret.ada1 <- train(taste ~ ., method = "ada", data = train,
tuneGrid = data.frame(iter =  150, maxdepth = 3,
nu = c(0.3, 0.1, 0.05, 0.01, 0.005)),
trControl = trainControl(method = "cv", number = 5))
caret.ada1
confusionMatrix(predict(caret.ada1, newdata = test), test$taste, positive = "good")
load("datos/winequality.RData")
set.seed(1)
df <- winequality
nobs <- nrow(df)
itrain <- sample(nobs, 0.8 * nobs)
train <- df[itrain, ]
test <- df[-itrain, ]
library(gbm)
gbm.fit <- gbm(quality ~ ., data = train)
gbm.fit
summary(gbm.fit)
plot(gbm.fit, i = "alcohol")
pred <- predict(gbm.fit, newdata = test)
obs <- test$quality
# Con el paquete caret
caret::postResample(pred, obs)
# Con la función accuracy()
accuracy <- function(pred, obs, na.rm = FALSE,
tol = sqrt(.Machine$double.eps)) {
err <- obs - pred     # Errores
if(na.rm) {
is.a <- !is.na(err)
err <- err[is.a]
obs <- obs[is.a]
}
perr <- 100*err/pmax(obs, tol)  # Errores porcentuales
return(c(
me = mean(err),           # Error medio
rmse = sqrt(mean(err^2)), # Raíz del error cuadrático medio
mae = mean(abs(err)),     # Error absoluto medio
mpe = mean(perr),         # Error porcentual medio
mape = mean(abs(perr)),   # Error porcentual absoluto medio
r.squared = 1 - sum(err^2)/sum((obs - mean(obs))^2)
))
}
accuracy(pred, obs)
library(caret)
modelLookup("gbm")
set.seed(1)
caret.gbm0 <- train(quality ~ ., method = "gbm", data = train,
trControl = trainControl(method = "cv", number = 5))
caret.gbm0
caret.gbm1 <- train(quality ~ ., method = "gbm", data = train,
tuneGrid = data.frame(n.trees =  100, interaction.depth = 2,
shrinkage = c(0.3, 0.1, 0.05, 0.01, 0.005),
n.minobsinnode = 10),
trControl = trainControl(method = "cv", number = 5))
caret.gbm1
varImp(caret.gbm1)
postResample(predict(caret.gbm1, newdata = test), test$quality)
library(caret)
# names(getModelInfo("xgb"))
modelLookup("xgbTree")
load("datos/winetaste.RData")
set.seed(1)
df <- winetaste
nobs <- nrow(df)
itrain <- sample(nobs, 0.8 * nobs)
train <- df[itrain, ]
test <- df[-itrain, ]
caret.xgb <- train(taste ~ ., method = "xgbTree", data = train, trControl = trainControl(method = "cv", number = 5))
install.packages("xgb2sql")
install.packages("xgboost")
install.packages("xgobi")
install.packages("xgxr")
install.packages("caretEnsemble")
